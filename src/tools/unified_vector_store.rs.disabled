use anyhow::Result;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::PathBuf;
use std::sync::{Arc, Mutex};
use crate::tools::docs::openai_vectorizer::OpenAIVectorizer;
use crate::tools::docs::doc_traits::{DocumentStore, DocumentVectorizer, DocumentFragment, SearchFilter, SearchResult as DocSearchResult, DocElementKind};
use tantivy::collector::TopDocs;
use tantivy::query::QueryParser;
use tantivy::schema::*;
use tantivy::{doc, Index, IndexWriter, Term};

/// ç»Ÿä¸€å‘é‡å­˜å‚¨ï¼Œé›†æˆTantivyå…¨æ–‡æœç´¢å’Œè¯­ä¹‰å‘é‡æœç´¢
pub struct UnifiedVectorStore {
    /// å‘é‡åŒ–å™¨ï¼ˆä½¿ç”¨é¡¹ç›®å·²æœ‰çš„OpenAIå…¼å®¹æœåŠ¡ï¼‰
    vectorizer: Arc<OpenAIVectorizer>,
    /// Tantivyç´¢å¼•ï¼ˆç”¨äºå…¨æ–‡æœç´¢ï¼‰
    tantivy_index: Index,
    /// Tantivyç´¢å¼•å†™å…¥å™¨
    tantivy_writer: Arc<Mutex<IndexWriter>>,
    /// TantivyæŸ¥è¯¢è§£æå™¨
    query_parser: QueryParser,
    /// å‘é‡å­˜å‚¨ï¼ˆç”¨äºè¯­ä¹‰æœç´¢ï¼‰
    vector_store: Arc<Mutex<VectorMemoryStore>>,
    /// Schemaå­—æ®µ
    schema_fields: SchemaFields,
}

/// Tantivy Schemaå­—æ®µå®šä¹‰
#[derive(Clone)]
struct SchemaFields {
    id: Field,
    content: Field,
    title: Field,
    language: Field,
    package_name: Field,
    doc_type: Field,
    metadata: Field,
}

/// å†…å­˜å‘é‡å­˜å‚¨
struct VectorMemoryStore {
    /// æ–‡æ¡£IDåˆ°å‘é‡çš„æ˜ å°„
    vectors: HashMap<String, Vec<f32>>,
    /// æ–‡æ¡£å…ƒæ•°æ®
    documents: HashMap<String, DocumentFragment>,
    /// æ–‡æ¡£IDåˆ—è¡¨ï¼ˆç”¨äºé«˜æ•ˆç´¢å¼•ï¼‰
    doc_ids: Vec<String>,
}

/// ç»Ÿä¸€æœç´¢ç»“æœ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct UnifiedSearchResult {
    /// æ–‡æ¡£ç‰‡æ®µ
    pub fragment: DocumentFragment,
    /// å…¨æ–‡æœç´¢å¾—åˆ†
    pub fulltext_score: f32,
    /// è¯­ä¹‰ç›¸ä¼¼åº¦å¾—åˆ†
    pub semantic_score: f32,
    /// æ··åˆå¾—åˆ†
    pub hybrid_score: f32,
    /// åŒ¹é…ç±»å‹
    pub match_type: MatchType,
}

/// åŒ¹é…ç±»å‹
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum MatchType {
    FullText,      // ä»…å…¨æ–‡åŒ¹é…
    Semantic,      // ä»…è¯­ä¹‰åŒ¹é…
    Hybrid,        // æ··åˆåŒ¹é…
}

impl UnifiedVectorStore {
    /// åˆ›å»ºæ–°çš„ç»Ÿä¸€å‘é‡å­˜å‚¨
    pub async fn new(index_path: PathBuf) -> Result<Self> {
        // åˆå§‹åŒ–å‘é‡åŒ–å™¨ï¼ˆä½¿ç”¨é¡¹ç›®å·²æœ‰æœåŠ¡ï¼‰
        let vectorizer = Arc::new(OpenAIVectorizer::from_env()?);

        // åˆ›å»ºTantivy schema
        let mut schema_builder = Schema::builder();
        let id = schema_builder.add_text_field("id", TEXT | STORED);
        let content = schema_builder.add_text_field("content", TEXT | STORED);
        let title = schema_builder.add_text_field("title", TEXT | STORED);
        let language = schema_builder.add_text_field("language", STRING | STORED);
        let package_name = schema_builder.add_text_field("package_name", STRING | STORED);
        let doc_type = schema_builder.add_text_field("doc_type", STRING | STORED);
        let metadata = schema_builder.add_text_field("metadata", TEXT | STORED);
        
        let schema = schema_builder.build();
        
        // åˆ›å»ºæˆ–æ‰“å¼€Tantivyç´¢å¼•
        let index = if index_path.exists() {
            Index::open_in_dir(&index_path)?
        } else {
            std::fs::create_dir_all(&index_path)?;
            Index::create_in_dir(&index_path, schema.clone())?
        };

        // åˆ›å»ºç´¢å¼•å†™å…¥å™¨
        let writer = index.writer(50_000_000)?; // 50MB ç¼“å†²åŒº
        
        // åˆ›å»ºæŸ¥è¯¢è§£æå™¨
        let query_parser = QueryParser::for_index(&index, vec![content, title]);

        // åˆ›å»ºå†…å­˜å‘é‡å­˜å‚¨
        let vector_store = Arc::new(Mutex::new(VectorMemoryStore {
            vectors: HashMap::new(),
            documents: HashMap::new(),
            doc_ids: Vec::new(),
        }));

        let schema_fields = SchemaFields {
            id, content, title, language, package_name, doc_type, metadata,
        };

        Ok(Self {
            vectorizer,
            tantivy_index: index,
            tantivy_writer: Arc::new(Mutex::new(writer)),
            query_parser,
            vector_store,
            schema_fields,
        })
    }

    /// æ·»åŠ æ–‡æ¡£ï¼ˆåŒæ—¶ç´¢å¼•å…¨æ–‡å’Œå‘é‡ï¼‰
    pub async fn add_document(&self, fragment: &DocumentFragment) -> Result<()> {
        tracing::info!("ğŸ“ æ·»åŠ æ–‡æ¡£åˆ°ç»Ÿä¸€å­˜å‚¨: {}", fragment.id);

        // 1. ç”Ÿæˆè¯­ä¹‰å‘é‡ï¼ˆä½¿ç”¨é¡¹ç›®å·²æœ‰çš„å‘é‡åŒ–æœåŠ¡ï¼‰
        let vector = self.vectorizer.vectorize(&fragment.content).await?;

        // 2. æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        {
            let mut store = self.vector_store.lock().unwrap();
            store.vectors.insert(fragment.id.clone(), vector);
            store.documents.insert(fragment.id.clone(), fragment.clone());
            store.doc_ids.push(fragment.id.clone());
        }

        // 3. æ·»åŠ åˆ°Tantivyç´¢å¼•
        {
            let writer = self.tantivy_writer.lock().unwrap();
            
            // åºåˆ—åŒ–å…ƒæ•°æ®
            let metadata_json = serde_json::to_string(&fragment.metadata)?;
            
            let doc = doc!(
                self.schema_fields.id => fragment.id.clone(),
                self.schema_fields.content => fragment.content.clone(),
                self.schema_fields.title => fragment.title.clone(),
                self.schema_fields.language => fragment.language.clone(),
                self.schema_fields.package_name => fragment.package_name.clone(),
                self.schema_fields.doc_type => format!("{:?}", fragment.doc_type),
                self.schema_fields.metadata => metadata_json,
            );
            
            writer.add_document(doc)?;
        }

        tracing::debug!("âœ… æ–‡æ¡£å·²æ·»åŠ åˆ°ç»Ÿä¸€å­˜å‚¨");
        Ok(())
    }

    /// æ‰¹é‡æ·»åŠ æ–‡æ¡£
    pub async fn add_documents_batch(&self, fragments: &[DocumentFragment]) -> Result<()> {
        tracing::info!("ğŸ“¦ æ‰¹é‡æ·»åŠ  {} ä¸ªæ–‡æ¡£åˆ°ç»Ÿä¸€å­˜å‚¨", fragments.len());

        // æ‰¹é‡ç”Ÿæˆå‘é‡
        let contents: Vec<String> = fragments.iter().map(|f| f.content.clone()).collect();
        let vectors = self.batch_vectorize(&contents).await?;

        // æ‰¹é‡æ·»åŠ åˆ°å­˜å‚¨
        {
            let mut store = self.vector_store.lock().unwrap();
            let writer = self.tantivy_writer.lock().unwrap();

            for (fragment, vector) in fragments.iter().zip(vectors.iter()) {
                // å‘é‡å­˜å‚¨
                store.vectors.insert(fragment.id.clone(), vector.clone());
                store.documents.insert(fragment.id.clone(), fragment.clone());
                store.doc_ids.push(fragment.id.clone());

                // Tantivyç´¢å¼•
                let metadata_json = serde_json::to_string(&fragment.metadata)?;
                let doc = doc!(
                    self.schema_fields.id => fragment.id.clone(),
                    self.schema_fields.content => fragment.content.clone(),
                    self.schema_fields.title => fragment.title.clone(),
                    self.schema_fields.language => fragment.language.clone(),
                    self.schema_fields.package_name => fragment.package_name.clone(),
                    self.schema_fields.doc_type => format!("{:?}", fragment.doc_type),
                    self.schema_fields.metadata => metadata_json,
                );
                writer.add_document(doc)?;
            }
        }

        tracing::info!("âœ… æ‰¹é‡æ·»åŠ å®Œæˆ");
        Ok(())
    }

    /// æ··åˆæœç´¢ï¼ˆç»“åˆå…¨æ–‡æœç´¢å’Œè¯­ä¹‰æœç´¢ï¼‰
    pub async fn hybrid_search(
        &self,
        query: &str,
        limit: usize,
        semantic_weight: f32, // è¯­ä¹‰æœç´¢æƒé‡ (0.0-1.0)
    ) -> Result<Vec<UnifiedSearchResult>> {
        tracing::info!("ğŸ” æ‰§è¡Œæ··åˆæœç´¢: '{}', é™åˆ¶: {}, è¯­ä¹‰æƒé‡: {}", query, limit, semantic_weight);

        // 1. å…¨æ–‡æœç´¢
        let fulltext_results = self.fulltext_search(query, limit * 2).await?;
        
        // 2. è¯­ä¹‰æœç´¢
        let semantic_results = self.semantic_search(query, limit * 2).await?;

        // 3. åˆå¹¶å’Œé‡æ’åº
        let mut combined_results = HashMap::new();
        
        // æ·»åŠ å…¨æ–‡æœç´¢ç»“æœ
        for result in fulltext_results {
            combined_results.insert(result.fragment.id.clone(), UnifiedSearchResult {
                fragment: result.fragment,
                fulltext_score: result.score,
                semantic_score: 0.0,
                hybrid_score: result.score * (1.0 - semantic_weight),
                match_type: MatchType::FullText,
            });
        }

        // æ·»åŠ è¯­ä¹‰æœç´¢ç»“æœå¹¶è®¡ç®—æ··åˆå¾—åˆ†
        for result in semantic_results {
            if let Some(existing) = combined_results.get_mut(&result.fragment.id) {
                // æ›´æ–°ç°æœ‰ç»“æœ
                existing.semantic_score = result.score;
                existing.hybrid_score = existing.fulltext_score * (1.0 - semantic_weight) + 
                                       result.score * semantic_weight;
                existing.match_type = MatchType::Hybrid;
            } else {
                // æ–°çš„è¯­ä¹‰åŒ¹é…ç»“æœ
                combined_results.insert(result.fragment.id.clone(), UnifiedSearchResult {
                    fragment: result.fragment,
                    fulltext_score: 0.0,
                    semantic_score: result.score,
                    hybrid_score: result.score * semantic_weight,
                    match_type: MatchType::Semantic,
                });
            }
        }

        // æŒ‰æ··åˆå¾—åˆ†æ’åº
        let mut final_results: Vec<UnifiedSearchResult> = combined_results.into_values().collect();
        final_results.sort_by(|a, b| b.hybrid_score.partial_cmp(&a.hybrid_score)
                                    .unwrap_or(std::cmp::Ordering::Equal));
        final_results.truncate(limit);

        tracing::info!("âœ… æ··åˆæœç´¢å®Œæˆï¼Œè¿”å› {} ä¸ªç»“æœ", final_results.len());
        Ok(final_results)
    }

    /// å…¨æ–‡æœç´¢ï¼ˆä½¿ç”¨Tantivyï¼‰
    async fn fulltext_search(&self, query: &str, limit: usize) -> Result<Vec<DocSearchResult>> {
        let reader = self.tantivy_index.reader()?;
        let searcher = reader.searcher();
        
        // è§£ææŸ¥è¯¢
        let query = self.query_parser.parse_query(query)?;
        
        // æ‰§è¡Œæœç´¢
        let top_docs = searcher.search(&query, &TopDocs::with_limit(limit))?;
        
        let mut results = Vec::new();
        for (score, doc_address) in top_docs {
            let retrieved_doc: tantivy::TantivyDocument = searcher.doc(doc_address)?;
            
            // æå–å­—æ®µå€¼ - ä½¿ç”¨æ­£ç¡®çš„OwnedValue API
            let id = retrieved_doc.get_first(self.schema_fields.id)
                .and_then(|v| match v {
                    tantivy::schema::OwnedValue::Str(s) => Some(s.clone()),
                    _ => None,
                })
                .unwrap_or_default();
            
            let content = retrieved_doc.get_first(self.schema_fields.content)
                .and_then(|v| match v {
                    tantivy::schema::OwnedValue::Str(s) => Some(s.clone()),
                    _ => None,
                })
                .unwrap_or_default();
            
            let title = retrieved_doc.get_first(self.schema_fields.title)
                .and_then(|v| match v {
                    tantivy::schema::OwnedValue::Str(s) => Some(s.clone()),
                    _ => None,
                })
                .unwrap_or_default();
            
            let language = retrieved_doc.get_first(self.schema_fields.language)
                .and_then(|v| match v {
                    tantivy::schema::OwnedValue::Str(s) => Some(s.clone()),
                    _ => None,
                })
                .unwrap_or_default();
            
            let package_name = retrieved_doc.get_first(self.schema_fields.package_name)
                .and_then(|v| match v {
                    tantivy::schema::OwnedValue::Str(s) => Some(s.clone()),
                    _ => None,
                })
                .unwrap_or_default();

            // ä»å‘é‡å­˜å‚¨è·å–å®Œæ•´æ–‡æ¡£ä¿¡æ¯
            let fragment = {
                let store = self.vector_store.lock().unwrap();
                if let Some(doc) = store.documents.get(&id) {
                    doc.clone()
                } else {
                    // å¦‚æœå‘é‡å­˜å‚¨ä¸­æ²¡æœ‰ï¼Œåˆ›å»ºåŸºç¡€ç‰‡æ®µ
                    DocumentFragment {
                        id: id.clone(),
                        content,
                        title,
                        language,
                        package_name,
                        version: Some("unknown".to_string()),
                        doc_type: DocElementKind::Other,
                        metadata: HashMap::new(),
                    }
                }
            };

            results.push(DocSearchResult {
                fragment,
                score,
            });
        }

        Ok(results)
    }

    /// è¯­ä¹‰æœç´¢ï¼ˆä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦ï¼‰
    async fn semantic_search(&self, query: &str, limit: usize) -> Result<Vec<DocSearchResult>> {
        // ç”ŸæˆæŸ¥è¯¢å‘é‡
        let query_vector = self.vectorizer.vectorize(query).await?;
        
        // è®¡ç®—ç›¸ä¼¼åº¦å¹¶æ’åº
        let store = self.vector_store.lock().unwrap();
        let mut similarities: Vec<(String, f32)> = store.vectors
            .iter()
            .map(|(id, vector)| {
                let similarity = self.vectorizer.calculate_similarity(&query_vector, vector);
                (id.clone(), similarity)
            })
            .collect();

        similarities.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));
        similarities.truncate(limit);

        // æ„å»ºç»“æœ
        let mut results = Vec::new();
        for (doc_id, score) in similarities {
            if let Some(fragment) = store.documents.get(&doc_id) {
                results.push(DocSearchResult {
                    fragment: fragment.clone(),
                    score,
                });
            }
        }

        Ok(results)
    }

    /// æ‰¹é‡å‘é‡åŒ–
    async fn batch_vectorize(&self, texts: &[String]) -> Result<Vec<Vec<f32>>> {
        let mut vectors = Vec::new();
        
        // å¯ä»¥åœ¨è¿™é‡Œå®ç°æ‰¹é‡APIè°ƒç”¨ä¼˜åŒ–
        // ç›®å‰ç®€å•åœ°é€ä¸ªè°ƒç”¨
        for text in texts {
            let vector = self.vectorizer.vectorize(text).await?;
            vectors.push(vector);
        }
        
        Ok(vectors)
    }

    /// æäº¤ç´¢å¼•æ›´æ”¹
    pub fn commit(&self) -> Result<()> {
        let mut writer = self.tantivy_writer.lock().unwrap();
        writer.commit()?;
        Ok(())
    }

    /// è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
    pub fn get_stats(&self) -> (usize, usize) {
        let store = self.vector_store.lock().unwrap();
        (store.documents.len(), store.vectors.len())
    }

    /// åˆ é™¤æ–‡æ¡£
    pub fn delete_document(&self, doc_id: &str) -> Result<()> {
        // ä»å‘é‡å­˜å‚¨åˆ é™¤
        {
            let mut store = self.vector_store.lock().unwrap();
            store.vectors.remove(doc_id);
            store.documents.remove(doc_id);
            store.doc_ids.retain(|id| id != doc_id);
        }

        // ä»Tantivyåˆ é™¤
        {
            let mut writer = self.tantivy_writer.lock().unwrap();
            let id_term = Term::from_field_text(self.schema_fields.id, doc_id);
            writer.delete_term(id_term);
        }

        Ok(())
    }
}

/// å®ç°DocumentStore traitä»¥æä¾›æ ‡å‡†æ¥å£
#[async_trait::async_trait]
impl DocumentStore for UnifiedVectorStore {
    async fn store(&self, fragment: &DocumentFragment) -> Result<()> {
        self.add_document(fragment).await
    }

    async fn get(&self, id: &str) -> Result<Option<DocumentFragment>> {
        let store = self.vector_store.lock().unwrap();
        Ok(store.documents.get(id).cloned())
    }

    async fn delete(&self, id: &str) -> Result<()> {
        self.delete_document(id)?;
        self.commit()?;
        Ok(())
    }

    async fn search(&self, query: &str, filter: &SearchFilter) -> Result<Vec<DocSearchResult>> {
        // ä½¿ç”¨æ··åˆæœç´¢ï¼Œæƒé‡å¯é…ç½®
        let unified_results = self.hybrid_search(query, filter.limit.unwrap_or(10), 0.3).await?;
        
        // è½¬æ¢ä¸ºæ ‡å‡†æœç´¢ç»“æœ
        let results = unified_results
            .into_iter()
            .map(|ur| DocSearchResult {
                fragment: ur.fragment,
                score: ur.hybrid_score,
            })
            .collect();

        Ok(results)
    }
} 