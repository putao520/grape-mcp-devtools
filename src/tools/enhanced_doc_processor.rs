use anyhow::{Result, anyhow};
use tracing::{info, warn, debug};
use serde::{Serialize, Deserialize};
use tokio::time::{timeout, Duration};
use std::sync::Arc;

use crate::tools::base::{FileDocumentFragment, MCPTool};
use crate::tools::vector_docs_tool::VectorDocsTool;
use crate::tools::doc_processor::DocumentProcessor;

/// å¢å¼ºçš„æ–‡æ¡£å¤„ç†å™¨
pub struct EnhancedDocumentProcessor {
    /// åŸºç¡€æ–‡æ¡£å¤„ç†å™¨
    base_processor: DocumentProcessor,
    /// å‘é‡å·¥å…·
    vector_tool: Arc<VectorDocsTool>,
    /// é…ç½®
    config: ProcessorConfig,
}

/// å¤„ç†å™¨é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProcessorConfig {
    /// æœ€å¤§æ–‡æ¡£é•¿åº¦ï¼ˆå­—ç¬¦ï¼‰
    pub max_document_length: usize,
    /// æ–‡æ¡£åˆ†å—å¤§å°
    pub chunk_size: usize,
    /// åˆ†å—é‡å å¤§å°
    pub chunk_overlap: usize,
    /// æœ€å¤§é‡è¯•æ¬¡æ•°
    pub max_retries: u32,
    /// è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    pub request_timeout_secs: u64,
    /// å¯ç”¨æ™ºèƒ½åˆ†å—
    pub enable_smart_chunking: bool,
    /// å¯ç”¨å†…å®¹è¿‡æ»¤
    pub enable_content_filtering: bool,
}

impl Default for ProcessorConfig {
    fn default() -> Self {
        Self {
            max_document_length: 10000,
            chunk_size: 1000,
            chunk_overlap: 100,
            max_retries: 3,
            request_timeout_secs: 30,
            enable_smart_chunking: true,
            enable_content_filtering: true,
        }
    }
}

/// æ–‡æ¡£åˆ†å—ç»“æœ
#[derive(Debug, Clone)]
pub struct DocumentChunk {
    pub id: String,
    pub content: String,
    pub chunk_index: usize,
    pub total_chunks: usize,
    pub metadata: ChunkMetadata,
}

/// åˆ†å—å…ƒæ•°æ®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChunkMetadata {
    pub original_file: String,
    pub language: String,
    pub package_name: String,
    pub version: String,
    pub content_type: String,
    pub keywords: Vec<String>,
    pub importance_score: f32,
}

/// æœç´¢ç»“æœ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EnhancedSearchResult {
    pub fragment: FileDocumentFragment,
    pub score: f32,
    pub relevance_explanation: String,
    pub matched_keywords: Vec<String>,
    pub content_preview: String,
}

impl EnhancedDocumentProcessor {
    /// åˆ›å»ºæ–°çš„å¢å¼ºæ–‡æ¡£å¤„ç†å™¨
    pub async fn new(vector_tool: Arc<VectorDocsTool>) -> Result<Self> {
        let base_processor = DocumentProcessor::new().await?;
        
        Ok(Self {
            base_processor,
            vector_tool,
            config: ProcessorConfig::default(),
        })
    }
    
    /// ä½¿ç”¨è‡ªå®šä¹‰é…ç½®åˆ›å»ºå¤„ç†å™¨
    pub async fn with_config(config: ProcessorConfig, vector_tool: Arc<VectorDocsTool>) -> Result<Self> {
        let mut processor = Self::new(vector_tool).await?;
        processor.config = config;
        Ok(processor)
    }
    
    /// å¤„ç†æ–‡æ¡£è¯·æ±‚çš„ä¸»è¦å…¥å£ç‚¹ï¼ˆå¢å¼ºç‰ˆï¼‰
    pub async fn process_documentation_request_enhanced(
        &self,
        language: &str,
        package_name: &str,
        version: Option<&str>,
        query: &str,
    ) -> Result<Vec<EnhancedSearchResult>> {
        let version = version.unwrap_or("latest");
        
        info!("ğŸ” å¢å¼ºå¤„ç†æ–‡æ¡£è¯·æ±‚: {} {} {} - æŸ¥è¯¢: {}", language, package_name, version, query);
        
        // 1. é¦–å…ˆå°è¯•æ™ºèƒ½æœç´¢ç°æœ‰æ–‡æ¡£
        if let Ok(search_results) = self.smart_search_existing_docs(language, package_name, version, query).await {
            if !search_results.is_empty() {
                info!("âœ… ä»å‘é‡åº“æ‰¾åˆ° {} ä¸ªç›¸å…³æ–‡æ¡£", search_results.len());
                return Ok(search_results);
            }
        }
        
        // 2. ç”Ÿæˆæ–°æ–‡æ¡£ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        info!("ğŸ“ å‘é‡åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œå¼€å§‹ç”Ÿæˆæ–°æ–‡æ¡£");
        let fragments = self.generate_docs_with_retry(language, package_name, version).await?;
        
        if fragments.is_empty() {
            warn!("âš ï¸ æ²¡æœ‰ç”Ÿæˆä»»ä½•æ–‡æ¡£ç‰‡æ®µ");
            return Ok(Vec::new());
        }
        
        // 3. æ™ºèƒ½åˆ†å—å’Œå‘é‡åŒ–
        let chunks = self.smart_chunk_documents(&fragments).await?;
        self.vectorize_and_store_chunks(&chunks).await?;
        
        // 4. å†æ¬¡æœç´¢ä»¥è¿”å›ç›¸å…³ç»“æœ
        let search_results = self.smart_search_existing_docs(language, package_name, version, query).await
            .unwrap_or_else(|_| self.create_fallback_results(&fragments, query));
        
        Ok(search_results)
    }
    
    /// æ™ºèƒ½æœç´¢ç°æœ‰æ–‡æ¡£
    async fn smart_search_existing_docs(
        &self,
        language: &str,
        package_name: &str,
        _version: &str,
        query: &str,
    ) -> Result<Vec<EnhancedSearchResult>> {
        // æ„å»ºå¢å¼ºçš„æœç´¢æŸ¥è¯¢
        let enhanced_query = self.build_enhanced_query(language, package_name, query);
        
        let search_params = serde_json::json!({
            "action": "search",
            "query": enhanced_query,
            "limit": 10
        });
        
        let search_result = self.vector_tool.execute(search_params).await?;
        
        if search_result["status"] == "success" && search_result["results_count"].as_u64().unwrap_or(0) > 0 {
            let empty_vec = vec![];
            let results = search_result["results"].as_array().unwrap_or(&empty_vec);
            let mut enhanced_results = Vec::new();
            
            for result in results {
                if let Some(enhanced_result) = self.create_enhanced_result(result, query, language).await {
                    enhanced_results.push(enhanced_result);
                }
            }
            
            // æŒ‰ç›¸å…³æ€§æ’åº
            enhanced_results.sort_by(|a, b| b.score.partial_cmp(&a.score).unwrap_or(std::cmp::Ordering::Equal));
            
            Ok(enhanced_results)
        } else {
            Ok(Vec::new())
        }
    }
    
    /// æ„å»ºå¢å¼ºçš„æœç´¢æŸ¥è¯¢
    fn build_enhanced_query(&self, language: &str, package_name: &str, query: &str) -> String {
        let keywords = vec![
            language.to_string(),
            package_name.to_string(),
            query.to_string(),
        ];
        
        // æ·»åŠ è¯­è¨€ç‰¹å®šçš„å…³é”®è¯
        let mut enhanced_keywords = keywords;
        match language.to_lowercase().as_str() {
            "rust" => {
                enhanced_keywords.extend(vec!["crate".to_string(), "cargo".to_string()]);
            }
            "python" => {
                enhanced_keywords.extend(vec!["pip".to_string(), "package".to_string()]);
            }
            "javascript" => {
                enhanced_keywords.extend(vec!["npm".to_string(), "node".to_string()]);
            }
            "go" => {
                enhanced_keywords.extend(vec!["module".to_string(), "pkg".to_string()]);
            }
            "java" => {
                enhanced_keywords.extend(vec!["maven".to_string(), "jar".to_string()]);
            }
            _ => {}
        }
        
        enhanced_keywords.join(" ")
    }
    
    /// åˆ›å»ºå¢å¼ºçš„æœç´¢ç»“æœ
    async fn create_enhanced_result(
        &self,
        result: &serde_json::Value,
        query: &str,
        expected_language: &str,
    ) -> Option<EnhancedSearchResult> {
        let title = result["title"].as_str()?;
        let content = result["content"].as_str()?;
        let score = result["score"].as_f64().unwrap_or(0.0) as f32;
        
        // æå–è¯­è¨€ä¿¡æ¯
        let language = result["language"].as_str().unwrap_or("unknown");
        
        // è®¡ç®—ç›¸å…³æ€§
        let relevance_score = self.calculate_relevance(content, query, language, expected_language);
        let final_score = (score + relevance_score) / 2.0;
        
        // ç”Ÿæˆç›¸å…³æ€§è§£é‡Š
        let relevance_explanation = self.generate_relevance_explanation(content, query, language);
        
        // æå–åŒ¹é…çš„å…³é”®è¯
        let matched_keywords = self.extract_matched_keywords(content, query);
        
        // ç”Ÿæˆå†…å®¹é¢„è§ˆ
        let content_preview = self.generate_content_preview(content, query);
        
        // åˆ›å»ºæ–‡æ¡£ç‰‡æ®µ
        let fragment = FileDocumentFragment::new(
            language.to_string(),
            result["package"].as_str().unwrap_or("unknown").to_string(),
            result["version"].as_str().unwrap_or("latest").to_string(),
            title.to_string(),
            content.to_string(),
        );
        
        Some(EnhancedSearchResult {
            fragment,
            score: final_score,
            relevance_explanation,
            matched_keywords,
            content_preview,
        })
    }
    
    /// è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
    fn calculate_relevance(&self, content: &str, query: &str, language: &str, expected_language: &str) -> f32 {
        let mut score = 0.0;
        let content_lower = content.to_lowercase();
        let query_lower = query.to_lowercase();
        
        // è¯­è¨€åŒ¹é…
        if language == expected_language {
            score += 0.3;
        }
        
        // æŸ¥è¯¢è¯åŒ¹é…
        let query_words: Vec<&str> = query_lower.split_whitespace().collect();
        let matched_words = query_words.iter()
            .filter(|word| content_lower.contains(*word))
            .count();
        
        if !query_words.is_empty() {
            score += (matched_words as f32 / query_words.len() as f32) * 0.5;
        }
        
        // å†…å®¹è´¨é‡è¯„ä¼°
        if content.len() > 100 {
            score += 0.1;
        }
        if content.len() > 500 {
            score += 0.1;
        }
        
        score.min(1.0)
    }
    
    /// ç”Ÿæˆç›¸å…³æ€§è§£é‡Š
    fn generate_relevance_explanation(&self, content: &str, query: &str, language: &str) -> String {
        let content_lower = content.to_lowercase();
        let query_lower = query.to_lowercase();
        
        let mut explanations = Vec::new();
        
        // æ£€æŸ¥ç›´æ¥åŒ¹é…
        if content_lower.contains(&query_lower) {
            explanations.push(format!("åŒ…å«æŸ¥è¯¢è¯ '{}'", query));
        }
        
        // æ£€æŸ¥éƒ¨åˆ†åŒ¹é…
        let query_words: Vec<&str> = query_lower.split_whitespace().collect();
        let matched_words: Vec<&str> = query_words.iter()
            .filter(|word| content_lower.contains(*word))
            .cloned()
            .collect();
        
        if !matched_words.is_empty() && matched_words.len() < query_words.len() {
            explanations.push(format!("åŒ¹é…å…³é”®è¯: {}", matched_words.join(", ")));
        }
        
        // è¯­è¨€åŒ¹é…
        explanations.push(format!("è¯­è¨€: {}", language));
        
        if explanations.is_empty() {
            "åŸºäºå‘é‡ç›¸ä¼¼åº¦åŒ¹é…".to_string()
        } else {
            explanations.join("; ")
        }
    }
    
    /// æå–åŒ¹é…çš„å…³é”®è¯
    fn extract_matched_keywords(&self, content: &str, query: &str) -> Vec<String> {
        let content_lower = content.to_lowercase();
        let query_lower = query.to_lowercase();
        
        let query_words: Vec<&str> = query_lower.split_whitespace().collect();
        query_words.iter()
            .filter(|word| content_lower.contains(*word))
            .map(|word| word.to_string())
            .collect()
    }
    
    /// ç”Ÿæˆå†…å®¹é¢„è§ˆ
    fn generate_content_preview(&self, content: &str, query: &str) -> String {
        let query_lower = query.to_lowercase();
        
        // æŸ¥æ‰¾åŒ…å«æŸ¥è¯¢è¯çš„å¥å­
        for sentence in content.split('.') {
            if sentence.to_lowercase().contains(&query_lower) {
                let trimmed = sentence.trim();
                if trimmed.len() > 20 {
                    return if trimmed.len() > 200 {
                        format!("{}...", &trimmed[..200])
                    } else {
                        trimmed.to_string()
                    };
                }
            }
        }
        
        // å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ…å«æŸ¥è¯¢è¯çš„å¥å­ï¼Œè¿”å›å¼€å¤´éƒ¨åˆ†
        if content.len() > 200 {
            format!("{}...", &content[..200])
        } else {
            content.to_string()
        }
    }
    
    /// å¸¦é‡è¯•æœºåˆ¶çš„æ–‡æ¡£ç”Ÿæˆ
    async fn generate_docs_with_retry(
        &self,
        language: &str,
        package_name: &str,
        version: &str,
    ) -> Result<Vec<FileDocumentFragment>> {
        let mut last_error = None;
        
        for attempt in 1..=self.config.max_retries {
            info!("ğŸ“ å°è¯•ç”Ÿæˆæ–‡æ¡£ (ç¬¬ {}/{} æ¬¡): {} {} {}", 
                  attempt, self.config.max_retries, language, package_name, version);
            
            match timeout(
                Duration::from_secs(self.config.request_timeout_secs),
                self.base_processor.process_documentation_request(language, package_name, Some(version), "documentation")
            ).await {
                Ok(Ok(fragments)) => {
                    if !fragments.is_empty() {
                        info!("âœ… æˆåŠŸç”Ÿæˆ {} ä¸ªæ–‡æ¡£ç‰‡æ®µ", fragments.len());
                        return Ok(fragments);
                    } else {
                        warn!("âš ï¸ ç”Ÿæˆäº†ç©ºçš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨");
                    }
                }
                Ok(Err(e)) => {
                    warn!("âš ï¸ ç¬¬ {} æ¬¡å°è¯•å¤±è´¥: {}", attempt, e);
                    last_error = Some(e);
                }
                Err(_) => {
                    warn!("âš ï¸ ç¬¬ {} æ¬¡å°è¯•è¶…æ—¶", attempt);
                    last_error = Some(anyhow!("è¯·æ±‚è¶…æ—¶"));
                }
            }
            
            if attempt < self.config.max_retries {
                tokio::time::sleep(Duration::from_millis(1000 * attempt as u64)).await;
            }
        }
        
        Err(last_error.unwrap_or_else(|| anyhow!("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")))
    }
    
    /// æ™ºèƒ½æ–‡æ¡£åˆ†å—
    async fn smart_chunk_documents(&self, fragments: &[FileDocumentFragment]) -> Result<Vec<DocumentChunk>> {
        let mut chunks = Vec::new();
        
        for fragment in fragments {
            let fragment_chunks = if self.config.enable_smart_chunking {
                self.smart_chunk_single_document(fragment).await?
            } else {
                self.simple_chunk_document(fragment)
            };
            
            chunks.extend(fragment_chunks);
        }
        
        info!("ğŸ“¦ æ€»å…±åˆ›å»ºäº† {} ä¸ªæ–‡æ¡£åˆ†å—", chunks.len());
        Ok(chunks)
    }
    
    /// æ™ºèƒ½åˆ†å—å•ä¸ªæ–‡æ¡£
    async fn smart_chunk_single_document(&self, fragment: &FileDocumentFragment) -> Result<Vec<DocumentChunk>> {
        let content = &fragment.content;
        
        // å¦‚æœæ–‡æ¡£è¾ƒçŸ­ï¼Œä¸éœ€è¦åˆ†å—
        if content.len() <= self.config.chunk_size {
            return Ok(vec![self.create_single_chunk(fragment, 0, 1)]);
        }
        
        // æŒ‰æ®µè½åˆ†å—
        let paragraphs: Vec<&str> = content.split("\n\n").collect();
        let mut chunks = Vec::new();
        let mut current_chunk = String::new();
        let mut chunk_index = 0;
        
        for paragraph in paragraphs {
            if current_chunk.len() + paragraph.len() > self.config.chunk_size && !current_chunk.is_empty() {
                // åˆ›å»ºå½“å‰åˆ†å—
                chunks.push(self.create_chunk_from_content(fragment, &current_chunk, chunk_index));
                chunk_index += 1;
                
                // å¼€å§‹æ–°åˆ†å—ï¼ˆä¿ç•™é‡å ï¼‰
                current_chunk = if current_chunk.len() > self.config.chunk_overlap {
                    current_chunk[current_chunk.len() - self.config.chunk_overlap..].to_string()
                } else {
                    String::new()
                };
            }
            
            if !current_chunk.is_empty() {
                current_chunk.push_str("\n\n");
            }
            current_chunk.push_str(paragraph);
        }
        
        // æ·»åŠ æœ€åä¸€ä¸ªåˆ†å—
        if !current_chunk.is_empty() {
            chunks.push(self.create_chunk_from_content(fragment, &current_chunk, chunk_index));
        }
        
        // æ›´æ–°æ€»åˆ†å—æ•°
        let total_chunks = chunks.len();
        for chunk in &mut chunks {
            chunk.total_chunks = total_chunks;
        }
        
        Ok(chunks)
    }
    
    /// ç®€å•åˆ†å—æ–‡æ¡£
    fn simple_chunk_document(&self, fragment: &FileDocumentFragment) -> Vec<DocumentChunk> {
        let content = &fragment.content;
        let mut chunks = Vec::new();
        let mut start = 0;
        let mut chunk_index = 0;
        
        while start < content.len() {
            let end = (start + self.config.chunk_size).min(content.len());
            let chunk_content = content[start..end].to_string();
            
            chunks.push(self.create_chunk_from_content(fragment, &chunk_content, chunk_index));
            
            start = if end == content.len() {
                break;
            } else {
                (end - self.config.chunk_overlap).max(start + 1)
            };
            chunk_index += 1;
        }
        
        // æ›´æ–°æ€»åˆ†å—æ•°
        let total_chunks = chunks.len();
        for chunk in &mut chunks {
            chunk.total_chunks = total_chunks;
        }
        
        chunks
    }
    
    /// åˆ›å»ºå•ä¸ªåˆ†å—
    fn create_single_chunk(&self, fragment: &FileDocumentFragment, chunk_index: usize, _total_chunks: usize) -> DocumentChunk {
        self.create_chunk_from_content(fragment, &fragment.content, chunk_index)
    }
    
    /// ä»å†…å®¹åˆ›å»ºåˆ†å—
    fn create_chunk_from_content(&self, fragment: &FileDocumentFragment, content: &str, chunk_index: usize) -> DocumentChunk {
        let chunk_id = format!("{}_{}", fragment.id, chunk_index);
        let keywords = self.extract_keywords(content);
        let importance_score = self.calculate_importance_score(content);
        
        DocumentChunk {
            id: chunk_id,
            content: content.to_string(),
            chunk_index,
            total_chunks: 1, // å°†åœ¨åé¢æ›´æ–°
            metadata: ChunkMetadata {
                original_file: fragment.file_path.clone(),
                language: fragment.language.clone(),
                package_name: fragment.package_name.clone(),
                version: fragment.version.clone(),
                content_type: "documentation".to_string(),
                keywords,
                importance_score,
            },
        }
    }
    
    /// æå–å…³é”®è¯
    fn extract_keywords(&self, content: &str) -> Vec<String> {
        let words: Vec<&str> = content
            .split_whitespace()
            .filter(|word| word.len() > 3)
            .take(10)
            .collect();
        
        words.iter().map(|word| word.to_lowercase()).collect()
    }
    
    /// è®¡ç®—é‡è¦æ€§åˆ†æ•°
    fn calculate_importance_score(&self, content: &str) -> f32 {
        let mut score = 0.0;
        
        // åŸºäºé•¿åº¦
        score += (content.len() as f32 / 1000.0).min(0.3);
        
        // åŸºäºå…³é”®è¯
        let important_keywords = ["function", "class", "method", "api", "example", "usage"];
        let content_lower = content.to_lowercase();
        for keyword in important_keywords {
            if content_lower.contains(keyword) {
                score += 0.1;
            }
        }
        
        score.min(1.0)
    }
    
    /// å‘é‡åŒ–å¹¶å­˜å‚¨åˆ†å—
    async fn vectorize_and_store_chunks(&self, chunks: &[DocumentChunk]) -> Result<()> {
        info!("ğŸ”„ å¼€å§‹å‘é‡åŒ–å¹¶å­˜å‚¨ {} ä¸ªæ–‡æ¡£åˆ†å—", chunks.len());
        
        let mut successful_stores = 0;
        let mut failed_stores = 0;
        
        for chunk in chunks {
            // æ£€æŸ¥å†…å®¹é•¿åº¦ï¼Œå¦‚æœå¤ªé•¿åˆ™è·³è¿‡
            if chunk.content.len() > self.config.max_document_length {
                warn!("âš ï¸ è·³è¿‡è¿‡é•¿çš„åˆ†å—: {} ({} å­—ç¬¦)", chunk.id, chunk.content.len());
                failed_stores += 1;
                continue;
            }
            
            let store_params = serde_json::json!({
                "action": "store",
                "title": format!("{} - åˆ†å— {}", chunk.metadata.original_file, chunk.chunk_index),
                "content": chunk.content,
                "language": chunk.metadata.language,
                "package": chunk.metadata.package_name,
                "version": chunk.metadata.version,
                "metadata": {
                    "chunk_id": chunk.id,
                    "chunk_index": chunk.chunk_index,
                    "total_chunks": chunk.total_chunks,
                    "content_type": chunk.metadata.content_type,
                    "keywords": chunk.metadata.keywords,
                    "importance_score": chunk.metadata.importance_score
                }
            });
            
            match self.vector_tool.execute(store_params).await {
                Ok(result) => {
                    if result["status"] == "success" {
                        debug!("âœ… æˆåŠŸå­˜å‚¨åˆ†å—: {}", chunk.id);
                        successful_stores += 1;
                    } else {
                        warn!("âš ï¸ å­˜å‚¨åˆ†å—å¤±è´¥: {} - {}", chunk.id, result["error"]);
                        failed_stores += 1;
                    }
                }
                Err(e) => {
                    warn!("âš ï¸ å­˜å‚¨åˆ†å—æ—¶å‘ç”Ÿé”™è¯¯: {} - {}", chunk.id, e);
                    failed_stores += 1;
                }
            }
        }
        
        info!("ğŸ“Š åˆ†å—å­˜å‚¨å®Œæˆ: {} æˆåŠŸ, {} å¤±è´¥", successful_stores, failed_stores);
        Ok(())
    }
    
    /// åˆ›å»ºå›é€€ç»“æœ
    fn create_fallback_results(&self, fragments: &[FileDocumentFragment], query: &str) -> Vec<EnhancedSearchResult> {
        fragments.iter().map(|fragment| {
            let score = self.calculate_relevance(&fragment.content, query, &fragment.language, &fragment.language);
            let relevance_explanation = "åŸºäºç”Ÿæˆçš„æ–‡æ¡£å†…å®¹".to_string();
            let matched_keywords = self.extract_matched_keywords(&fragment.content, query);
            let content_preview = self.generate_content_preview(&fragment.content, query);
            
            EnhancedSearchResult {
                fragment: fragment.clone(),
                score,
                relevance_explanation,
                matched_keywords,
                content_preview,
            }
        }).collect()
    }
    
    /// è·å–å¤„ç†å™¨ç»Ÿè®¡ä¿¡æ¯
    pub async fn get_processor_stats(&self) -> Result<ProcessorStats> {
        let stats_params = serde_json::json!({
            "action": "stats"
        });
        
        let result = self.vector_tool.execute(stats_params).await?;
        
        Ok(ProcessorStats {
            total_documents: result["total_documents"].as_u64().unwrap_or(0),
            total_vectors: result["total_vectors"].as_u64().unwrap_or(0),
            supported_languages: vec!["rust".to_string(), "python".to_string(), "javascript".to_string(), "go".to_string(), "java".to_string()],
            config: self.config.clone(),
        })
    }
}

/// å¤„ç†å™¨ç»Ÿè®¡ä¿¡æ¯
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProcessorStats {
    pub total_documents: u64,
    pub total_vectors: u64,
    pub supported_languages: Vec<String>,
    pub config: ProcessorConfig,
} 