use anyhow::Result;
use tracing::{info, debug, warn, error};
use std::collections::{HashMap, HashSet, VecDeque};
use url::Url;
use chrono::{DateTime, Utc, Duration};
use tokio::time::sleep;
use std::sync::Arc;

use super::ai_service::AIService;
use super::intelligent_web_analyzer::{
    IntelligentWebAnalyzer, CrawlTask, PageRelevanceAnalysis, 
    ContentRegionAnalysis, ExtractedLink, RecommendedAction
};

/// æ™ºèƒ½URLçˆ¬è™«
/// ä»»åŠ¡å¯¼å‘çš„é˜²å¾ªç¯çˆ¬è™«ç³»ç»Ÿ
pub struct SmartUrlCrawler {
    web_analyzer: IntelligentWebAnalyzer,
    http_client: reqwest::Client,
    crawl_state: Arc<tokio::sync::RwLock<CrawlState>>,
}

/// çˆ¬è™«çŠ¶æ€
#[derive(Debug)]
struct CrawlState {
    /// å·²è®¿é—®çš„URL
    visited_urls: HashSet<String>,
    /// å¾…å¤„ç†çš„URLé˜Ÿåˆ—
    pending_urls: VecDeque<PendingUrl>,
    /// å½“å‰ä»»åŠ¡
    current_task: Option<CrawlTask>,
    /// ä»»åŠ¡ç»“æœ
    task_results: Vec<TaskResult>,
    /// å¾ªç¯æ£€æµ‹è®°å½•
    loop_detection: HashMap<String, LoopDetectionInfo>,
    /// çˆ¬è™«ç»Ÿè®¡
    statistics: CrawlStatistics,
}

/// å¾…å¤„ç†çš„URL
#[derive(Debug, Clone)]
struct PendingUrl {
    /// URLåœ°å€
    url: String,
    /// ä¼˜å…ˆçº§ (1-5, 5æœ€é«˜)
    priority: u8,
    /// æ·±åº¦
    depth: u32,
    /// çˆ¶URL
    parent_url: Option<String>,
    /// å‘ç°æ—¶é—´
    discovered_at: DateTime<Utc>,
    /// é¢„æœŸå†…å®¹ç±»å‹
    expected_content_type: Option<String>,
}

/// ä»»åŠ¡ç»“æœ
#[derive(Debug, Clone)]
pub struct TaskResult {
    /// ä»»åŠ¡ID
    pub task_id: String,
    /// å¤„ç†çš„URL
    pub url: String,
    /// ç›¸å…³æ€§åˆ†æ
    pub relevance_analysis: PageRelevanceAnalysis,
    /// å†…å®¹åŒºåŸŸ
    pub content_regions: ContentRegionAnalysis,
    /// æå–çš„å†…å®¹æ‘˜è¦
    pub content_summary: String,
    /// å‘ç°çš„ç›¸å…³é“¾æ¥æ•°é‡
    pub discovered_links_count: usize,
    /// å¤„ç†æ—¶é—´
    pub processed_at: DateTime<Utc>,
    /// å¤„ç†è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
    pub processing_time_ms: u64,
}

/// å¾ªç¯æ£€æµ‹ä¿¡æ¯
#[derive(Debug, Clone)]
struct LoopDetectionInfo {
    /// è®¿é—®æ¬¡æ•°
    visit_count: u32,
    /// é¦–æ¬¡è®¿é—®æ—¶é—´
    first_visit: DateTime<Utc>,
    /// æœ€åè®¿é—®æ—¶é—´
    last_visit: DateTime<Utc>,
    /// è®¿é—®è·¯å¾„
    visit_path: Vec<String>,
}

/// çˆ¬è™«ç»Ÿè®¡
#[derive(Debug, Clone)]
pub struct CrawlStatistics {
    /// æ€»è®¿é—®é¡µé¢æ•°
    pub total_pages_visited: u32,
    /// ç›¸å…³é¡µé¢æ•°
    pub relevant_pages_count: u32,
    /// è·³è¿‡é¡µé¢æ•°
    pub skipped_pages_count: u32,
    /// å¾ªç¯æ£€æµ‹æ¬¡æ•°
    pub loop_detections: u32,
    /// æ€»å¤„ç†æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub total_processing_time_ms: u64,
    /// å¹³å‡ç›¸å…³æ€§åˆ†æ•°
    pub average_relevance_score: f32,
    /// å¼€å§‹æ—¶é—´
    pub start_time: DateTime<Utc>,
    /// ç»“æŸæ—¶é—´
    pub end_time: Option<DateTime<Utc>>,
}

/// çˆ¬è™«é…ç½®
#[derive(Debug, Clone)]
pub struct CrawlerConfig {
    /// å¹¶å‘åº¦
    pub concurrency: u32,
    /// è¯·æ±‚é—´éš”ï¼ˆæ¯«ç§’ï¼‰
    pub delay_ms: u64,
    /// æœ€å¤§é‡è¯•æ¬¡æ•°
    pub max_retries: u32,
    /// è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    pub timeout_secs: u64,
    /// å¾ªç¯æ£€æµ‹é˜ˆå€¼
    pub loop_detection_threshold: u32,
    /// æœ€å°ç›¸å…³æ€§åˆ†æ•°
    pub min_relevance_score: f32,
    /// ç”¨æˆ·ä»£ç†
    pub user_agent: String,
}

impl Default for CrawlerConfig {
    fn default() -> Self {
        Self {
            concurrency: 3,
            delay_ms: 1000,
            max_retries: 3,
            timeout_secs: 30,
            loop_detection_threshold: 3,
            min_relevance_score: 0.5,
            user_agent: "GrapeMCPDevtools/2.0 (Intelligent Web Crawler)".to_string(),
        }
    }
}

impl SmartUrlCrawler {
    /// åˆ›å»ºæ–°çš„æ™ºèƒ½çˆ¬è™«
    pub async fn new(ai_service: AIService, config: CrawlerConfig) -> Result<Self> {
        let web_analyzer = IntelligentWebAnalyzer::new(ai_service).await?;
        
        let http_client = reqwest::Client::builder()
            .timeout(std::time::Duration::from_secs(config.timeout_secs))
            .user_agent(config.user_agent.clone())
            .build()?;

        let crawl_state = Arc::new(tokio::sync::RwLock::new(CrawlState {
            visited_urls: HashSet::new(),
            pending_urls: VecDeque::new(),
            current_task: None,
            task_results: Vec::new(),
            loop_detection: HashMap::new(),
            statistics: CrawlStatistics {
                total_pages_visited: 0,
                relevant_pages_count: 0,
                skipped_pages_count: 0,
                loop_detections: 0,
                total_processing_time_ms: 0,
                average_relevance_score: 0.0,
                start_time: Utc::now(),
                end_time: None,
            },
        }));

        info!("ğŸš€ æ™ºèƒ½URLçˆ¬è™«åˆå§‹åŒ–å®Œæˆ");
        info!("âš™ï¸ é…ç½® - å¹¶å‘åº¦: {}, å»¶è¿Ÿ: {}ms, æœ€å°ç›¸å…³æ€§: {}", 
              config.concurrency, config.delay_ms, config.min_relevance_score);

        Ok(Self {
            web_analyzer,
            http_client,
            crawl_state,
        })
    }

    /// å¼€å§‹æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
    pub async fn execute_task(&self, task: CrawlTask, config: CrawlerConfig) -> Result<Vec<TaskResult>> {
        info!("ğŸ¯ å¼€å§‹æ‰§è¡Œçˆ¬è™«ä»»åŠ¡: {}", task.target_description);
        info!("ğŸ“ èµ·å§‹URL: {}", task.start_url);

        // åˆå§‹åŒ–ä»»åŠ¡
        {
            let mut state = self.crawl_state.write().await;
            state.current_task = Some(task.clone());
            state.visited_urls.clear();
            state.pending_urls.clear();
            state.task_results.clear();
            state.loop_detection.clear();
            state.statistics = CrawlStatistics {
                total_pages_visited: 0,
                relevant_pages_count: 0,
                skipped_pages_count: 0,
                loop_detections: 0,
                total_processing_time_ms: 0,
                average_relevance_score: 0.0,
                start_time: Utc::now(),
                end_time: None,
            };

            // æ·»åŠ èµ·å§‹URLåˆ°å¾…å¤„ç†é˜Ÿåˆ—
            state.pending_urls.push_back(PendingUrl {
                url: task.start_url.clone(),
                priority: 5, // èµ·å§‹URLæœ€é«˜ä¼˜å…ˆçº§
                depth: 0,
                parent_url: None,
                discovered_at: Utc::now(),
                expected_content_type: None,
            });
        }

        // æ‰§è¡Œçˆ¬è™«å¾ªç¯
        while let Some(pending_url) = self.get_next_url().await {
            if self.should_stop_crawling(&task).await {
                info!("â¹ï¸ è¾¾åˆ°çˆ¬è™«åœæ­¢æ¡ä»¶");
                break;
            }

            // å¾ªç¯æ£€æµ‹
            if self.detect_loop(&pending_url.url).await {
                warn!("ğŸ”„ æ£€æµ‹åˆ°å¾ªç¯ï¼Œè·³è¿‡URL: {}", pending_url.url);
                self.increment_loop_detection().await;
                continue;
            }

            // å¤„ç†URL
            match self.process_url(&pending_url, &task, &config).await {
                Ok(result) => {
                    if let Some(task_result) = result {
                        self.add_task_result(task_result).await;
                    }
                }
                Err(e) => {
                    error!("âŒ å¤„ç†URLå¤±è´¥ {}: {}", pending_url.url, e);
                }
            }

            // å»¶è¿Ÿ
            if config.delay_ms > 0 {
                sleep(std::time::Duration::from_millis(config.delay_ms)).await;
            }
        }

        // å®Œæˆä»»åŠ¡
        let results = {
            let mut state = self.crawl_state.write().await;
            state.statistics.end_time = Some(Utc::now());
            state.task_results.clone()
        };

        info!("âœ… çˆ¬è™«ä»»åŠ¡å®Œæˆ");
        self.print_statistics().await;

        Ok(results)
    }

    /// å¤„ç†å•ä¸ªURL
    async fn process_url(&self, pending_url: &PendingUrl, task: &CrawlTask, config: &CrawlerConfig) -> Result<Option<TaskResult>> {
        let start_time = std::time::Instant::now();
        
        info!("ğŸ” å¤„ç†URL: {} (æ·±åº¦: {}, ä¼˜å…ˆçº§: {})", 
              pending_url.url, pending_url.depth, pending_url.priority);

        // æ£€æŸ¥æ˜¯å¦å·²è®¿é—®
        if self.is_visited(&pending_url.url).await {
            debug!("â­ï¸ URLå·²è®¿é—®ï¼Œè·³è¿‡: {}", pending_url.url);
            return Ok(None);
        }

        // æ£€æŸ¥æ·±åº¦é™åˆ¶
        if pending_url.depth >= task.max_depth {
            debug!("ğŸ“ è¾¾åˆ°æœ€å¤§æ·±åº¦ï¼Œè·³è¿‡: {}", pending_url.url);
            return Ok(None);
        }

        // æ ‡è®°ä¸ºå·²è®¿é—®
        self.mark_as_visited(&pending_url.url).await;

        // è·å–é¡µé¢å†…å®¹
        let html_content = match self.fetch_page_content(&pending_url.url, config).await {
            Ok(content) => content,
            Err(e) => {
                warn!("ğŸ“„ æ— æ³•è·å–é¡µé¢å†…å®¹ {}: {}", pending_url.url, e);
                return Ok(None);
            }
        };

        // ç»¼åˆåˆ†æé¡µé¢
        let (relevance_analysis, content_regions, extracted_links) = self.web_analyzer
            .comprehensive_page_analysis(&html_content, &pending_url.url, task)
            .await?;

        // æ£€æŸ¥ç›¸å…³æ€§
        if relevance_analysis.relevance_score < config.min_relevance_score {
            info!("ğŸ“‰ ç›¸å…³æ€§åˆ†æ•°è¿‡ä½ ({:.2})ï¼Œè·³è¿‡é¡µé¢: {}", 
                  relevance_analysis.relevance_score, pending_url.url);
            self.increment_skipped_pages().await;
            return Ok(None);
        }

        // ç”Ÿæˆå†…å®¹æ‘˜è¦
        let content_summary = self.web_analyzer
            .generate_task_focused_summary(&content_regions, task)
            .await?;

        // å¤„ç†æå–çš„é“¾æ¥
        self.process_extracted_links(&extracted_links, &pending_url.url, pending_url.depth + 1).await;

        let processing_time = start_time.elapsed().as_millis() as u64;

        // æ›´æ–°ç»Ÿè®¡
        self.update_statistics(relevance_analysis.relevance_score, processing_time).await;

        info!("âœ… URLå¤„ç†å®Œæˆï¼Œç›¸å…³æ€§: {:.2}, å‘ç°é“¾æ¥: {}", 
              relevance_analysis.relevance_score, extracted_links.len());

        Ok(Some(TaskResult {
            task_id: task.task_id.clone(),
            url: pending_url.url.clone(),
            relevance_analysis,
            content_regions,
            content_summary,
            discovered_links_count: extracted_links.len(),
            processed_at: Utc::now(),
            processing_time_ms: processing_time,
        }))
    }

    /// è·å–é¡µé¢å†…å®¹
    async fn fetch_page_content(&self, url: &str, config: &CrawlerConfig) -> Result<String> {
        debug!("ğŸ“¥ è·å–é¡µé¢å†…å®¹: {}", url);

        let mut attempts = 0;
        while attempts < config.max_retries {
            match self.http_client.get(url).send().await {
                Ok(response) => {
                    if response.status().is_success() {
                        let content = response.text().await?;
                        debug!("âœ… æˆåŠŸè·å–é¡µé¢å†…å®¹ï¼Œé•¿åº¦: {} å­—ç¬¦", content.len());
                        return Ok(content);
                    } else {
                        warn!("ğŸš« HTTPé”™è¯¯: {} - {}", response.status(), url);
                    }
                }
                Err(e) => {
                    warn!("ğŸŒ ç½‘ç»œè¯·æ±‚å¤±è´¥ (å°è¯• {}/{}): {}", attempts + 1, config.max_retries, e);
                }
            }
            
            attempts += 1;
            if attempts < config.max_retries {
                sleep(std::time::Duration::from_millis(1000 * attempts as u64)).await;
            }
        }

        Err(anyhow::anyhow!("æ— æ³•è·å–é¡µé¢å†…å®¹ï¼Œå·²é‡è¯•{}æ¬¡", config.max_retries))
    }

    /// å¤„ç†æå–çš„é“¾æ¥
    async fn process_extracted_links(&self, links: &[ExtractedLink], parent_url: &str, depth: u32) {
        let mut state = self.crawl_state.write().await;
        let current_task = state.current_task.as_ref().unwrap();

        for link in links {
            // éªŒè¯å’Œè§„èŒƒåŒ–URL
            if let Ok(absolute_url) = self.normalize_url(&link.url, parent_url) {
                // é¿å…é‡å¤æ·»åŠ 
                if !state.visited_urls.contains(&absolute_url) && 
                   !state.pending_urls.iter().any(|p| p.url == absolute_url) {
                    
                    let pending_url = PendingUrl {
                        url: absolute_url,
                        priority: link.priority,
                        depth,
                        parent_url: Some(parent_url.to_string()),
                        discovered_at: Utc::now(),
                        expected_content_type: Some(format!("{:?}", link.link_type)),
                    };

                    // æŒ‰ä¼˜å…ˆçº§æ’å…¥é˜Ÿåˆ—
                    self.insert_by_priority(&mut state.pending_urls, pending_url);
                }
            }
        }

        debug!("ğŸ”— å¤„ç†äº†{}ä¸ªé“¾æ¥ï¼Œé˜Ÿåˆ—ä¸­æœ‰{}ä¸ªå¾…å¤„ç†URL", 
               links.len(), state.pending_urls.len());
    }

    /// æŒ‰ä¼˜å…ˆçº§æ’å…¥é˜Ÿåˆ—
    fn insert_by_priority(&self, queue: &mut VecDeque<PendingUrl>, new_url: PendingUrl) {
        let mut insert_index = queue.len();
        
        for (i, existing) in queue.iter().enumerate() {
            if new_url.priority > existing.priority || 
               (new_url.priority == existing.priority && new_url.depth < existing.depth) {
                insert_index = i;
                break;
            }
        }
        
        queue.insert(insert_index, new_url);
    }

    /// è§„èŒƒåŒ–URL
    fn normalize_url(&self, url: &str, base_url: &str) -> Result<String> {
        let base = Url::parse(base_url)?;
        let absolute = base.join(url)?;
        Ok(absolute.to_string())
    }

    /// å¾ªç¯æ£€æµ‹
    async fn detect_loop(&self, url: &str) -> bool {
        let mut state = self.crawl_state.write().await;
        
        let info = state.loop_detection.entry(url.to_string()).or_insert_with(|| {
            LoopDetectionInfo {
                visit_count: 0,
                first_visit: Utc::now(),
                last_visit: Utc::now(),
                visit_path: Vec::new(),
            }
        });

        info.visit_count += 1;
        info.last_visit = Utc::now();
        info.visit_path.push(url.to_string());

        // æ£€æŸ¥æ˜¯å¦è¶…è¿‡é˜ˆå€¼
        if info.visit_count > 3 {
            warn!("ğŸ”„ æ£€æµ‹åˆ°å¯èƒ½çš„å¾ªç¯: {} (è®¿é—®{}æ¬¡)", url, info.visit_count);
            return true;
        }

        // æ£€æŸ¥æ—¶é—´çª—å£å†…çš„é¢‘ç¹è®¿é—®
        let time_diff = info.last_visit.signed_duration_since(info.first_visit);
        if info.visit_count > 2 && time_diff < Duration::minutes(5) {
            warn!("â° æ£€æµ‹åˆ°çŸ­æ—¶é—´å†…é¢‘ç¹è®¿é—®: {}", url);
            return true;
        }

        false
    }

    /// è·å–ä¸‹ä¸€ä¸ªå¾…å¤„ç†çš„URL
    async fn get_next_url(&self) -> Option<PendingUrl> {
        let mut state = self.crawl_state.write().await;
        state.pending_urls.pop_front()
    }

    /// æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢çˆ¬è™«
    async fn should_stop_crawling(&self, task: &CrawlTask) -> bool {
        let state = self.crawl_state.read().await;
        
        // æ£€æŸ¥é¡µé¢æ•°é‡é™åˆ¶
        if state.statistics.total_pages_visited >= task.max_pages {
            return true;
        }

        // æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦ä¸ºç©º
        if state.pending_urls.is_empty() {
            return true;
        }

        // æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„ç›¸å…³ç»“æœ
        if state.statistics.relevant_pages_count >= 20 {
            return true;
        }

        false
    }

    /// æ ‡è®°URLä¸ºå·²è®¿é—®
    async fn mark_as_visited(&self, url: &str) {
        let mut state = self.crawl_state.write().await;
        state.visited_urls.insert(url.to_string());
    }

    /// æ£€æŸ¥URLæ˜¯å¦å·²è®¿é—®
    async fn is_visited(&self, url: &str) -> bool {
        let state = self.crawl_state.read().await;
        state.visited_urls.contains(url)
    }

    /// æ·»åŠ ä»»åŠ¡ç»“æœ
    async fn add_task_result(&self, result: TaskResult) {
        let mut state = self.crawl_state.write().await;
        state.task_results.push(result);
    }

    /// æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
    async fn update_statistics(&self, relevance_score: f32, processing_time: u64) {
        let mut state = self.crawl_state.write().await;
        
        state.statistics.total_pages_visited += 1;
        state.statistics.total_processing_time_ms += processing_time;
        
        if relevance_score >= 0.5 {
            state.statistics.relevant_pages_count += 1;
        }

        // è®¡ç®—å¹³å‡ç›¸å…³æ€§åˆ†æ•°
        let total_score = state.statistics.average_relevance_score * (state.statistics.total_pages_visited - 1) as f32 + relevance_score;
        state.statistics.average_relevance_score = total_score / state.statistics.total_pages_visited as f32;
    }

    /// å¢åŠ è·³è¿‡é¡µé¢è®¡æ•°
    async fn increment_skipped_pages(&self) {
        let mut state = self.crawl_state.write().await;
        state.statistics.skipped_pages_count += 1;
    }

    /// å¢åŠ å¾ªç¯æ£€æµ‹è®¡æ•°
    async fn increment_loop_detection(&self) {
        let mut state = self.crawl_state.write().await;
        state.statistics.loop_detections += 1;
    }

    /// æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    async fn print_statistics(&self) {
        let state = self.crawl_state.read().await;
        let stats = &state.statistics;
        
        info!("ğŸ“Š çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯:");
        info!("   æ€»é¡µé¢æ•°: {}", stats.total_pages_visited);
        info!("   ç›¸å…³é¡µé¢æ•°: {}", stats.relevant_pages_count);
        info!("   è·³è¿‡é¡µé¢æ•°: {}", stats.skipped_pages_count);
        info!("   å¾ªç¯æ£€æµ‹æ¬¡æ•°: {}", stats.loop_detections);
        info!("   å¹³å‡ç›¸å…³æ€§åˆ†æ•°: {:.2}", stats.average_relevance_score);
        info!("   æ€»å¤„ç†æ—¶é—´: {}ms", stats.total_processing_time_ms);
        
        if let Some(end_time) = stats.end_time {
            let duration = end_time.signed_duration_since(stats.start_time);
            info!("   æ€»è€—æ—¶: {}ç§’", duration.num_seconds());
        }
    }

    /// è·å–çˆ¬è™«ç»Ÿè®¡
    pub async fn get_statistics(&self) -> CrawlStatistics {
        let state = self.crawl_state.read().await;
        state.statistics.clone()
    }

    /// åœæ­¢çˆ¬è™«
    pub async fn stop_crawling(&self) {
        let mut state = self.crawl_state.write().await;
        state.pending_urls.clear();
        state.statistics.end_time = Some(Utc::now());
        info!("â¹ï¸ çˆ¬è™«å·²æ‰‹åŠ¨åœæ­¢");
    }

    /// è·å–ä»»åŠ¡ç»“æœ
    pub async fn get_task_results(&self) -> Vec<TaskResult> {
        let state = self.crawl_state.read().await;
        state.task_results.clone()
    }

    /// æ¸…ç†ç¼“å­˜
    pub async fn clear_cache(&self) {
        self.web_analyzer.clear_cache().await;
        info!("ğŸ§¹ æ™ºèƒ½çˆ¬è™«ç¼“å­˜å·²æ¸…ç†");
    }
} 