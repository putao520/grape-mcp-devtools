use anyhow::Result;
use reqwest::Client;
use serde_json::{json, Value};
use std::env;
use tracing::{info, warn, debug};
use tokio::time::{timeout, Duration};

/// AIæœåŠ¡é…ç½®
#[derive(Debug, Clone)]
pub struct AIServiceConfig {
    /// APIåŸºç¡€URL
    pub api_base: String,
    /// APIå¯†é’¥
    pub api_key: String,
    /// é»˜è®¤æ¨¡å‹
    pub default_model: String,
    /// è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    pub timeout_secs: u64,
    /// æœ€å¤§é‡è¯•æ¬¡æ•°
    pub max_retries: u32,
    /// æ˜¯å¦å¯ç”¨ç¼“å­˜
    pub enable_cache: bool,
    /// ç¼“å­˜TTLï¼ˆç§’ï¼‰
    pub cache_ttl_secs: u64,
}

impl Default for AIServiceConfig {
    fn default() -> Self {
        // åŠ è½½ç¯å¢ƒå˜é‡
        dotenv::dotenv().ok();
        
        Self {
            api_base: env::var("LLM_API_BASE_URL")
                .unwrap_or_else(|_| "https://integrate.api.nvidia.com/v1".to_string()),
            api_key: env::var("LLM_API_KEY")
                .expect("LLM_API_KEY environment variable is required"),
            default_model: env::var("LLM_MODEL_NAME")
                .unwrap_or_else(|_| "nvidia/llama-3.1-nemotron-70b-instruct".to_string()),
            timeout_secs: env::var("AI_TIMEOUT_SECS")
                .unwrap_or_else(|_| "30".to_string())
                .parse().unwrap_or(30),
            max_retries: env::var("AI_MAX_RETRIES")
                .unwrap_or_else(|_| "3".to_string())
                .parse().unwrap_or(3),
            enable_cache: env::var("AI_ENABLE_CACHE")
                .unwrap_or_else(|_| "true".to_string())
                .parse().unwrap_or(true),
            cache_ttl_secs: env::var("AI_CACHE_TTL_SECS")
                .unwrap_or_else(|_| "3600".to_string())
                .parse().unwrap_or(3600),
        }
    }
}

/// AIè¯·æ±‚å‚æ•°
#[derive(Debug, Clone)]
pub struct AIRequest {
    /// æ¨¡å‹åç§°
    pub model: Option<String>,
    /// ç³»ç»Ÿæç¤º
    pub system_prompt: Option<String>,
    /// ç”¨æˆ·æ¶ˆæ¯
    pub user_message: String,
    /// æ¸©åº¦å‚æ•°
    pub temperature: Option<f32>,
    /// æœ€å¤§tokenæ•°
    pub max_tokens: Option<u32>,
    /// æ˜¯å¦æµå¼å“åº”
    pub stream: bool,
}

/// AIå“åº”ç»“æœ
#[derive(Debug, Clone)]
pub struct AIResponse {
    /// å“åº”å†…å®¹
    pub content: String,
    /// ä½¿ç”¨çš„æ¨¡å‹
    pub model: String,
    /// æ¶ˆè€—çš„tokenæ•°
    pub tokens_used: Option<u32>,
    /// å“åº”æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub response_time_ms: u64,
    /// æ˜¯å¦æ¥è‡ªç¼“å­˜
    pub from_cache: bool,
}

/// AIæœåŠ¡æ ¸å¿ƒå®ç°
#[derive(Clone)]
pub struct AIService {
    config: AIServiceConfig,
    client: Client,
    cache: std::sync::Arc<tokio::sync::RwLock<std::collections::HashMap<String, CachedResponse>>>,
}

/// ç¼“å­˜å“åº”
#[derive(Debug, Clone)]
struct CachedResponse {
    response: AIResponse,
    timestamp: chrono::DateTime<chrono::Utc>,
}

impl AIService {
    /// åˆ›å»ºæ–°çš„AIæœåŠ¡å®ä¾‹
    pub fn new(config: AIServiceConfig) -> Result<Self> {
        let client = Client::builder()
            .timeout(Duration::from_secs(config.timeout_secs))
            .build()?;

        info!("ğŸ¤– åˆå§‹åŒ–AIæœåŠ¡");
        info!("API Base: {}", config.api_base);
        info!("é»˜è®¤æ¨¡å‹: {}", config.default_model);
        info!("ç¼“å­˜å¯ç”¨: {}", config.enable_cache);

        Ok(Self {
            config,
            client,
            cache: std::sync::Arc::new(tokio::sync::RwLock::new(std::collections::HashMap::new())),
        })
    }

    /// ä»ç¯å¢ƒå˜é‡åˆ›å»ºAIæœåŠ¡
    pub fn from_env() -> Result<Self> {
        let config = AIServiceConfig::default();
        Self::new(config)
    }

    /// å‘é€AIè¯·æ±‚
    pub async fn request(&self, request: AIRequest) -> Result<AIResponse> {
        let start_time = std::time::Instant::now();
        
        // æ£€æŸ¥ç¼“å­˜
        if self.config.enable_cache {
            let cache_key = self.generate_cache_key(&request);
            if let Some(cached) = self.get_cached_response(&cache_key).await {
                debug!("ğŸ¯ ä½¿ç”¨ç¼“å­˜çš„AIå“åº”");
                return Ok(cached);
            }
        }

        // å‘é€è¯·æ±‚
        let response = self.send_request_with_retry(&request).await?;
        
        // ç¼“å­˜å“åº”
        if self.config.enable_cache {
            let cache_key = self.generate_cache_key(&request);
            self.cache_response(&cache_key, &response).await;
        }

        let elapsed = start_time.elapsed().as_millis() as u64;
        debug!("ğŸ¤– AIè¯·æ±‚å®Œæˆï¼Œè€—æ—¶: {}ms", elapsed);

        Ok(AIResponse {
            response_time_ms: elapsed,
            from_cache: false,
            ..response
        })
    }

    /// å¸¦é‡è¯•çš„è¯·æ±‚å‘é€
    async fn send_request_with_retry(&self, request: &AIRequest) -> Result<AIResponse> {
        let mut last_error = None;
        
        for attempt in 1..=self.config.max_retries {
            match self.send_single_request(request).await {
                Ok(response) => {
                    if attempt > 1 {
                        info!("âœ… AIè¯·æ±‚é‡è¯•æˆåŠŸ (ç¬¬{}æ¬¡å°è¯•)", attempt);
                    }
                    return Ok(response);
                }
                Err(e) => {
                    last_error = Some(e);
                    if attempt < self.config.max_retries {
                        warn!("âš ï¸ AIè¯·æ±‚å¤±è´¥ï¼Œå°†é‡è¯• (ç¬¬{}æ¬¡å°è¯•): {}", attempt, last_error.as_ref().unwrap());
                        tokio::time::sleep(Duration::from_millis(1000 * attempt as u64)).await;
                    }
                }
            }
        }
        
        Err(last_error.unwrap())
    }

    /// å‘é€å•æ¬¡è¯·æ±‚
    async fn send_single_request(&self, request: &AIRequest) -> Result<AIResponse> {
        let model = request.model.as_ref()
            .unwrap_or(&self.config.default_model);

        let mut messages = Vec::new();
        
        // æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
        if let Some(system_prompt) = &request.system_prompt {
            messages.push(json!({
                "role": "system",
                "content": system_prompt
            }));
        }
        
        // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        messages.push(json!({
            "role": "user",
            "content": request.user_message
        }));

        let mut request_body = json!({
            "model": model,
            "messages": messages,
            "stream": request.stream
        });

        // æ·»åŠ å¯é€‰å‚æ•°
        if let Some(temperature) = request.temperature {
            request_body["temperature"] = json!(temperature);
        }
        if let Some(max_tokens) = request.max_tokens {
            request_body["max_tokens"] = json!(max_tokens);
        }

        let response = timeout(
            Duration::from_secs(self.config.timeout_secs),
            self.client
                .post(&format!("{}/chat/completions", self.config.api_base))
                .header("Authorization", format!("Bearer {}", self.config.api_key))
                .header("Content-Type", "application/json")
                .json(&request_body)
                .send()
        ).await??;

        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await?;
            return Err(anyhow::anyhow!("AI APIè°ƒç”¨å¤±è´¥: {} - {}", status, error_text));
        }

        let response_json: Value = response.json().await?;
        
        // è§£æå“åº”
        let content = response_json
            .get("choices")
            .and_then(|choices| choices.as_array())
            .and_then(|arr| arr.first())
            .and_then(|choice| choice.get("message"))
            .and_then(|message| message.get("content"))
            .and_then(|content| content.as_str())
            .ok_or_else(|| anyhow::anyhow!("æ— æ•ˆçš„AIå“åº”æ ¼å¼"))?
            .to_string();

        let tokens_used = response_json
            .get("usage")
            .and_then(|usage| usage.get("total_tokens"))
            .and_then(|tokens| tokens.as_u64())
            .map(|t| t as u32);

        Ok(AIResponse {
            content,
            model: model.clone(),
            tokens_used,
            response_time_ms: 0, // å°†åœ¨ä¸Šå±‚è®¾ç½®
            from_cache: false,
        })
    }

    /// ç”Ÿæˆç¼“å­˜é”®
    fn generate_cache_key(&self, request: &AIRequest) -> String {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};

        let mut hasher = DefaultHasher::new();
        request.user_message.hash(&mut hasher);
        request.system_prompt.hash(&mut hasher);
        request.model.hash(&mut hasher);
        request.temperature.map(|t| (t * 1000.0) as u32).hash(&mut hasher);
        request.max_tokens.hash(&mut hasher);
        
        format!("ai_cache_{:x}", hasher.finish())
    }

    /// è·å–ç¼“å­˜å“åº”
    async fn get_cached_response(&self, cache_key: &str) -> Option<AIResponse> {
        let cache = self.cache.read().await;
        
        if let Some(cached) = cache.get(cache_key) {
            let age = chrono::Utc::now().signed_duration_since(cached.timestamp);
            if age.num_seconds() < self.config.cache_ttl_secs as i64 {
                let mut response = cached.response.clone();
                response.from_cache = true;
                return Some(response);
            }
        }
        
        None
    }

    /// ç¼“å­˜å“åº”
    async fn cache_response(&self, cache_key: &str, response: &AIResponse) {
        let mut cache = self.cache.write().await;
        
        cache.insert(cache_key.to_string(), CachedResponse {
            response: response.clone(),
            timestamp: chrono::Utc::now(),
        });
        
        // æ¸…ç†è¿‡æœŸç¼“å­˜
        let now = chrono::Utc::now();
        cache.retain(|_, cached| {
            let age = now.signed_duration_since(cached.timestamp);
            age.num_seconds() < self.config.cache_ttl_secs as i64
        });
    }

    /// æ¸…ç†ç¼“å­˜
    pub async fn clear_cache(&self) {
        let mut cache = self.cache.write().await;
        cache.clear();
        info!("ğŸ§¹ AIæœåŠ¡ç¼“å­˜å·²æ¸…ç†");
    }

    /// è·å–ç¼“å­˜ç»Ÿè®¡
    pub async fn get_cache_stats(&self) -> (usize, usize) {
        let cache = self.cache.read().await;
        let total = cache.len();
        let now = chrono::Utc::now();
        let valid = cache.values().filter(|cached| {
            let age = now.signed_duration_since(cached.timestamp);
            age.num_seconds() < self.config.cache_ttl_secs as i64
        }).count();
        
        (total, valid)
    }

    /// å¥åº·æ£€æŸ¥
    pub async fn health_check(&self) -> Result<bool> {
        let test_request = AIRequest {
            model: None,
            system_prompt: Some("You are a helpful assistant.".to_string()),
            user_message: "Hello, this is a health check. Please respond with 'OK'.".to_string(),
            temperature: Some(0.1),
            max_tokens: Some(10),
            stream: false,
        };

        match timeout(Duration::from_secs(10), self.send_single_request(&test_request)).await {
            Ok(Ok(response)) => {
                debug!("ğŸ¥ AIæœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡: {}", response.content);
                Ok(true)
            }
            Ok(Err(e)) => {
                warn!("ğŸ¥ AIæœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥: {}", e);
                Ok(false)
            }
            Err(_) => {
                warn!("ğŸ¥ AIæœåŠ¡å¥åº·æ£€æŸ¥è¶…æ—¶");
                Ok(false)
            }
        }
    }
} 