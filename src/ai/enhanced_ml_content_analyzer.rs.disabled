use anyhow::Result;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use crate::ai::ai_service::{AIService, AIRequest};
use crate::tools::docs::openai_vectorizer::OpenAIVectorizer;
use crate::tools::docs::doc_traits::DocumentVectorizer;

/// å¢å¼ºå‹MLå†…å®¹åˆ†æå™¨ï¼Œå……åˆ†åˆ©ç”¨é¡¹ç›®å·²æœ‰çš„AIæœåŠ¡
pub struct EnhancedMLContentAnalyzer {
    /// AIæœåŠ¡å®ä¾‹ï¼Œç”¨äºè¯­ä¹‰åˆ†æ
    ai_service: AIService,
    /// å‘é‡åŒ–å™¨ï¼Œç”¨äºè¯­ä¹‰åµŒå…¥
    vectorizer: OpenAIVectorizer,
    /// ç»Ÿè®¡åˆ†æå™¨ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
    fallback_analyzer: Option<super::ml_content_analyzer::MLContentAnalyzer>,
}

/// å¢å¼ºå‹åˆ†æç»“æœï¼ŒåŒ…å«AIé©±åŠ¨çš„æ´å¯Ÿ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EnhancedAnalysisResult {
    /// åŸºç¡€ç‰¹å¾
    pub basic_features: super::ml_content_analyzer::DocumentFeatures,
    /// AIç”Ÿæˆçš„è´¨é‡è¯„åˆ†å’Œè¯¦ç»†åˆ†æ
    pub ai_quality_analysis: AIQualityAnalysis,
    /// è¯­ä¹‰å‘é‡è¡¨ç¤º
    pub semantic_embedding: Vec<f32>,
    /// AIæå–çš„ä¸»é¢˜å’Œæ ‡ç­¾
    pub ai_topics: Vec<String>,
    /// AIç”Ÿæˆçš„æ”¹è¿›å»ºè®®
    pub ai_recommendations: Vec<String>,
    /// è¯­ä¹‰ç›¸ä¼¼åº¦å¾—åˆ†ï¼ˆä¸æŸ¥è¯¢çš„åŒ¹é…ç¨‹åº¦ï¼‰
    pub semantic_relevance: f32,
    /// ç»¼åˆå¾—åˆ†ï¼ˆç»“åˆç»Ÿè®¡å’ŒAIåˆ†æï¼‰
    pub composite_score: f64,
}

/// AIé©±åŠ¨çš„è´¨é‡åˆ†æ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AIQualityAnalysis {
    /// å†…å®¹è´¨é‡å¾—åˆ† (1-10)
    pub quality_score: f64,
    /// æ¸…æ™°åº¦è¯„ä¼°
    pub clarity_score: f64,
    /// æŠ€æœ¯æ·±åº¦è¯„ä¼°
    pub technical_depth: f64,
    /// å®ç”¨æ€§è¯„ä¼°
    pub usefulness_score: f64,
    /// AIç”Ÿæˆçš„è´¨é‡è¯„ä¼°è¯¦æƒ…
    pub quality_explanation: String,
}

impl EnhancedMLContentAnalyzer {
    /// åˆ›å»ºæ–°çš„å¢å¼ºå‹åˆ†æå™¨
    pub async fn new() -> Result<Self> {
        let ai_service = AIService::from_env()?;
        let vectorizer = OpenAIVectorizer::from_env()?;
        let fallback_analyzer = Some(super::ml_content_analyzer::MLContentAnalyzer::new());

        Ok(Self {
            ai_service,
            vectorizer,
            fallback_analyzer,
        })
    }

    /// æ‰§è¡Œå…¨é¢çš„å†…å®¹åˆ†æ
    pub async fn analyze_content_enhanced(
        &self,
        content: &str,
        context: Option<&str>,
        target_query: Option<&str>
    ) -> Result<EnhancedAnalysisResult> {
        tracing::info!("ğŸ¤– å¼€å§‹AIå¢å¼ºå†…å®¹åˆ†æï¼Œå†…å®¹é•¿åº¦: {} å­—ç¬¦", content.len());

        // 1. åŸºç¡€ç»Ÿè®¡åˆ†æï¼ˆä½œä¸ºåŸºå‡†ï¼‰
        let basic_features = self.extract_basic_features(content).await?;

        // 2. AIé©±åŠ¨çš„è´¨é‡åˆ†æ
        let ai_quality_analysis = self.ai_quality_analysis(content, context).await?;

        // 3. ç”Ÿæˆè¯­ä¹‰åµŒå…¥å‘é‡
        let semantic_embedding = self.vectorizer.vectorize(content).await?;

        // 4. AIä¸»é¢˜æå–
        let ai_topics = self.ai_topic_extraction(content).await?;

        // 5. AIç”Ÿæˆæ”¹è¿›å»ºè®®
        let ai_recommendations = self.ai_generate_recommendations(content, &ai_quality_analysis).await?;

        // 6. è®¡ç®—è¯­ä¹‰ç›¸å…³æ€§ï¼ˆå¦‚æœæä¾›äº†ç›®æ ‡æŸ¥è¯¢ï¼‰
        let semantic_relevance = if let Some(query) = target_query {
            self.calculate_semantic_relevance(content, query).await?
        } else {
            0.5 // é»˜è®¤ä¸­ç­‰ç›¸å…³æ€§
        };

        // 7. è®¡ç®—ç»¼åˆå¾—åˆ†
        let composite_score = self.calculate_composite_score(
            &basic_features,
            &ai_quality_analysis,
            semantic_relevance
        );

        Ok(EnhancedAnalysisResult {
            basic_features,
            ai_quality_analysis,
            semantic_embedding,
            ai_topics,
            ai_recommendations,
            semantic_relevance,
            composite_score,
        })
    }

    /// AIé©±åŠ¨çš„è´¨é‡åˆ†æ
    async fn ai_quality_analysis(&self, content: &str, context: Option<&str>) -> Result<AIQualityAnalysis> {
        let system_prompt = r#"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹å‡ ä¸ªç»´åº¦åˆ†æç»™å®šçš„æŠ€æœ¯æ–‡æ¡£ï¼š

1. å†…å®¹è´¨é‡ (1-10åˆ†): ä¿¡æ¯çš„å‡†ç¡®æ€§ã€å®Œæ•´æ€§å’Œä»·å€¼
2. æ¸…æ™°åº¦ (1-10åˆ†): è¡¨è¾¾çš„æ¸…æ™°ç¨‹åº¦å’Œæ˜“ç†è§£æ€§  
3. æŠ€æœ¯æ·±åº¦ (1-10åˆ†): æŠ€æœ¯å†…å®¹çš„æ·±åº¦å’Œä¸“ä¸šæ€§
4. å®ç”¨æ€§ (1-10åˆ†): å¯¹è¯»è€…çš„å®é™…å¸®åŠ©ç¨‹åº¦

è¯·æä¾›JSONæ ¼å¼çš„è¯„ä¼°ç»“æœï¼ŒåŒ…å«å…·ä½“åˆ†æ•°å’Œè¯¦ç»†è§£é‡Šã€‚"#;

        let context_info = if let Some(ctx) = context {
            format!("\n\nä¸Šä¸‹æ–‡ä¿¡æ¯: {}", ctx)
        } else {
            String::new()
        };

        let user_message = format!(
            "è¯·åˆ†æä»¥ä¸‹æŠ€æœ¯æ–‡æ¡£ï¼š\n\n{}{}\n\nè¯·æä¾›JSONæ ¼å¼çš„è¯„ä¼°ç»“æœã€‚",
            content, context_info
        );

        let ai_request = AIRequest {
            model: None, // ä½¿ç”¨é»˜è®¤æ¨¡å‹
            system_prompt: Some(system_prompt.to_string()),
            user_message,
            temperature: Some(0.3), // è¾ƒä½æ¸©åº¦ä¿è¯ä¸€è‡´æ€§
            max_tokens: Some(1000),
            stream: false,
        };

        let ai_response = self.ai_service.request(ai_request).await?;
        
        // è§£æAIå“åº”
        self.parse_ai_quality_response(&ai_response.content)
    }

    /// è§£æAIè´¨é‡åˆ†æå“åº”
    fn parse_ai_quality_response(&self, response: &str) -> Result<AIQualityAnalysis> {
        // å°è¯•æå–JSONå†…å®¹
        if let Some(json_start) = response.find('{') {
            if let Some(json_end) = response.rfind('}') {
                let json_str = &response[json_start..=json_end];
                if let Ok(parsed) = serde_json::from_str::<serde_json::Value>(json_str) {
                    return Ok(AIQualityAnalysis {
                        quality_score: parsed["quality_score"].as_f64().unwrap_or(5.0),
                        clarity_score: parsed["clarity_score"].as_f64().unwrap_or(5.0),
                        technical_depth: parsed["technical_depth"].as_f64().unwrap_or(5.0),
                        usefulness_score: parsed["usefulness_score"].as_f64().unwrap_or(5.0),
                        quality_explanation: parsed["explanation"].as_str()
                            .unwrap_or("AIåˆ†æç»“æœ")
                            .to_string(),
                    });
                }
            }
        }

        // å¦‚æœJSONè§£æå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬åˆ†æä½œä¸ºå¤‡é€‰
        self.fallback_parse_quality_response(response)
    }

    /// å¤‡é€‰çš„è´¨é‡å“åº”è§£æ
    fn fallback_parse_quality_response(&self, response: &str) -> Result<AIQualityAnalysis> {
        // ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æˆ–å…³é”®è¯æå–åˆ†æ•°
        let quality_score = self.extract_score_from_text(response, "è´¨é‡").unwrap_or(6.0);
        let clarity_score = self.extract_score_from_text(response, "æ¸…æ™°").unwrap_or(6.0);
        let technical_depth = self.extract_score_from_text(response, "æŠ€æœ¯æ·±åº¦").unwrap_or(6.0);
        let usefulness_score = self.extract_score_from_text(response, "å®ç”¨").unwrap_or(6.0);

        Ok(AIQualityAnalysis {
            quality_score,
            clarity_score,
            technical_depth,
            usefulness_score,
            quality_explanation: response.to_string(),
        })
    }

    /// ä»æ–‡æœ¬ä¸­æå–åˆ†æ•°
    fn extract_score_from_text(&self, text: &str, keyword: &str) -> Option<f64> {
        let text_lower = text.to_lowercase();
        let keyword_lower = keyword.to_lowercase();
        
        // æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„å¥å­ï¼Œç„¶åæå–æ•°å­—
        if let Some(pos) = text_lower.find(&keyword_lower) {
            let search_area = &text_lower[pos..std::cmp::min(pos + 100, text_lower.len())];
            
            // æŸ¥æ‰¾æ•°å­—æ¨¡å¼ X/10, Xåˆ†, X.Xç­‰
            use regex::Regex;
            if let Ok(re) = Regex::new(r"(\d+(?:\.\d+)?)\s*[/åˆ†]?\s*(?:10|åˆ†|ç‚¹)") {
                if let Some(captures) = re.find(search_area) {
                    if let Ok(score) = captures.as_str().split_whitespace()
                        .find(|s| s.chars().any(|c| c.is_ascii_digit()))
                        .unwrap_or("0")
                        .trim_end_matches(|c: char| !c.is_ascii_digit() && c != '.')
                        .parse::<f64>() {
                        return Some(score.min(10.0).max(1.0));
                    }
                }
            }
        }
        
        None
    }

    /// AIä¸»é¢˜æå–
    async fn ai_topic_extraction(&self, content: &str) -> Result<Vec<String>> {
        let system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£ä¸»é¢˜æå–ä¸“å®¶ã€‚è¯·ä»ç»™å®šçš„æŠ€æœ¯æ–‡æ¡£ä¸­æå–5-10ä¸ªæœ€é‡è¦çš„ä¸»é¢˜æ ‡ç­¾ã€‚è¿”å›JSONæ•°ç»„æ ¼å¼çš„ç»“æœã€‚";

        let user_message = format!(
            "è¯·ä»ä»¥ä¸‹æŠ€æœ¯æ–‡æ¡£ä¸­æå–ä¸»è¦ä¸»é¢˜æ ‡ç­¾ï¼š\n\n{}\n\nè¯·è¿”å›JSONæ•°ç»„æ ¼å¼ï¼Œä¾‹å¦‚ï¼š[\"Rustç¼–ç¨‹\", \"Webå¼€å‘\", \"APIè®¾è®¡\"]",
            content
        );

        let ai_request = AIRequest {
            model: None,
            system_prompt: Some(system_prompt.to_string()),
            user_message,
            temperature: Some(0.2),
            max_tokens: Some(500),
            stream: false,
        };

        let ai_response = self.ai_service.request(ai_request).await?;
        self.parse_ai_topics_response(&ai_response.content)
    }

    /// è§£æAIä¸»é¢˜å“åº”
    fn parse_ai_topics_response(&self, response: &str) -> Result<Vec<String>> {
        // å°è¯•è§£æJSONæ•°ç»„
        if let Some(json_start) = response.find('[') {
            if let Some(json_end) = response.rfind(']') {
                let json_str = &response[json_start..=json_end];
                if let Ok(topics) = serde_json::from_str::<Vec<String>>(json_str) {
                    return Ok(topics);
                }
            }
        }

        // å¤‡é€‰æ–¹æ¡ˆï¼šä»æ–‡æœ¬ä¸­æå–ä¸»é¢˜
        let topics = response
            .lines()
            .filter_map(|line| {
                let trimmed = line.trim();
                if trimmed.len() > 2 && trimmed.len() < 50 {
                    Some(trimmed.trim_matches(|c: char| !c.is_alphanumeric() && !c.is_whitespace()).to_string())
                } else {
                    None
                }
            })
            .take(10)
            .collect();

        Ok(topics)
    }

    /// AIç”Ÿæˆæ”¹è¿›å»ºè®®
    async fn ai_generate_recommendations(
        &self,
        content: &str,
        quality_analysis: &AIQualityAnalysis
    ) -> Result<Vec<String>> {
        let system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯å†™ä½œé¡¾é—®ã€‚åŸºäºæ–‡æ¡£è´¨é‡åˆ†æç»“æœï¼Œè¯·æä¾›3-5æ¡å…·ä½“çš„æ”¹è¿›å»ºè®®ã€‚";

        let user_message = format!(
            "æ–‡æ¡£å†…å®¹ï¼š\n{}\n\nè´¨é‡åˆ†æï¼š\n- å†…å®¹è´¨é‡: {}/10\n- æ¸…æ™°åº¦: {}/10\n- æŠ€æœ¯æ·±åº¦: {}/10\n- å®ç”¨æ€§: {}/10\n\nè¯·æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®ï¼ˆJSONæ•°ç»„æ ¼å¼ï¼‰ã€‚",
            content,
            quality_analysis.quality_score,
            quality_analysis.clarity_score,
            quality_analysis.technical_depth,
            quality_analysis.usefulness_score
        );

        let ai_request = AIRequest {
            model: None,
            system_prompt: Some(system_prompt.to_string()),
            user_message,
            temperature: Some(0.4),
            max_tokens: Some(800),
            stream: false,
        };

        let ai_response = self.ai_service.request(ai_request).await?;
        self.parse_ai_recommendations_response(&ai_response.content)
    }

    /// è§£æAIå»ºè®®å“åº”
    fn parse_ai_recommendations_response(&self, response: &str) -> Result<Vec<String>> {
        // ç±»ä¼¼ä¸»é¢˜è§£æçš„é€»è¾‘
        if let Some(json_start) = response.find('[') {
            if let Some(json_end) = response.rfind(']') {
                let json_str = &response[json_start..=json_end];
                if let Ok(recommendations) = serde_json::from_str::<Vec<String>>(json_str) {
                    return Ok(recommendations);
                }
            }
        }

        // å¤‡é€‰ï¼šæå–ç¼–å·åˆ—è¡¨
        let recommendations = response
            .lines()
            .filter_map(|line| {
                let trimmed = line.trim();
                if trimmed.len() > 10 && (
                    trimmed.starts_with("1.") || 
                    trimmed.starts_with("2.") || 
                    trimmed.starts_with("3.") ||
                    trimmed.starts_with("4.") ||
                    trimmed.starts_with("5.") ||
                    trimmed.starts_with("-") ||
                    trimmed.starts_with("â€¢")
                ) {
                    Some(trimmed.trim_start_matches(|c: char| c.is_numeric() || c == '.' || c == '-' || c == 'â€¢').trim().to_string())
                } else {
                    None
                }
            })
            .take(5)
            .collect();

        Ok(recommendations)
    }

    /// è®¡ç®—è¯­ä¹‰ç›¸å…³æ€§
    async fn calculate_semantic_relevance(&self, content: &str, query: &str) -> Result<f32> {
        let content_embedding = self.vectorizer.vectorize(content).await?;
        let query_embedding = self.vectorizer.vectorize(query).await?;
        
        Ok(self.vectorizer.calculate_similarity(&content_embedding, &query_embedding))
    }

    /// æå–åŸºç¡€ç‰¹å¾ï¼ˆå¤ç”¨ç°æœ‰å®ç°ï¼‰
    async fn extract_basic_features(&self, content: &str) -> Result<super::ml_content_analyzer::DocumentFeatures> {
        if let Some(ref fallback) = self.fallback_analyzer {
            let result = fallback.analyze_content(content, None).await?;
            Ok(result.features)
        } else {
            // å¦‚æœæ²¡æœ‰å¤‡é€‰åˆ†æå™¨ï¼Œåˆ›å»ºåŸºç¡€ç‰¹å¾
            Ok(super::ml_content_analyzer::DocumentFeatures {
                word_count: content.split_whitespace().count(),
                sentence_count: content.matches(&['.', '!', '?'][..]).count(),
                paragraph_count: content.split("\n\n").count(),
                avg_word_length: content.split_whitespace()
                    .map(|w| w.len())
                    .sum::<usize>() as f64 / content.split_whitespace().count().max(1) as f64,
                avg_sentence_length: content.split_whitespace().count() as f64 / content.matches(&['.', '!', '?'][..]).count().max(1) as f64,
                code_block_count: content.matches("```").count() / 2,
                link_count: content.matches("http").count(),
                heading_count: content.matches('#').count(),
                complexity_score: content.len() as f64 / 100.0, // ç®€åŒ–è®¡ç®—
                readability_score: 50.0, // é»˜è®¤å€¼
            })
        }
    }

    /// è®¡ç®—ç»¼åˆå¾—åˆ†
    fn calculate_composite_score(
        &self,
        basic_features: &super::ml_content_analyzer::DocumentFeatures,
        ai_analysis: &AIQualityAnalysis,
        semantic_relevance: f32
    ) -> f64 {
        // æƒé‡é…ç½®
        let basic_weight = 0.2;  // åŸºç¡€ç»Ÿè®¡ç‰¹å¾æƒé‡
        let ai_weight = 0.6;     // AIåˆ†ææƒé‡
        let semantic_weight = 0.2; // è¯­ä¹‰ç›¸å…³æ€§æƒé‡

        // åŸºç¡€å¾—åˆ†ï¼ˆæ ‡å‡†åŒ–åˆ°0-10ï¼‰
        let basic_score = (basic_features.word_count.min(1000) as f64 / 100.0 +
                          basic_features.code_block_count as f64 +
                          basic_features.heading_count as f64).min(10.0);

        // AIå¾—åˆ†ï¼ˆå·²ç»æ˜¯0-10èŒƒå›´ï¼‰
        let ai_score = (ai_analysis.quality_score + 
                       ai_analysis.clarity_score + 
                       ai_analysis.technical_depth + 
                       ai_analysis.usefulness_score) / 4.0;

        // è¯­ä¹‰å¾—åˆ†ï¼ˆ0-1èŒƒå›´ï¼Œè½¬æ¢ä¸º0-10ï¼‰
        let semantic_score = semantic_relevance as f64 * 10.0;

        // è®¡ç®—åŠ æƒç»¼åˆå¾—åˆ†
        let composite = basic_score * basic_weight + 
                       ai_score * ai_weight + 
                       semantic_score * semantic_weight;

        composite.min(10.0).max(0.0)
    }
} 