use anyhow::Result;
use tracing::{info, debug, warn, error};
use std::collections::{HashMap, HashSet, VecDeque};
use std::sync::Arc;
use tokio::sync::RwLock;
use serde_json;
use regex;
use url;

use crate::ai::ai_service::{AIService, AIRequest, AIServiceConfig};
use crate::ai::intelligent_web_analyzer::{CrawlTask, ContentType, PageRelevanceAnalysis, ExtractedLink};
use crate::ai::smart_url_crawler::{CrawlerConfig, TaskResult as BasicTaskResult}; // Renaming to avoid conflict

// --- æ–°çš„æ ¸å¿ƒç»“æ„ä½“å®šä¹‰ ---

/// é«˜çº§æ™ºèƒ½çˆ¬è™«ï¼Œç”¨äºæ·±åº¦å†…å®¹æå–å’ŒçŸ¥è¯†æ„å»º
pub struct AdvancedIntelligentCrawler {
    ai_service: Arc<AIService>,
    config: Arc<CrawlerConfig>,
    // æœªæ¥å¯ä»¥æ·»åŠ æ›´å¤šé…ç½®ï¼Œå¦‚ç‰¹å®šäºé«˜çº§çˆ¬è™«çš„å‚æ•°
}

/// URLå‘ç°ä»£ç†ï¼Œè´Ÿè´£æ™ºèƒ½åœ°å‘ç°å’Œä¼˜å…ˆå¤„ç†URL
struct UrlDiscoveryAgent {
    ai_service: Arc<AIService>,
    task: Arc<CrawlTask>,
    visited_urls: Arc<RwLock<HashSet<String>>>,
    pending_urls: Arc<RwLock<VecDeque<PrioritizedUrl>>>,
    config: Arc<CrawlerConfig>,
}

/// å†…å®¹æå–ä»£ç†ï¼Œè´Ÿè´£ä»å•ä¸ªé¡µé¢æå–ç»“æ„åŒ–ä¿¡æ¯
struct ContentExtractionAgent {
    ai_service: Arc<AIService>,
    task: Arc<CrawlTask>,
    config: Arc<CrawlerConfig>,
}

/// çŸ¥è¯†èšåˆå™¨ï¼Œè´Ÿè´£å°†æ¥è‡ªå¤šä¸ªé¡µé¢çš„ç‰‡æ®µèšåˆæˆç»¼åˆæ–‡æ¡£
struct KnowledgeAggregator {
    ai_service: Arc<AIService>,
    task: Arc<CrawlTask>,
    collected_fragments: Arc<RwLock<Vec<ContentFragment>>>,
}

/// å¸¦æœ‰ä¼˜å…ˆçº§çš„URLç»“æ„
#[derive(Debug, Clone)]
struct PrioritizedUrl {
    url: String,
    priority: f32, // 0.0 - 1.0, AIè¯„ä¼°çš„ä¼˜å…ˆçº§
    depth: u32,
    source_page_url: Option<String>, // ä»å“ªä¸ªé¡µé¢å‘ç°çš„æ­¤URL
}

/// ä»é¡µé¢æå–çš„å†…å®¹ç‰‡æ®µ
#[derive(Debug, Clone)]
pub struct ContentFragment {
    pub source_url: String,
    pub fragment_type: ContentType, // å¤ç”¨ç°æœ‰çš„ContentTypeæˆ–å®šä¹‰æ–°çš„
    pub title: Option<String>,
    pub content: String,
    pub relevance_score: f32, // AIè¯„ä¼°çš„ç‰‡æ®µä¸ä»»åŠ¡çš„ç›¸å…³æ€§
    // å¯ä»¥æ·»åŠ æ›´å¤šå…ƒæ•°æ®ï¼Œå¦‚ä»£ç è¯­è¨€ã€APIç­¾åç­‰
}

/// é«˜çº§çˆ¬è™«æ‰§è¡Œçš„æœ€ç»ˆç»“æœ
#[derive(Debug, Clone)]
pub struct AdvancedTaskResult {
    pub task: CrawlTask,
    pub aggregated_document: String, // æœ€ç»ˆç”Ÿæˆçš„ç»¼åˆæ–‡æ¡£
    pub source_fragments: Vec<ContentFragment>, // ç”¨äºç”Ÿæˆæ–‡æ¡£çš„æ‰€æœ‰ç‰‡æ®µ
    pub visited_urls_count: usize,
    // å¯ä»¥æ·»åŠ æ›´å¤šç»Ÿè®¡ä¿¡æ¯
}

/// å‘ç°çš„é“¾æ¥ä¿¡æ¯
#[derive(Debug, Clone)]
struct DiscoveredLink {
    url: String,
    priority: f32,
    link_text: String,
    context: String,
}


impl AdvancedIntelligentCrawler {
    pub async fn new(ai_service_config: AIServiceConfig, crawler_config: CrawlerConfig) -> Result<Self> {
        let ai_service = Arc::new(AIService::new(ai_service_config)?);
        let config = Arc::new(crawler_config);
        info!("ğŸš€ é«˜çº§æ™ºèƒ½çˆ¬è™«åˆå§‹åŒ–å®Œæˆ");
        Ok(Self { ai_service, config })
    }

    pub async fn execute_task(&self, task: CrawlTask) -> Result<AdvancedTaskResult> {
        info!("ğŸ¯ å¼€å§‹æ‰§è¡Œé«˜çº§çˆ¬è™«ä»»åŠ¡: {}", task.target_description);
        let task_arc = Arc::new(task.clone());

        let visited_urls = Arc::new(RwLock::new(HashSet::new()));
        let pending_urls = Arc::new(RwLock::new(VecDeque::new()));
        let collected_fragments = Arc::new(RwLock::new(Vec::new()));

        // 1. åˆå§‹åŒ–URLå‘ç°ä»£ç†å¹¶æ·»åŠ èµ·å§‹URL
        let url_discoverer = UrlDiscoveryAgent::new(
            self.ai_service.clone(),
            task_arc.clone(),
            visited_urls.clone(),
            pending_urls.clone(),
            self.config.clone(),
        );
        url_discoverer.add_start_url().await;

        // 2. åˆå§‹åŒ–å†…å®¹æå–ä»£ç†å’ŒçŸ¥è¯†èšåˆå™¨
        let content_extractor = ContentExtractionAgent::new(
            self.ai_service.clone(),
            task_arc.clone(),
            self.config.clone(),
        );
        let knowledge_aggregator = KnowledgeAggregator::new(
            self.ai_service.clone(),
            task_arc.clone(),
            collected_fragments.clone(),
        );
        
        // 3. ä¸»çˆ¬å–å¾ªç¯
        let mut pages_processed = 0;
        while let Some(current_url) = url_discoverer.get_next_url().await {
            if pages_processed >= task_arc.max_pages {
                info!("è¾¾åˆ°æœ€å¤§é¡µé¢å¤„ç†é™åˆ¶ ({})ï¼Œåœæ­¢çˆ¬å–ã€‚", task_arc.max_pages);
                break;
            }
            if current_url.depth >= task_arc.max_depth {
                info!("è¾¾åˆ°æœ€å¤§æ·±åº¦ ({})ï¼Œè·³è¿‡ URL: {}", task_arc.max_depth, current_url.url);
                continue;
            }

            debug!("å¤„ç†URL: {} (æ·±åº¦: {}, ä¼˜å…ˆçº§: {:.2})", current_url.url, current_url.depth, current_url.priority);
            
            // è·å–å¹¶æå–å†…å®¹
            match content_extractor.fetch_and_extract(&current_url.url).await {
                Ok(fragments) => {
                    if !fragments.is_empty() {
                        info!("ğŸ“„ ä» {} æå–äº† {} ä¸ªå†…å®¹ç‰‡æ®µ", current_url.url, fragments.len());
                        for fragment in fragments {
                            knowledge_aggregator.add_fragment(fragment).await;
                        }
                        
                        // ğŸ”¥ å…³é”®æ”¹è¿›ï¼šä»å½“å‰é¡µé¢å‘ç°æ–°çš„ç›¸å…³é“¾æ¥
                        if current_url.depth < task_arc.max_depth - 1 {
                            // é‡æ–°è·å–é¡µé¢å†…å®¹ç”¨äºé“¾æ¥å‘ç°
                            match content_extractor.fetch_page_content(&current_url.url).await {
                                Ok(page_content) => {
                                    if let Err(e) = url_discoverer.discover_links_from_content(
                                        &page_content, 
                                        &current_url.url, 
                                        current_url.depth
                                    ).await {
                                        warn!("é“¾æ¥å‘ç°å¤±è´¥ {}: {}", current_url.url, e);
                                    }
                                }
                                Err(e) => {
                                    warn!("é‡æ–°è·å–é¡µé¢å†…å®¹å¤±è´¥ {}: {}", current_url.url, e);
                                }
                            }
                        }
                    }
                }
                Err(e) => {
                    warn!("æå–å†…å®¹å¤±è´¥ {}: {}", current_url.url, e);
                }
            }
            
            // æ ‡è®°å·²è®¿é—®ï¼ˆå³ä½¿æå–å¤±è´¥ä¹Ÿæ ‡è®°ï¼Œé¿å…é‡è¯•æ— æ³•è®¿é—®çš„é¡µé¢ï¼‰
            url_discoverer.mark_as_visited(&current_url.url).await;
            pages_processed += 1;

            // çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¿‡äºé¢‘ç¹è¯·æ±‚
            tokio::time::sleep(std::time::Duration::from_millis(self.config.delay_ms)).await;
        }

        // 4. èšåˆçŸ¥è¯†
        let aggregated_document = knowledge_aggregator.aggregate_knowledge().await?;
        
        // æå‰è·å–å€¼ä»¥é¿å…å€Ÿç”¨æ£€æŸ¥å™¨é—®é¢˜
        let source_fragments = collected_fragments.read().await.clone();
        let visited_urls_count = visited_urls.read().await.len();
        
        info!("âœ… é«˜çº§çˆ¬è™«ä»»åŠ¡å®Œæˆ: {}ï¼Œç”Ÿæˆäº† {} å­—ç¬¦çš„æ–‡æ¡£ï¼Œè®¿é—®äº† {} ä¸ªURL", 
            task_arc.target_description, aggregated_document.len(), visited_urls_count);

        Ok(AdvancedTaskResult {
            task: task_arc.as_ref().clone(),
            aggregated_document,
            source_fragments,
            visited_urls_count,
        })
    }
}

impl UrlDiscoveryAgent {
    fn new(
        ai_service: Arc<AIService>,
        task: Arc<CrawlTask>,
        visited_urls: Arc<RwLock<HashSet<String>>>,
        pending_urls: Arc<RwLock<VecDeque<PrioritizedUrl>>>,
        config: Arc<CrawlerConfig>,
    ) -> Self {
        Self { ai_service, task, visited_urls, pending_urls, config }
    }

    async fn add_start_url(&self) {
        let mut queue = self.pending_urls.write().await;
        queue.push_back(PrioritizedUrl {
            url: self.task.start_url.clone(),
            priority: 1.0, // èµ·å§‹URLæœ€é«˜ä¼˜å…ˆçº§
            depth: 0,
            source_page_url: None,
        });
        info!("ç§å­URLå·²æ·»åŠ : {}", self.task.start_url);
    }

    async fn get_next_url(&self) -> Option<PrioritizedUrl> {
        let mut queue = self.pending_urls.write().await;
        // ç®€å•åœ°ä»é˜Ÿåˆ—å‰ç«¯è·å–ï¼Œæœªæ¥å¯ä»¥å®ç°æ›´å¤æ‚çš„ä¼˜å…ˆçº§è°ƒåº¦
        queue.pop_front()
    }

    async fn mark_as_visited(&self, url: &str) {
        let mut visited = self.visited_urls.write().await;
        visited.insert(url.to_string());
    }
    
    /// æ™ºèƒ½é“¾æ¥å‘ç°ï¼šä»é¡µé¢å†…å®¹ä¸­è¯†åˆ«ç›¸å…³é“¾æ¥
    async fn discover_links_from_content(&self, page_content: &str, current_url: &str, current_depth: u32) -> Result<()> {
        info!("ğŸ” URLå‘ç°ä»£ç†: å¼€å§‹ä» {} å‘ç°ç›¸å…³é“¾æ¥", current_url);
        
        // 1. æ„å»ºAIè¯·æ±‚ï¼Œè¦æ±‚æ™ºèƒ½æå–å’Œè¯„ä¼°é“¾æ¥
        let system_prompt = self.get_link_discovery_system_prompt();
        let user_message = self.get_link_discovery_user_message(page_content, current_url);
        
        let request = AIRequest {
            model: None,
            system_prompt: Some(system_prompt),
            user_message,
            temperature: Some(0.3), // è¾ƒä½æ¸©åº¦ç¡®ä¿å‡†ç¡®æ€§
            max_tokens: Some(2000),
            stream: false,
        };
        
        let response = self.ai_service.request(request).await?;
        
        // 2. è§£æAIå“åº”ï¼Œæå–é“¾æ¥ä¿¡æ¯
        let discovered_links = self.parse_link_discovery_response(&response.content, current_url).await?;
        
        // 3. å¤„ç†å‘ç°çš„é“¾æ¥
        let mut queue = self.pending_urls.write().await;
        let visited = self.visited_urls.read().await;
        
        for link in discovered_links {
            // è§„èŒƒåŒ–URL
            if let Ok(normalized_url) = self.normalize_url(&link.url, current_url) {
                // æ£€æŸ¥æ˜¯å¦å·²è®¿é—®æˆ–å·²åœ¨é˜Ÿåˆ—ä¸­
                if !visited.contains(&normalized_url) && 
                   !queue.iter().any(|p| p.url == normalized_url) {
                    
                    let prioritized_url = PrioritizedUrl {
                        url: normalized_url.clone(),
                        priority: link.priority,
                        depth: current_depth + 1,
                        source_page_url: Some(current_url.to_string()),
                    };
                    
                    // æŒ‰ä¼˜å…ˆçº§æ’å…¥é˜Ÿåˆ—
                    self.insert_by_priority(&mut queue, prioritized_url);
                    debug!("ğŸ”— å‘ç°æ–°é“¾æ¥: {} (ä¼˜å…ˆçº§: {:.2})", normalized_url, link.priority);
                }
            }
        }
        
        info!("âœ… é“¾æ¥å‘ç°å®Œæˆï¼Œé˜Ÿåˆ—ä¸­ç°æœ‰ {} ä¸ªå¾…å¤„ç†URL", queue.len());
        Ok(())
    }
    
    fn get_link_discovery_system_prompt(&self) -> String {
        r#"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç½‘é¡µé“¾æ¥åˆ†æä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»HTMLå†…å®¹ä¸­æ™ºèƒ½è¯†åˆ«å’Œè¯„ä¼°ä¸ç‰¹å®šæŠ€æœ¯ä»»åŠ¡ç›¸å…³çš„é“¾æ¥ã€‚

ä½ éœ€è¦ï¼š
1. åˆ†æHTMLä¸­çš„æ‰€æœ‰é“¾æ¥ï¼ˆ<a>æ ‡ç­¾ã€å¯¼èˆªèœå•ã€ç›¸å…³é“¾æ¥ç­‰ï¼‰
2. æ ¹æ®ä»»åŠ¡ç›®æ ‡è¯„ä¼°æ¯ä¸ªé“¾æ¥çš„ç›¸å…³æ€§å’Œä»·å€¼
3. ä¸ºæ¯ä¸ªç›¸å…³é“¾æ¥åˆ†é…ä¼˜å…ˆçº§åˆ†æ•°ï¼ˆ0.0-1.0ï¼‰
4. è¿”å›ç»“æ„åŒ–çš„JSONç»“æœ

è¯„ä¼°æ ‡å‡†ï¼š
- å®˜æ–¹æ–‡æ¡£é“¾æ¥ï¼šä¼˜å…ˆçº§ 0.9-1.0
- APIå‚è€ƒå’Œæ•™ç¨‹ï¼šä¼˜å…ˆçº§ 0.7-0.9
- ä»£ç ç¤ºä¾‹å’Œç”¨ä¾‹ï¼šä¼˜å…ˆçº§ 0.6-0.8
- ç¤¾åŒºè®¨è®ºå’Œåšå®¢ï¼šä¼˜å…ˆçº§ 0.4-0.6
- ç›¸å…³ä½†éæ ¸å¿ƒå†…å®¹ï¼šä¼˜å…ˆçº§ 0.2-0.4
- æ— å…³å†…å®¹ï¼šä¼˜å…ˆçº§ 0.0-0.2

è¿”å›æ ¼å¼ï¼šJSONæ•°ç»„ï¼Œæ¯ä¸ªå¯¹è±¡åŒ…å«ï¼š
{
  "url": "é“¾æ¥URL",
  "priority": 0.85,
  "link_text": "é“¾æ¥æ–‡æœ¬",
  "context": "é“¾æ¥ä¸Šä¸‹æ–‡æè¿°",
  "reasoning": "é€‰æ‹©æ­¤é“¾æ¥çš„åŸå› "
}"#.to_string()
    }
    
    fn get_link_discovery_user_message(&self, page_content: &str, current_url: &str) -> String {
        format!(
            r#"ä»»åŠ¡ç›®æ ‡ï¼šä¸º {} è¯­è¨€çš„ {} åº“æ”¶é›†ç›¸å…³æ–‡æ¡£
å…·ä½“æŸ¥è¯¢ï¼š{}
å½“å‰é¡µé¢ï¼š{}

HTMLå†…å®¹ï¼ˆå‰8000å­—ç¬¦ï¼‰ï¼š
{}

è¯·åˆ†ææ­¤é¡µé¢ä¸­çš„æ‰€æœ‰é“¾æ¥ï¼Œè¯†åˆ«ä¸ä»»åŠ¡ç›®æ ‡ç›¸å…³çš„é“¾æ¥ï¼Œå¹¶æŒ‰ç›¸å…³æ€§æ’åºã€‚
åªè¿”å›ä¼˜å…ˆçº§ >= 0.3 çš„é“¾æ¥ã€‚
ç¡®ä¿URLæ˜¯å®Œæ•´çš„ç»å¯¹è·¯å¾„ã€‚"#,
            self.task.programming_language,
            self.task.library_name,
            self.task.target_description,
            current_url,
            page_content.chars().take(8000).collect::<String>()
        )
    }
    
    async fn parse_link_discovery_response(&self, response: &str, current_url: &str) -> Result<Vec<DiscoveredLink>> {
        debug!("è§£æé“¾æ¥å‘ç°å“åº”: {}", response.chars().take(200).collect::<String>());
        
        // å°è¯•è§£æJSONå“åº”
        if let Ok(json_value) = serde_json::from_str::<serde_json::Value>(response) {
            if let Some(links_array) = json_value.as_array() {
                let mut discovered_links = Vec::new();
                
                for link_obj in links_array {
                    if let (Some(url), Some(priority)) = (
                        link_obj.get("url").and_then(|v| v.as_str()),
                        link_obj.get("priority").and_then(|v| v.as_f64())
                    ) {
                        let link_text = link_obj.get("link_text")
                            .and_then(|v| v.as_str())
                            .unwrap_or("")
                            .to_string();
                        
                        let context = link_obj.get("context")
                            .and_then(|v| v.as_str())
                            .unwrap_or("")
                            .to_string();
                        
                        discovered_links.push(DiscoveredLink {
                            url: url.to_string(),
                            priority: priority as f32,
                            link_text,
                            context,
                        });
                    }
                }
                
                return Ok(discovered_links);
            }
        }
        
        // å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–é“¾æ¥
        warn!("JSONè§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–é“¾æ¥");
        Ok(self.extract_links_from_text(response, current_url).await)
    }
    
    async fn extract_links_from_text(&self, text: &str, _current_url: &str) -> Vec<DiscoveredLink> {
        // ç®€å•çš„æ–‡æœ¬é“¾æ¥æå–ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
        let mut links = Vec::new();
        
        // æŸ¥æ‰¾HTTP/HTTPSé“¾æ¥
        let url_regex = regex::Regex::new(r"https?://[^\s<>]+").unwrap();
        for mat in url_regex.find_iter(text) {
            let url = mat.as_str().to_string();
            // ç»™æ–‡æœ¬æå–çš„é“¾æ¥ä¸€ä¸ªä¸­ç­‰ä¼˜å…ˆçº§
            links.push(DiscoveredLink {
                url,
                priority: 0.5,
                link_text: "ä»æ–‡æœ¬æå–".to_string(),
                context: "å¤‡ç”¨é“¾æ¥æå–".to_string(),
            });
        }
        
        links
    }
    
    fn normalize_url(&self, url: &str, base_url: &str) -> Result<String> {
        use url::Url;
        
        // å¦‚æœå·²ç»æ˜¯ç»å¯¹URLï¼Œç›´æ¥è¿”å›
        if url.starts_with("http://") || url.starts_with("https://") {
            return Ok(url.to_string());
        }
        
        // å¦åˆ™åŸºäºbase_urlæ„å»ºç»å¯¹URL
        let base = Url::parse(base_url)?;
        let absolute = base.join(url)?;
        Ok(absolute.to_string())
    }
    
    fn insert_by_priority(&self, queue: &mut VecDeque<PrioritizedUrl>, new_url: PrioritizedUrl) {
        // æŒ‰ä¼˜å…ˆçº§é™åºæ’å…¥
        let mut insert_index = queue.len();
        
        for (i, existing) in queue.iter().enumerate() {
            if new_url.priority > existing.priority {
                insert_index = i;
                break;
            }
        }
        
        queue.insert(insert_index, new_url);
    }
}

impl ContentExtractionAgent {
    fn new(ai_service: Arc<AIService>, task: Arc<CrawlTask>, config: Arc<CrawlerConfig>) -> Self {
        Self { ai_service, task, config }
    }

    /// å†…å®¹æå–ï¼šè·å–é¡µé¢å†…å®¹å¹¶ä½¿ç”¨AIæå–ç»“æ„åŒ–ä¿¡æ¯ç‰‡æ®µ
    async fn fetch_and_extract(&self, url: &str) -> Result<Vec<ContentFragment>> {
        info!("å†…å®¹æå–ä»£ç†: å¼€å§‹ä» {} æå–å†…å®¹ (éƒ¨åˆ†å®ç°)", url);
        // 1. HTTP GETè¯·æ±‚è·å–é¡µé¢HTML
        let html_content = self.fetch_page_content(url).await?;

        // 2. æ„å»ºAIè¯·æ±‚ï¼Œè¦æ±‚ä»HTMLä¸­æå–ä¸ä»»åŠ¡ç›¸å…³çš„ç»“æ„åŒ–ä¿¡æ¯ç‰‡æ®µ
        //    - Promptåº”åŒ…å«ä»»åŠ¡æè¿° (ç›®æ ‡è¯­è¨€ã€åº“ã€å…·ä½“æŸ¥è¯¢)
        //    - è¦æ±‚AIè¯†åˆ«ä¸åŒç±»å‹çš„å†…å®¹ï¼ˆä»£ç ã€APIå®šä¹‰ã€æ–‡æœ¬è§£é‡Šã€åˆ—è¡¨ç­‰ï¼‰
        //    - è¦æ±‚AIè¯„ä¼°æ¯ä¸ªç‰‡æ®µçš„ç›¸å…³æ€§
        //    - AIå“åº”åº”æ˜¯ç»“æ„åŒ–çš„ï¼Œä¾‹å¦‚JSONæ•°ç»„ï¼Œæ¯ä¸ªå¯¹è±¡æ˜¯ä¸€ä¸ªå†…å®¹ç‰‡æ®µ
        let request = AIRequest {
            model: None, // ä½¿ç”¨é»˜è®¤æ¨¡å‹
            system_prompt: Some(self.get_extraction_system_prompt()),
            user_message: self.get_extraction_user_message(&html_content, url),
            temperature: Some(0.2), // ä½æ¸©ä»¥è·å–æ›´ç²¾ç¡®çš„æå–
            max_tokens: Some(3000), // æ ¹æ®é¢„æœŸå†…å®¹è°ƒæ•´
            stream: false,
        };
        
        let response = self.ai_service.request(request).await?;
        
        // 3. è§£æAIå“åº”åˆ° Vec<ContentFragment>
        let fragments = self.parse_extraction_response(&response.content, url).await?;
        
        Ok(fragments)
    }

    async fn fetch_page_content(&self, url: &str) -> Result<String> {
        debug!("ğŸ“¥ è·å–é¡µé¢å†…å®¹: {}", url);
        let client = reqwest::Client::builder()
            .timeout(std::time::Duration::from_secs(self.config.timeout_secs))
            .user_agent(self.config.user_agent.clone())
            .build()?;

        let mut attempts = 0;
        while attempts < self.config.max_retries {
            match client.get(url).send().await {
                Ok(response) => {
                    if response.status().is_success() {
                        let content = response.text().await?;
                        debug!("âœ… æˆåŠŸè·å–é¡µé¢å†…å®¹ï¼Œé•¿åº¦: {} å­—ç¬¦", content.len());
                        return Ok(content);
                    } else {
                        warn!("ğŸš« HTTPé”™è¯¯: {} - {}", response.status(), url);
                    }
                }
                Err(e) => {
                    warn!("ğŸŒ ç½‘ç»œè¯·æ±‚å¤±è´¥ (å°è¯• {}/{}): {} for URL {}", attempts + 1, self.config.max_retries, e, url);
                }
            }
            attempts += 1;
            if attempts < self.config.max_retries {
                tokio::time::sleep(std::time::Duration::from_millis(1000 * attempts as u64)).await;
            }
        }
        Err(anyhow::anyhow!("è·å–é¡µé¢ {} å†…å®¹å¤±è´¥ï¼Œå·²é‡è¯• {} æ¬¡", url, self.config.max_retries))
    }
    
    fn get_extraction_system_prompt(&self) -> String {
        r#"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯å†…å®¹æå–ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»HTMLç½‘é¡µä¸­æå–ä¸ç‰¹å®šç¼–ç¨‹ä»»åŠ¡ç›¸å…³çš„ç»“æ„åŒ–ä¿¡æ¯ã€‚

ä½ éœ€è¦ï¼š
1. åˆ†æHTMLå†…å®¹ï¼Œè¯†åˆ«ä¸ä»»åŠ¡ç›®æ ‡ç›¸å…³çš„æŠ€æœ¯ä¿¡æ¯
2. æå–ä»£ç ç¤ºä¾‹ã€APIæ–‡æ¡£ã€æ•™ç¨‹æ­¥éª¤ã€é…ç½®è¯´æ˜ç­‰
3. ä¸ºæ¯ä¸ªå†…å®¹ç‰‡æ®µåˆ†é…ç›¸å…³æ€§åˆ†æ•°ï¼ˆ0.0-1.0ï¼‰
4. è¿”å›ç»“æ„åŒ–çš„JSONç»“æœ

å†…å®¹ç±»å‹åˆ†ç±»ï¼š
- Documentation: å®˜æ–¹æ–‡æ¡£å’Œè¯´æ˜
- Tutorial: æ•™ç¨‹å’ŒæŒ‡å—
- ApiReference: APIå‚è€ƒæ–‡æ¡£
- Examples: ä»£ç ç¤ºä¾‹å’Œç”¨ä¾‹
- GettingStarted: å…¥é—¨æŒ‡å—
- Installation: å®‰è£…è¯´æ˜
- Configuration: é…ç½®æ–‡æ¡£
- Troubleshooting: æ•…éšœæ’é™¤

è¿”å›æ ¼å¼ï¼šJSONæ•°ç»„ï¼Œæ¯ä¸ªå¯¹è±¡åŒ…å«ï¼š
{
  "fragment_type": "Documentation",
  "title": "ç‰‡æ®µæ ‡é¢˜",
  "content": "æå–çš„å†…å®¹æ–‡æœ¬",
  "relevance_score": 0.85,
  "code_language": "rust" // å¦‚æœåŒ…å«ä»£ç 
}"#.to_string()
    }

    fn get_extraction_user_message(&self, html_content: &str, url: &str) -> String {
        format!(
            r#"ä»»åŠ¡ç›®æ ‡ï¼šä¸º {} è¯­è¨€çš„ {} åº“æå–ç›¸å…³æŠ€æœ¯å†…å®¹
å…·ä½“æŸ¥è¯¢ï¼š{}
å½“å‰é¡µé¢ï¼š{}

HTMLå†…å®¹ï¼ˆå‰10000å­—ç¬¦ï¼‰ï¼š
{}

è¯·ä»æ­¤é¡µé¢æå–ä¸ä»»åŠ¡ç›®æ ‡ç›¸å…³çš„æ‰€æœ‰æŠ€æœ¯å†…å®¹ç‰‡æ®µã€‚
é‡ç‚¹å…³æ³¨ï¼š
1. ä¸ {} åº“ç›¸å…³çš„ä»£ç ç¤ºä¾‹
2. APIä½¿ç”¨è¯´æ˜å’Œå‚æ•°æè¿°
3. é…ç½®å’Œå®‰è£…æŒ‡å—
4. å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
5. æœ€ä½³å®è·µå’Œä½¿ç”¨å»ºè®®

åªè¿”å›ç›¸å…³æ€§åˆ†æ•° >= 0.4 çš„å†…å®¹ç‰‡æ®µã€‚
ç¡®ä¿æå–çš„å†…å®¹å®Œæ•´ä¸”æœ‰æ„ä¹‰ã€‚"#,
            self.task.programming_language,
            self.task.library_name,
            self.task.target_description,
            url,
            html_content.chars().take(10000).collect::<String>(),
            self.task.library_name
        )
    }

    async fn parse_extraction_response(&self, ai_response_content: &str, source_url: &str) -> Result<Vec<ContentFragment>> {
        debug!("è§£æå†…å®¹æå–å“åº”: {}", ai_response_content.chars().take(200).collect::<String>());
        
        // å°è¯•è§£æJSONå“åº”
        if let Ok(json_value) = serde_json::from_str::<serde_json::Value>(ai_response_content) {
            if let Some(fragments_array) = json_value.as_array() {
                let mut content_fragments = Vec::new();
                
                for fragment_obj in fragments_array {
                    if let (Some(content), Some(relevance_score)) = (
                        fragment_obj.get("content").and_then(|v| v.as_str()),
                        fragment_obj.get("relevance_score").and_then(|v| v.as_f64())
                    ) {
                        let fragment_type = fragment_obj.get("fragment_type")
                            .and_then(|v| v.as_str())
                            .and_then(|s| self.parse_content_type(s))
                            .unwrap_or(ContentType::Documentation);
                        
                        let title = fragment_obj.get("title")
                            .and_then(|v| v.as_str())
                            .map(|s| s.to_string());
                        
                        content_fragments.push(ContentFragment {
                            source_url: source_url.to_string(),
                            fragment_type,
                            title,
                            content: content.to_string(),
                            relevance_score: relevance_score as f32,
                        });
                    }
                }
                
                info!("âœ… æˆåŠŸè§£æ {} ä¸ªå†…å®¹ç‰‡æ®µ", content_fragments.len());
                return Ok(content_fragments);
            }
        }
        
        // å¦‚æœJSONè§£æå¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«AIå“åº”çš„åŸºæœ¬ç‰‡æ®µ
        warn!("JSONè§£æå¤±è´¥ï¼Œåˆ›å»ºåŸºæœ¬å†…å®¹ç‰‡æ®µ");
        Ok(vec![ContentFragment {
            source_url: source_url.to_string(),
            fragment_type: ContentType::Documentation,
            title: Some("AIæå–çš„å†…å®¹".to_string()),
            content: ai_response_content.chars().take(1000).collect(),
            relevance_score: 0.6,
        }])
    }
    
    fn parse_content_type(&self, type_str: &str) -> Option<ContentType> {
        match type_str {
            "Documentation" => Some(ContentType::Documentation),
            "Tutorial" => Some(ContentType::Tutorial),
            "ApiReference" => Some(ContentType::ApiReference),
            "Examples" => Some(ContentType::Examples),
            "GettingStarted" => Some(ContentType::GettingStarted),
            "Installation" => Some(ContentType::Installation),
            "Configuration" => Some(ContentType::Configuration),
            "Troubleshooting" => Some(ContentType::Troubleshooting),
            _ => None,
        }
    }
}

impl KnowledgeAggregator {
    fn new(ai_service: Arc<AIService>, task: Arc<CrawlTask>, collected_fragments: Arc<RwLock<Vec<ContentFragment>>>) -> Self {
        Self { ai_service, task, collected_fragments }
    }

    async fn add_fragment(&self, fragment: ContentFragment) {
        let mut fragments = self.collected_fragments.write().await;
        fragments.push(fragment);
    }

    /// çŸ¥è¯†èšåˆï¼šå°†æ”¶é›†çš„å†…å®¹ç‰‡æ®µèšåˆæˆè¿è´¯æ–‡æ¡£
    async fn aggregate_knowledge(&self) -> Result<String> {
        info!("çŸ¥è¯†èšåˆå™¨: å¼€å§‹èšåˆå·²æ”¶é›†çš„ç‰‡æ®µ (éƒ¨åˆ†å®ç°)");
        let fragments = self.collected_fragments.read().await;
        if fragments.is_empty() {
            return Ok("æœªæ”¶é›†åˆ°ä»»ä½•å†…å®¹ç‰‡æ®µè¿›è¡Œèšåˆã€‚".to_string());
        }

        // 1. é¢„å¤„ç†å’Œç­›é€‰ç‰‡æ®µ (ä¾‹å¦‚ï¼ŒæŒ‰ç›¸å…³æ€§ã€å»é‡ç­‰)
        let mut content_to_aggregate = String::new();
        for (i, fragment) in fragments.iter().enumerate() {
            content_to_aggregate.push_str(&format!(
                "--- Fragment {} from {} (Relevance: {:.2}) ---\nTitle: {}\nType: {:?}\nContent:\n{}\n\n",
                i + 1,
                fragment.source_url,
                fragment.relevance_score,
                fragment.title.as_deref().unwrap_or("N/A"),
                fragment.fragment_type,
                fragment.content
            ));
        }
        
        // 2. æ„å»ºAIè¯·æ±‚ï¼Œè¦æ±‚å°†è¿™äº›ç‰‡æ®µèšåˆæˆä¸€ä»½ç»“æ„è‰¯å¥½ã€ä¿¡æ¯å…¨é¢çš„æ–‡æ¡£
        //    - Promptåº”åŒ…å«åŸå§‹ä»»åŠ¡ç›®æ ‡
        //    - æŒ‡ç¤ºAIç»„ç»‡å†…å®¹ã€æ¶ˆé™¤å†—ä½™ã€ç¡®ä¿æµç•…æ€§å’Œå‡†ç¡®æ€§
        let request = AIRequest {
            model: None,
            system_prompt: Some(self.get_aggregation_system_prompt()),
            user_message: self.get_aggregation_user_message(&content_to_aggregate),
            temperature: Some(0.5), // é€‚ä¸­æ¸©åº¦ä»¥å¹³è¡¡åˆ›é€ æ€§å’Œå‡†ç¡®æ€§
            max_tokens: Some(4000), // å…è®¸ç”Ÿæˆè¾ƒé•¿çš„æ–‡æ¡£
            stream: false,
        };

        let response = self.ai_service.request(request).await?;
        
        // 3. è¿”å›AIç”Ÿæˆçš„èšåˆæ–‡æ¡£
        Ok(response.content)
    }
    
    fn get_aggregation_system_prompt(&self) -> String {
        r#"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£ç¼–å†™ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†æ¥è‡ªå¤šä¸ªç½‘é¡µçš„æŠ€æœ¯å†…å®¹ç‰‡æ®µæ•´åˆæˆä¸€ä»½è¿è´¯ã€å…¨é¢ã€é«˜è´¨é‡çš„æŠ€æœ¯æ–‡æ¡£ã€‚

ä½ éœ€è¦ï¼š
1. åˆ†ææ‰€æœ‰å†…å®¹ç‰‡æ®µï¼Œç†è§£å®ƒä»¬ä¹‹é—´çš„å…³ç³»å’Œå±‚æ¬¡
2. å»é™¤é‡å¤ä¿¡æ¯ï¼Œæ•´åˆç›¸å…³å†…å®¹
3. æŒ‰é€»è¾‘é¡ºåºç»„ç»‡å†…å®¹ï¼ˆæ¦‚è¿°â†’å®‰è£…â†’åŸºç¡€ç”¨æ³•â†’é«˜çº§ç‰¹æ€§â†’ç¤ºä¾‹â†’æ•…éšœæ’é™¤ï¼‰
4. ç¡®ä¿æŠ€æœ¯ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
5. æ·»åŠ å¿…è¦çš„è¿‡æ¸¡å’Œè§£é‡Šæ–‡æœ¬
6. ç”Ÿæˆç»“æ„åŒ–çš„Markdownæ–‡æ¡£

æ–‡æ¡£ç»“æ„è¦æ±‚ï¼š
- ä½¿ç”¨æ¸…æ™°çš„æ ‡é¢˜å±‚æ¬¡ï¼ˆ#, ##, ###ï¼‰
- ä»£ç å—ä½¿ç”¨æ­£ç¡®çš„è¯­è¨€æ ‡è¯†
- åŒ…å«ç›®å½•å’Œç« èŠ‚å¯¼èˆª
- çªå‡ºé‡è¦ä¿¡æ¯å’Œæœ€ä½³å®è·µ
- æä¾›å®ç”¨çš„ä»£ç ç¤ºä¾‹

è¾“å‡ºæ ¼å¼ï¼šå®Œæ•´çš„Markdownæ–‡æ¡£ï¼ŒåŒ…å«ï¼š
1. æ–‡æ¡£æ ‡é¢˜å’Œç®€ä»‹
2. ç›®å½•
3. ä¸»è¦å†…å®¹ç« èŠ‚
4. ä»£ç ç¤ºä¾‹å’Œç”¨æ³•
5. å‚è€ƒé“¾æ¥å’Œæ¥æº"#.to_string()
    }

    fn get_aggregation_user_message(&self, all_fragments_text: &str) -> String {
        format!(
            r#"ä»»åŠ¡ç›®æ ‡ï¼šä¸º {} è¯­è¨€çš„ {} åº“åˆ›å»ºç»¼åˆæŠ€æœ¯æ–‡æ¡£
ç”¨æˆ·æŸ¥è¯¢ï¼š{}

æ”¶é›†åˆ°çš„å†…å®¹ç‰‡æ®µï¼š
{}

è¯·å°†è¿™äº›ç‰‡æ®µæ•´åˆæˆä¸€ä»½ä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£ã€‚æ–‡æ¡£åº”è¯¥ï¼š

1. **ç»“æ„æ¸…æ™°**ï¼šæŒ‰é€»è¾‘é¡ºåºç»„ç»‡å†…å®¹
2. **å†…å®¹å…¨é¢**ï¼šæ¶µç›–å®‰è£…ã€é…ç½®ã€ä½¿ç”¨ã€ç¤ºä¾‹ç­‰æ–¹é¢
3. **å®ç”¨æ€§å¼º**ï¼šåŒ…å«å¯ç›´æ¥ä½¿ç”¨çš„ä»£ç ç¤ºä¾‹
4. **æ˜“äºç†è§£**ï¼šé€‚åˆä¸åŒæŠ€èƒ½æ°´å¹³çš„å¼€å‘è€…

é‡ç‚¹å…³æ³¨ï¼š
- {} åº“çš„æ ¸å¿ƒåŠŸèƒ½å’Œç‰¹æ€§
- å®é™…ä½¿ç”¨åœºæ™¯å’Œæœ€ä½³å®è·µ
- å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
- å®Œæ•´çš„ä»£ç ç¤ºä¾‹

è¯·ç”Ÿæˆä¸€ä»½é«˜è´¨é‡çš„MarkdownæŠ€æœ¯æ–‡æ¡£ã€‚"#,
            self.task.programming_language,
            self.task.library_name,
            self.task.target_description,
            all_fragments_text.chars().take(12000).collect::<String>(), // å¢åŠ è¾“å…¥é•¿åº¦
            self.task.library_name
        )
    }
}

// --- è¾…åŠ©å‡½æ•°å’Œç»“æ„ (å¦‚æœéœ€è¦) --- 
