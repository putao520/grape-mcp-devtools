#!/usr/bin/env python3
"""
ğŸ¤– Grape MCP DevTools æ™ºèƒ½å®¢æˆ·ç«¯
ä¸€ä¸ªæ”¯æŒMCPåè®®çš„AIåŠ©æ‰‹å®¢æˆ·ç«¯ï¼Œå¯ä»¥ä¸MCPæœåŠ¡å™¨é€šä¿¡å¹¶æä¾›æ™ºèƒ½å¯¹è¯
"""

import asyncio
import json
import os
import sys
import subprocess
import signal
import uuid
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path

import click
import httpx
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.markdown import Markdown
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.prompt import Prompt
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

console = Console()

@dataclass
class MCPRequest:
    """MCPè¯·æ±‚ç»“æ„"""
    jsonrpc: str = "2.0"
    version: str = "2025-03-26"
    id: str = ""
    method: str = ""
    params: Dict[str, Any] = None
    
    def __post_init__(self):
        if not self.id:
            self.id = str(uuid.uuid4())
        if self.params is None:
            self.params = {}

@dataclass
class MCPResponse:
    """MCPå“åº”ç»“æ„"""
    jsonrpc: str
    version: str
    id: str
    result: Optional[Dict[str, Any]] = None
    error: Optional[Dict[str, Any]] = None

class LLMClient:
    """LLMå®¢æˆ·ç«¯ï¼Œæ”¯æŒå¤šç§APIæä¾›å•†"""
    
    def __init__(self):
        self.client = httpx.AsyncClient(timeout=30.0)
        self.api_base_url = os.getenv("LLM_API_BASE_URL", "https://integrate.api.nvidia.com/v1")
        self.api_key = os.getenv("LLM_API_KEY")
        self.model_name = os.getenv("LLM_MODEL_NAME", "nvidia/llama-3.1-nemotron-70b-instruct")
        
        if not self.api_key:
            console.print("âš ï¸  è­¦å‘Š: æœªè®¾ç½®LLM_API_KEYï¼ŒLLMåŠŸèƒ½å°†ä¸å¯ç”¨", style="yellow")
    
    async def chat_completion(self, messages: List[Dict[str, str]], tools: Optional[List[Dict]] = None) -> str:
        """å‘é€èŠå¤©å®Œæˆè¯·æ±‚"""
        if not self.api_key:
            return "âŒ LLM APIå¯†é’¥æœªé…ç½®ï¼Œæ— æ³•ä½¿ç”¨LLMåŠŸèƒ½"
        
        try:
            payload = {
                "model": self.model_name,
                "messages": messages,
                "max_tokens": 1024,
                "temperature": 0.7
            }
            
            if tools:
                payload["tools"] = tools
                payload["tool_choice"] = "auto"
            
            response = await self.client.post(
                f"{self.api_base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json=payload
            )
            
            if response.status_code != 200:
                return f"âŒ LLM APIè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}"
            
            response_data = response.json()
            
            if "choices" in response_data and len(response_data["choices"]) > 0:
                choice = response_data["choices"][0]
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                if "tool_calls" in choice.get("message", {}):
                    return choice["message"]
                else:
                    return choice["message"]["content"]
            
            return "âŒ LLMå“åº”æ ¼å¼é”™è¯¯"
            
        except Exception as e:
            return f"âŒ LLMè¯·æ±‚å¤±è´¥: {e}"

class MCPClient:
    """MCPå®¢æˆ·ç«¯"""
    
    def __init__(self):
        self.server_process: Optional[subprocess.Popen] = None
        self.tools: List[Dict] = []
        self.llm_client = LLMClient()
        self.conversation_history: List[Dict[str, str]] = []
        
    async def start_server(self, server_command: List[str]) -> bool:
        """å¯åŠ¨MCPæœåŠ¡å™¨"""
        try:
            console.print("ğŸš€ å¯åŠ¨MCPæœåŠ¡å™¨...", style="blue")
            
            self.server_process = subprocess.Popen(
                server_command,
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=0  # æ— ç¼“å†²
            )
            
            # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
            await asyncio.sleep(2)
            
            if self.server_process.poll() is None:
                console.print("âœ… MCPæœåŠ¡å™¨å¯åŠ¨æˆåŠŸ", style="green")
                return True
            else:
                console.print("âŒ MCPæœåŠ¡å™¨å¯åŠ¨å¤±è´¥", style="red")
                return False
                
        except Exception as e:
            console.print(f"âŒ å¯åŠ¨æœåŠ¡å™¨å¤±è´¥: {e}", style="red")
            return False
    
    async def stop_server(self):
        """åœæ­¢MCPæœåŠ¡å™¨"""
        if self.server_process:
            self.server_process.terminate()
            self.server_process.wait()
            console.print("ğŸ‘‹ MCPæœåŠ¡å™¨å·²å…³é—­", style="yellow")
    
    async def send_request(self, request: MCPRequest) -> MCPResponse:
        """å‘é€MCPè¯·æ±‚"""
        if not self.server_process:
            raise Exception("MCPæœåŠ¡å™¨æœªå¯åŠ¨")
        
        request_json = json.dumps(asdict(request)) + "\n"
        
        try:
            self.server_process.stdin.write(request_json)
            self.server_process.stdin.flush()
            
            # è¯»å–å“åº”
            response_line = self.server_process.stdout.readline()
            if not response_line:
                raise Exception("æœåŠ¡å™¨æ— å“åº”")
            
            response_data = json.loads(response_line.strip())
            return MCPResponse(**response_data)
            
        except Exception as e:
            raise Exception(f"MCPé€šä¿¡å¤±è´¥: {e}")
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–MCPè¿æ¥"""
        request = MCPRequest(
            method="initialize",
            params={
                "client_name": "grape-mcp-client",
                "client_version": "1.0.0",
                "capabilities": ["documentSearch", "versionInfo"]
            }
        )
        
        try:
            response = await self.send_request(request)
            if response.error:
                console.print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {response.error}", style="red")
                return False
            
            console.print("âœ… MCPè¿æ¥åˆå§‹åŒ–æˆåŠŸ", style="green")
            return True
            
        except Exception as e:
            console.print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}", style="red")
            return False
    
    async def list_tools(self) -> bool:
        """è·å–å·¥å…·åˆ—è¡¨"""
        request = MCPRequest(method="tools/list")
        
        try:
            response = await self.send_request(request)
            if response.error:
                console.print(f"âŒ è·å–å·¥å…·åˆ—è¡¨å¤±è´¥: {response.error}", style="red")
                return False
            
            self.tools = response.result.get("tools", [])
            
            # æ˜¾ç¤ºå·¥å…·åˆ—è¡¨
            table = Table(title="ğŸ“š å¯ç”¨å·¥å…·åˆ—è¡¨")
            table.add_column("å·¥å…·åç§°", style="cyan")
            table.add_column("æè¿°", style="green")
            table.add_column("å‚æ•°", style="yellow")
            
            for tool in self.tools:
                params_str = ", ".join(tool.get("inputSchema", {}).get("properties", {}).keys())
                table.add_row(
                    tool["name"],
                    tool["description"],
                    params_str[:50] + "..." if len(params_str) > 50 else params_str
                )
            
            console.print(table)
            return True
            
        except Exception as e:
            console.print(f"âŒ è·å–å·¥å…·åˆ—è¡¨å¤±è´¥: {e}", style="red")
            return False
    
    async def call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Optional[Dict]:
        """è°ƒç”¨MCPå·¥å…·"""
        request = MCPRequest(
            method="tools/call",
            params={
                "name": tool_name,
                "arguments": arguments
            }
        )
        
        try:
            response = await self.send_request(request)
            if response.error:
                console.print(f"âŒ å·¥å…·è°ƒç”¨å¤±è´¥: {response.error}", style="red")
                return None
            
            return response.result
            
        except Exception as e:
            console.print(f"âŒ å·¥å…·è°ƒç”¨å¤±è´¥: {e}", style="red")
            return None
    
    def get_tools_for_llm(self) -> List[Dict]:
        """è·å–é€‚ç”¨äºLLMçš„å·¥å…·å®šä¹‰"""
        llm_tools = []
        
        for tool in self.tools:
            llm_tool = {
                "type": "function",
                "function": {
                    "name": tool["name"],
                    "description": tool["description"],
                    "parameters": tool.get("inputSchema", {})
                }
            }
            llm_tools.append(llm_tool)
        
        return llm_tools
    
    async def ai_chat(self, user_message: str) -> str:
        """AIå¯¹è¯ï¼Œè‡ªåŠ¨è°ƒç”¨å·¥å…·"""
        self.conversation_history.append({"role": "user", "content": user_message})
        
        # æ„å»ºç³»ç»Ÿæç¤º
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¼€å‘å·¥å…·åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·æŸ¥è¯¢åŒ…ä¿¡æ¯ã€æ–‡æ¡£å’Œç‰ˆæœ¬ä¿¡æ¯ã€‚

å¯ç”¨å·¥å…·ï¼š
- search_docs: æœç´¢æ–‡æ¡£å’ŒåŒ…ä¿¡æ¯
- check_version: æ£€æŸ¥åŒ…ç‰ˆæœ¬ä¿¡æ¯
- get_api_docs: è·å–APIæ–‡æ¡£
- vector_docs: å‘é‡åŒ–æ–‡æ¡£ç®¡ç†

è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æ™ºèƒ½é€‰æ‹©åˆé€‚çš„å·¥å…·ï¼Œå¹¶ä»¥ä¸­æ–‡å›å¤ã€‚å¦‚æœéœ€è¦è°ƒç”¨å·¥å…·ï¼Œè¯·æŒ‰ç…§JSONæ ¼å¼è¿”å›å·¥å…·è°ƒç”¨ä¿¡æ¯ã€‚"""

        messages = [{"role": "system", "content": system_prompt}] + self.conversation_history
        tools = self.get_tools_for_llm()
        
        response = await self.llm_client.chat_completion(messages, tools)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        if isinstance(response, dict) and "tool_calls" in response:
            # å¤„ç†å·¥å…·è°ƒç”¨
            tool_results = []
            for tool_call in response["tool_calls"]:
                function = tool_call["function"]
                tool_name = function["name"]
                arguments = json.loads(function["arguments"])
                
                console.print(f"ğŸ”§ è°ƒç”¨å·¥å…·: {tool_name}", style="blue")
                console.print(f"ğŸ“ å‚æ•°: {arguments}", style="dim")
                
                result = await self.call_tool(tool_name, arguments)
                if result:
                    tool_results.append(f"å·¥å…· {tool_name} çš„ç»“æœ: {json.dumps(result, ensure_ascii=False, indent=2)}")
            
            # å°†å·¥å…·ç»“æœå‘é€å›LLMç”Ÿæˆæœ€ç»ˆå›å¤
            if tool_results:
                tool_results_text = "\n\n".join(tool_results)
                final_messages = messages + [
                    {"role": "assistant", "content": response.get("content", "")},
                    {"role": "user", "content": f"å·¥å…·è°ƒç”¨ç»“æœï¼š\n{tool_results_text}\n\nè¯·åŸºäºè¿™äº›ç»“æœç»™å‡ºæœ€ç»ˆå›å¤ã€‚"}
                ]
                final_response = await self.llm_client.chat_completion(final_messages)
                self.conversation_history.append({"role": "assistant", "content": final_response})
                return final_response
        
        # æ™®é€šå›å¤
        if isinstance(response, str):
            self.conversation_history.append({"role": "assistant", "content": response})
            return response
        
        return "âŒ å¤„ç†AIå›å¤æ—¶å‡ºç°é”™è¯¯"

@click.group()
def cli():
    """ğŸ¤– Grape MCP DevTools æ™ºèƒ½å®¢æˆ·ç«¯"""
    pass

@cli.command()
@click.option("--server-cmd", default="cargo run --bin grape-mcp-devtools", help="MCPæœåŠ¡å™¨å¯åŠ¨å‘½ä»¤")
async def chat(server_cmd: str):
    """ğŸ’¬ å¯åŠ¨æ™ºèƒ½å¯¹è¯æ¨¡å¼"""
    client = MCPClient()
    
    # å¯åŠ¨MCPæœåŠ¡å™¨
    server_command = server_cmd.split()
    if not await client.start_server(server_command):
        return
    
    try:
        # åˆå§‹åŒ–è¿æ¥
        if not await client.initialize():
            return
        
        # è·å–å·¥å…·åˆ—è¡¨
        if not await client.list_tools():
            return
        
        console.print("\nğŸ¯ æ™ºèƒ½å¯¹è¯æ¨¡å¼å·²å¯åŠ¨ï¼", style="bold green")
        console.print("ğŸ’¡ ä½ å¯ä»¥è¯¢é—®å…³äºåŒ…ç®¡ç†ã€æ–‡æ¡£æŸ¥è¯¢ã€ç‰ˆæœ¬æ£€æŸ¥ç­‰é—®é¢˜", style="dim")
        console.print("ğŸ’¡ è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º\n", style="dim")
        
        while True:
            try:
                user_input = Prompt.ask("ğŸ¤” [bold blue]ä½ [/bold blue]")
                
                if user_input.lower() in ['quit', 'exit', 'bye', 'é€€å‡º']:
                    break
                
                with Progress(
                    SpinnerColumn(),
                    TextColumn("[progress.description]{task.description}"),
                    console=console
                ) as progress:
                    task = progress.add_task("ğŸ¤– AIæ­£åœ¨æ€è€ƒ...", total=None)
                    response = await client.ai_chat(user_input)
                
                console.print(f"\nğŸ¤– [bold green]åŠ©æ‰‹[/bold green]: {response}\n")
                
            except KeyboardInterrupt:
                break
            except EOFError:
                break
    
    finally:
        await client.stop_server()

@cli.command()
@click.option("--server-cmd", default="cargo run --bin grape-mcp-devtools", help="MCPæœåŠ¡å™¨å¯åŠ¨å‘½ä»¤")
async def test(server_cmd: str):
    """ğŸ§ª æµ‹è¯•MCPè¿æ¥å’Œå·¥å…·"""
    client = MCPClient()
    
    # å¯åŠ¨MCPæœåŠ¡å™¨
    server_command = server_cmd.split()
    if not await client.start_server(server_command):
        return
    
    try:
        # åˆå§‹åŒ–è¿æ¥
        if not await client.initialize():
            return
        
        # è·å–å·¥å…·åˆ—è¡¨
        if not await client.list_tools():
            return
        
        # æµ‹è¯•ä¸€äº›åŸºæœ¬å·¥å…·è°ƒç”¨
        test_cases = [
            {
                "tool": "search_docs",
                "args": {"query": "http client", "language": "rust", "limit": 3},
                "description": "æœç´¢Rust HTTPå®¢æˆ·ç«¯æ–‡æ¡£"
            },
            {
                "tool": "check_version",
                "args": {"package": "serde", "language": "rust"},
                "description": "æ£€æŸ¥serdeåŒ…ç‰ˆæœ¬"
            }
        ]
        
        console.print("\nğŸ§ª å¼€å§‹å·¥å…·æµ‹è¯•...\n", style="bold blue")
        
        for i, test_case in enumerate(test_cases, 1):
            console.print(f"ğŸ“‹ æµ‹è¯• {i}: {test_case['description']}", style="cyan")
            
            result = await client.call_tool(test_case["tool"], test_case["args"])
            if result:
                console.print("âœ… æµ‹è¯•æˆåŠŸ", style="green")
                # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœ
                result_str = json.dumps(result, ensure_ascii=False, indent=2)
                if len(result_str) > 300:
                    result_str = result_str[:300] + "..."
                console.print(Panel(result_str, title="è¿”å›ç»“æœ"))
            else:
                console.print("âŒ æµ‹è¯•å¤±è´¥", style="red")
            
            console.print()
    
    finally:
        await client.stop_server()

@cli.command()
@click.option("--tool-name", prompt="å·¥å…·åç§°", help="è¦è°ƒç”¨çš„å·¥å…·åç§°")
@click.option("--args", prompt="å‚æ•°(JSONæ ¼å¼)", help="å·¥å…·å‚æ•°ï¼ŒJSONæ ¼å¼")
@click.option("--server-cmd", default="cargo run --bin grape-mcp-devtools", help="MCPæœåŠ¡å™¨å¯åŠ¨å‘½ä»¤")
async def call(tool_name: str, args: str, server_cmd: str):
    """ğŸ”§ ç›´æ¥è°ƒç”¨MCPå·¥å…·"""
    client = MCPClient()
    
    try:
        arguments = json.loads(args)
    except json.JSONDecodeError as e:
        console.print(f"âŒ JSONå‚æ•°æ ¼å¼é”™è¯¯: {e}", style="red")
        return
    
    # å¯åŠ¨MCPæœåŠ¡å™¨
    server_command = server_cmd.split()
    if not await client.start_server(server_command):
        return
    
    try:
        # åˆå§‹åŒ–è¿æ¥
        if not await client.initialize():
            return
        
        # è·å–å·¥å…·åˆ—è¡¨
        if not await client.list_tools():
            return
        
        console.print(f"ğŸ”§ è°ƒç”¨å·¥å…·: {tool_name}", style="blue")
        console.print(f"ğŸ“ å‚æ•°: {arguments}", style="dim")
        
        result = await client.call_tool(tool_name, arguments)
        if result:
            console.print("âœ… è°ƒç”¨æˆåŠŸ", style="green")
            result_json = json.dumps(result, ensure_ascii=False, indent=2)
            console.print(Panel(result_json, title="å·¥å…·è¿”å›ç»“æœ"))
        else:
            console.print("âŒ è°ƒç”¨å¤±è´¥", style="red")
    
    finally:
        await client.stop_server()

def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®ä¿¡å·å¤„ç†
    def signal_handler(sig, frame):
        console.print("\nğŸ‘‹ å†è§ï¼", style="yellow")
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    
    # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
    console.print(Panel.fit(
        "[bold blue]ğŸ¤– Grape MCP DevTools æ™ºèƒ½å®¢æˆ·ç«¯[/bold blue]\n"
        "[dim]ä¸€ä¸ªæ”¯æŒMCPåè®®çš„AIåŠ©æ‰‹å®¢æˆ·ç«¯[/dim]",
        border_style="blue"
    ))
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv("LLM_API_KEY"):
        console.print("âš ï¸  æç¤º: æœªè®¾ç½®LLM_API_KEYç¯å¢ƒå˜é‡ï¼ŒAIå¯¹è¯åŠŸèƒ½å°†å—é™", style="yellow")
        console.print("ğŸ’¡ è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®LLMç›¸å…³é…ç½®ä»¥å¯ç”¨å®Œæ•´åŠŸèƒ½\n", style="dim")
    
    # è¿è¡ŒCLI
    cli()

if __name__ == "__main__":
    if sys.version_info >= (3, 7):
        # Python 3.7+ ä½¿ç”¨ asyncio.run
        import asyncio
        import inspect
        
        # ä¿®å¤clickå¼‚æ­¥æ”¯æŒ
        original_cli = cli
        def sync_cli():
            ctx = click.get_current_context()
            if ctx.invoked_subcommand:
                cmd = ctx.invoked_subcommand
                # å¦‚æœæ˜¯å¼‚æ­¥å‘½ä»¤ï¼Œè¿è¡Œå®ƒ
                if hasattr(original_cli.commands[cmd], 'callback') and inspect.iscoroutinefunction(original_cli.commands[cmd].callback):
                    return asyncio.run(original_cli.commands[cmd].callback(**ctx.params))
            return original_cli()
        
        # ä¸ºå¼‚æ­¥å‘½ä»¤åˆ›å»ºåŒ…è£…å™¨
        for cmd_name, cmd in original_cli.commands.items():
            if hasattr(cmd, 'callback') and inspect.iscoroutinefunction(cmd.callback):
                original_callback = cmd.callback
                def make_sync_wrapper(async_func):
                    def sync_wrapper(*args, **kwargs):
                        return asyncio.run(async_func(*args, **kwargs))
                    return sync_wrapper
                cmd.callback = make_sync_wrapper(original_callback)
        
        cli()
    else:
        console.print("âŒ éœ€è¦Python 3.7+ç‰ˆæœ¬", style="red")
        sys.exit(1) 