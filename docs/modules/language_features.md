# è¯­è¨€ç‰¹æ€§æ¨¡å—è®¾è®¡æ–‡æ¡£

## æ¨¡å—æ¦‚è§ˆ

è¯­è¨€ç‰¹æ€§æ¨¡å— (Language Features Module) æ˜¯ `grape-mcp-devtools` ä¸­çš„ä¸€ä¸ªå…±äº«æœåŠ¡å±‚ï¼Œæ—¨åœ¨ä¸ºå…¶ä»–æ¨¡å—ï¼ˆç‰¹åˆ«æ˜¯æ–‡æ¡£å¤„ç†æ¨¡å—ã€æœªæ¥å¯èƒ½çš„ä»£ç åˆ†æ/ç”Ÿæˆå·¥å…·ã€æ™ºèƒ½é‡æ„å·¥å…·ç­‰ï¼‰æä¾›è¶…è¶Šç®€å•æ–‡æœ¬å¤„ç†çš„é«˜çº§å†…å®¹å¤„ç†ã€æ™ºèƒ½åˆ†æå’ŒAIé©±åŠ¨çš„æ•°æ®æ”¶é›†ä¸ç”Ÿæˆèƒ½åŠ›ã€‚å…¶ç›®æ ‡æ˜¯èµ‹äºˆ `grape-mcp-devtools` æ›´æ·±å±‚æ¬¡çš„è¯­ä¹‰ç†è§£å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»è€Œæä¾›æ›´æ™ºèƒ½ã€æ›´ç²¾å‡†çš„è¾…åŠ©åŠŸèƒ½ã€‚

### æ¨¡å—åŸºæœ¬ä¿¡æ¯
- **æ¨¡å—è·¯å¾„**: `src/language_features/` (ä¸»è¦åŒ…æ‹¬ `ai_collector.rs`, `scraper.rs`, `url_discoverer.rs`, `doc_crawler.rs`, `content_analyzer.rs`, `pipeline.rs` ç­‰)
- **ä¸»è¦ä½œç”¨**: æ™ºèƒ½ç½‘é¡µçˆ¬å–ã€åŠ¨æ€å†…å®¹å¤„ç†ã€é«˜çº§æ–‡æœ¬åˆ†æï¼ˆå¦‚ä»£ç å—è¯†åˆ«ã€APIç­¾åæå–ã€NERï¼‰ã€ä¸å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) äº¤äº’ä»¥è¿›è¡Œå†…å®¹æ‘˜è¦/ç”Ÿæˆ/è§£é‡Šã€URLæ¨¡å¼åˆ†æå’Œå‘ç°ã€‚
- **æ ¸å¿ƒç‰¹æ€§**: LLMé›†æˆã€é«˜çº§çˆ¬è™«æŠ€æœ¯ã€å¤šç»´åº¦å†…å®¹åˆ†æã€å¯é…ç½®çš„AIèƒ½åŠ›ã€å¯æ‰©å±•çš„åˆ†æç»„ä»¶ã€‚
- **æœåŠ¡å¯¹è±¡**: `DocProcessor`, `SearchService`, æœªæ¥å¯èƒ½çš„ä»£ç æ™ºèƒ½å·¥å…·ç­‰ã€‚

## æ¶æ„è®¾è®¡

### 1. æ¨¡å—åœ¨ç³»ç»Ÿä¸­çš„ä½ç½®

è¯­è¨€ç‰¹æ€§æ¨¡å—ä½œä¸ºåå°æœåŠ¡ï¼Œè¢«å·¥å…·å±‚ï¼ˆå¦‚ `DocProcessor`ï¼‰æŒ‰éœ€è°ƒç”¨ï¼Œä»¥å¢å¼ºå…¶å†…å®¹è·å–å’Œç†è§£èƒ½åŠ›ã€‚å®ƒä¸ç›´æ¥æš´éœ²ç»™MCPå®¢æˆ·ç«¯ã€‚

```mermaid
graph LR
    A[DocProcessor] --> B(LanguageFeatures Module);
    C[SearchService] --> B;
    D[FutureCodeAnalysisTool] --> B;

    subgraph LanguageFeatures Module
        direction LR
        LF1[AICollector] <--> LF2[ContentAnalyzer];
        LF2 <--> LF3[URLDiscoveryEngine];
        LF3 --> LF4[MCPå·¥å…·ä»£ç†];
        LF4 --> LF5[å¤–éƒ¨MCPå·¥å…·];
        LF1 <--> ExternalLLMs[External LLM APIs (OpenAI, Anthropic)];
        LF2 <--> LocalNLPModels[Small Local NLP Models (rust-bert for NER, whatlang)];
        LF3 <--> SmartURLAnalyzer;
    end

    subgraph External_MCP_Tools [å¤–éƒ¨MCPå·¥å…·]
        Playwright[Playwright MCPæœåŠ¡å™¨]
        Git[Git MCPæœåŠ¡å™¨]
        FileSystem[æ–‡ä»¶ç³»ç»ŸMCPæœåŠ¡å™¨]
    end

    LF5 <--> External_MCP_Tools
```

### 2. å†…éƒ¨ç»„ä»¶æ¶æ„å›¾

```mermaid
digraph LanguageFeaturesInternal {
    rankdir=TB;
    node [shape=box, style=rounded];

    subgraph UserFacingComponents [label="è°ƒç”¨æ–¹ (e.g., DocProcessor)"]
        DocProc [label="DocProcessor"];
    end

    subgraph LanguageFeaturesCore [label="Language Features Module (`src/language_features/`)"]
        AICollector [label="AICollector\n(ai_collector.rs)\n- LLM Interaction (OpenAI, Anthropic)"];
        ContentAnalyzer [label="ContentAnalyzer\n(content_analyzer.rs)\n- Text analysis, code block/API extraction, NER"];
        URLDiscoveryEngine [label="URLDiscoveryEngine\n(url_discoverer.rs)\n- Finds related URLs"];
        SmartURLAnalyzer [label="SmartURLAnalyzer\n(smart_url_analyzer.rs)\n- URL pattern/type analysis"];
    end

    subgraph ExternalDependencies [label="External Libraries & Services"]
        HttpClient [label="HTTP Client (reqwest)"];
        HTMLParser [label="HTML Parser (scraper)"];
        NLPLocal [label="Local NLP (rust-bert, whatlang, natural, guesslang)"];
        LLMRemote [label="Remote LLM APIs (async-openai, etc.)"];
        HFHub [label="Hugging Face Hub (hf-hub for local models)"];
        HeadlessBrowser [label="Headless Browser (optional, e.g., headless_chrome)"];
    end

    DocProc --> AICollector;
    AICollector --> ContentAnalyzer;
    ContentAnalyzer --> URLDiscoveryEngine;
    URLDiscoveryEngine --> SmartURLAnalyzer;
    SmartURLAnalyzer --> URLDiscoveryEngine; 
}
```

### 3. ä¸»è¦ç»„ä»¶è¯´æ˜

#### 3.1 `AICollector` (`ai_collector.rs`)

**èŒè´£**: åˆ©ç”¨äº‘ç«¯LLM APIè¿›è¡Œæ™ºèƒ½å†…å®¹æ”¶é›†ã€åˆ†æå’Œç”Ÿæˆã€‚

**æ ¸å¿ƒåŠŸèƒ½**:
- **æç¤ºè¯é©±åŠ¨çš„å†…å®¹æå–**: æ ¹æ®ç‰¹å®šæç¤ºè¯ä»æ–‡æœ¬ä¸­æå–ç›®æ ‡ä¿¡æ¯ï¼ˆå¦‚ç‰ˆæœ¬å·ã€å®‰è£…å‘½ä»¤ã€APIç­¾åï¼‰
- **å†…å®¹æ‘˜è¦å’Œç»“æ„åŒ–**: å°†é•¿æ–‡æœ¬ï¼ˆå¦‚changelogã€READMEï¼‰è½¬æ¢ä¸ºç»“æ„åŒ–æ‘˜è¦
- **å¤šè¯­è¨€å†…å®¹ç”Ÿæˆ**: åŸºäºç°æœ‰å†…å®¹ç”Ÿæˆæ–‡æ¡£ã€ç¤ºä¾‹æˆ–è§£é‡Š
- **è¯­ä¹‰ç†è§£**: ç†è§£æ–‡æ¡£å†…å®¹çš„è¯­ä¹‰ï¼Œè¾…åŠ©URLå‘ç°å’Œå†…å®¹åˆ†æ

**å…³é”®æ¥å£**:
```rust
pub struct AICollector {
    llm_client: Arc<dyn LLMClient + Send + Sync>,
    prompt_templates: PromptTemplateManager,
    config: AICollectorConfig,
}

impl AICollector {
    pub async fn extract_with_prompt(&self, content: &str, extraction_prompt: &str) -> Result<ExtractedInfo, AIError>;
    pub async fn summarize_changelog(&self, changelog_content: &str, target_version: Option<&str>) -> Result<ChangelogSummary, AIError>;
    pub async fn generate_examples(&self, api_docs: &str, context: &GenerationContext) -> Result<Vec<CodeExample>, AIError>;
    pub async fn analyze_content_type(&self, content: &str) -> Result<ContentTypeAnalysis, AIError>;
}

// æç¤ºè¯é©±åŠ¨çš„ä¿¡æ¯æå–ç¤ºä¾‹
pub struct ExtractionPrompt {
    pub target_content_type: ContentType, // Examples, Changelog, API_Docs, etc.
    pub extraction_rules: Vec<String>,
    pub output_format: OutputFormat,
}

pub struct ExtractedInfo {
    pub structured_data: serde_json::Value,
    pub confidence_score: f32,
    pub source_references: Vec<SourceReference>,
}
```

**æç¤ºè¯æ¨¡æ¿ç¤ºä¾‹**:
```rust
impl PromptTemplateManager {
    pub fn get_changelog_extraction_prompt(&self, library_name: &str) -> String {
        format!(
            "ä»ä»¥ä¸‹{}åº“çš„changelogä¸­æå–å…³é”®ä¿¡æ¯ï¼š
            1. æœ€æ–°ç‰ˆæœ¬å·å’Œå‘å¸ƒæ—¥æœŸ
            2. é‡è¦çš„æ–°åŠŸèƒ½å’Œæ”¹è¿›
            3. ç ´åæ€§å˜æ›´ï¼ˆBreaking Changesï¼‰
            4. å®‰å…¨ä¿®å¤
            
            è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«versionã€dateã€featuresã€breaking_changesã€security_fixeså­—æ®µã€‚
            
            Changelogå†…å®¹ï¼š
            {{content}}",
            library_name
        )
    }
    
    pub fn get_example_extraction_prompt(&self, language: &str) -> String {
        format!(
            "ä»ä»¥ä¸‹{}ä»£ç æ–‡æ¡£ä¸­æå–å®Œæ•´çš„ä»£ç ç¤ºä¾‹ï¼š
            1. æå–æ‰€æœ‰å¯è¿è¡Œçš„ä»£ç å—
            2. ä¸ºæ¯ä¸ªç¤ºä¾‹æ·»åŠ ç®€çŸ­æè¿°
            3. æ ‡æ³¨ç¤ºä¾‹çš„å¤æ‚åº¦çº§åˆ«ï¼ˆåˆçº§/ä¸­çº§/é«˜çº§ï¼‰
            4. æå–ç›¸å…³çš„ä¾èµ–å’Œå¯¼å…¥è¯­å¥
            
            è¿”å›æ ¼å¼ï¼šJSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«codeã€descriptionã€levelã€dependencieså­—æ®µã€‚
            
            æ–‡æ¡£å†…å®¹ï¼š
            {{content}}",
            language
        )
    }
}
```

#### 3.2 `URLDiscoveryEngine` (`url_discovery.rs`)

**èŒè´£**: æ™ºèƒ½å‘ç°ä¸ç‰¹å®šç¼–ç¨‹è¯­è¨€ã€åº“æˆ–ä¸»é¢˜ç›¸å…³çš„URLèµ„æºã€‚

**æ ¸å¿ƒåŠŸèƒ½**:
- **åŸºäºä¸Šä¸‹æ–‡çš„URLç”Ÿæˆ**: æ ¹æ®`DiscoveryContext`ä¸­çš„è¯­è¨€ã€åº“åã€å†…å®¹ç±»å‹ç”Ÿæˆå€™é€‰URL
- **URLæœ‰æ•ˆæ€§éªŒè¯**: ä½¿ç”¨`SmartURLAnalyzer`è¯„ä¼°URLçš„ç›¸å…³æ€§å’Œå¯è®¿é—®æ€§
- **å¤šæºURLå‘ç°**: æ”¯æŒå®˜æ–¹æ–‡æ¡£ã€GitHubã€åŒ…ç®¡ç†å™¨ç­‰å¤šç§URLæº
- **åŠ¨æ€URLæ¨¡å¼å­¦ä¹ **: åŸºäºæˆåŠŸçš„URLæ¨¡å¼æ”¹è¿›å‘ç°ç®—æ³•

**å…³é”®æ¥å£**:
```rust
pub struct URLDiscoveryEngine {
    mcp_client: Arc<MCPClientManager>, // ç”¨äºè°ƒç”¨å¤–éƒ¨MCPå·¥å…·
    url_patterns: URLPatternDatabase,
    smart_analyzer: SmartURLAnalyzer,
    config: DiscoveryConfig,
}

impl URLDiscoveryEngine {
    pub async fn discover_urls(&self, context: &DiscoveryContext) -> Result<Vec<DiscoveredURL>, DiscoveryError>;
    pub async fn validate_urls(&self, urls: &[String]) -> Vec<URLValidationResult>;
    pub async fn find_documentation_urls(&self, library_name: &str, language: &str, content_type: ContentType) -> Result<Vec<String>, DiscoveryError>;
}

pub struct DiscoveryContext {
    pub language: String,
    pub library_name: String,
    pub target_content_type: ContentType, // Examples, Changelog, API_Docs, README
    pub version_constraint: Option<String>,
    pub additional_keywords: Vec<String>,
}

pub enum ContentType {
    Examples,
    Changelog,
    ApiDocs,
    Readme,
    Installation,
    Tutorial,
}

pub struct DiscoveredURL {
    pub url: String,
    pub relevance_score: f32,
    pub content_type: ContentType,
    pub source: URLSource, // Official, GitHub, PackageManager, Community
}
```

**URLå‘ç°ç­–ç•¥ç¤ºä¾‹**:
```rust
impl URLDiscoveryEngine {
    async fn generate_candidate_urls(&self, context: &DiscoveryContext) -> Vec<String> {
        let mut urls = Vec::new();
        
        match context.target_content_type {
            ContentType::Examples => {
                // GitHubç¤ºä¾‹æœç´¢
                urls.push(format!("https://github.com/search?q={} {} examples language:{}", 
                    context.library_name, context.language, context.language));
                
                // å®˜æ–¹æ–‡æ¡£ç¤ºä¾‹é¡µé¢
                if let Some(official_base) = self.get_official_docs_base(&context.language, &context.library_name) {
                    urls.push(format!("{}/examples", official_base));
                    urls.push(format!("{}/tutorial", official_base));
                }
            },
            ContentType::Changelog => {
                // GitHub releasesé¡µé¢
                if let Some(repo_url) = self.find_github_repo(&context.library_name, &context.language).await {
                    urls.push(format!("{}/releases", repo_url));
                    urls.push(format!("{}/blob/main/CHANGELOG.md", repo_url));
                }
                
                // åŒ…ç®¡ç†å™¨changelog
                urls.extend(self.get_package_manager_changelog_urls(context));
            },
            ContentType::ApiDocs => {
                // å®˜æ–¹APIæ–‡æ¡£
                urls.extend(self.get_official_api_docs_urls(context));
                
                // ç¬¬ä¸‰æ–¹æ–‡æ¡£ç«™ç‚¹
                urls.extend(self.get_third_party_docs_urls(context));
            },
            _ => {}
        }
        
        urls
    }
}
```

#### 3.3 `ContentAnalyzer` (`content_analyzer.rs`)

**èŒè´£**: åˆ†æå’Œå¤„ç†ä»å„ç§æºè·å–çš„å†…å®¹ï¼Œæå–æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚

**æ ¸å¿ƒåŠŸèƒ½**:
- **å†…å®¹ç±»å‹è¯†åˆ«**: è‡ªåŠ¨è¯†åˆ«å†…å®¹æ˜¯APIæ–‡æ¡£ã€ç¤ºä¾‹ä»£ç ã€changelogç­‰
- **ç»“æ„åŒ–å†…å®¹æå–**: ä»HTMLã€Markdownã€çº¯æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯
- **ä»£ç å—è¯†åˆ«å’Œåˆ†ç±»**: è¯†åˆ«å¹¶åˆ†ç±»ä»£ç ç¤ºä¾‹ã€APIç­¾åã€é…ç½®æ–‡ä»¶ç­‰
- **å¤šè¯­è¨€å†…å®¹å¤„ç†**: æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€çš„è¯­æ³•é«˜äº®å’Œç»“æ„åˆ†æ

**å…³é”®æ¥å£**:
```rust
pub struct ContentAnalyzer {
    mcp_client: Arc<MCPClientManager>, // ç”¨äºè°ƒç”¨å¤–éƒ¨å·¥å…·è·å–å†…å®¹
    nlp_models: LocalNLPModels,
    analysis_config: AnalysisConfig,
}

impl ContentAnalyzer {
    pub async fn analyze_web_content(&self, url: &str, analysis_config: &AnalysisConfig) -> Result<AnalyzedContent, AnalysisError>;
    pub async fn extract_code_examples(&self, content: &str, language: &str) -> Result<Vec<CodeExample>, AnalysisError>;
    pub async fn parse_changelog(&self, content: &str) -> Result<ParsedChangelog, AnalysisError>;
    pub async fn identify_content_structure(&self, content: &str) -> Result<ContentStructure, AnalysisError>;
}

pub struct AnalysisConfig {
    pub target_content_type: ContentType,
    pub extraction_selectors: Vec<String>, // CSSé€‰æ‹©å™¨ç”¨äºPlaywright
    pub language_context: Option<String>,
    pub depth_limit: usize,
}

pub struct AnalyzedContent {
    pub content_type: ContentType,
    pub structured_data: ContentStructure,
    pub extracted_code: Vec<CodeExample>,
    pub metadata: ContentMetadata,
    pub quality_score: f32,
}
```

**å†…å®¹åˆ†ææµç¨‹**:
```rust
impl ContentAnalyzer {
    pub async fn analyze_web_content(&self, url: &str, config: &AnalysisConfig) -> Result<AnalyzedContent, AnalysisError> {
        // 1. ä½¿ç”¨Playwright MCPè·å–é¡µé¢å†…å®¹
        let page_content = self.fetch_page_content(url, config).await?;
        
        // 2. æ ¹æ®å†…å®¹ç±»å‹åº”ç”¨ç‰¹å®šçš„åˆ†æé€»è¾‘
        let structured_data = match config.target_content_type {
            ContentType::Examples => self.extract_examples_structure(&page_content).await?,
            ContentType::Changelog => self.extract_changelog_structure(&page_content).await?,
            ContentType::ApiDocs => self.extract_api_structure(&page_content).await?,
            _ => self.extract_generic_structure(&page_content).await?,
        };
        
        // 3. æå–ä»£ç å—
        let extracted_code = self.extract_code_examples(&page_content.text, 
            config.language_context.as_deref().unwrap_or("")).await?;
        
        // 4. ç”Ÿæˆå†…å®¹è´¨é‡è¯„åˆ†
        let quality_score = self.calculate_quality_score(&structured_data, &extracted_code);
        
        Ok(AnalyzedContent {
            content_type: config.target_content_type.clone(),
            structured_data,
            extracted_code,
            metadata: page_content.metadata,
            quality_score,
        })
    }
    
    async fn fetch_page_content(&self, url: &str, config: &AnalysisConfig) -> Result<PageContent, AnalysisError> {
        // è°ƒç”¨Playwright MCPæœåŠ¡å™¨è·å–é¡µé¢å†…å®¹
        let params = json!({
            "url": url,
            "wait_for": "networkidle",
            "selectors": config.extraction_selectors,
            "extract_text": true,
            "extract_html": true
        });
        
        let result = self.mcp_client.call_tool("playwright_extract_content", params).await
            .map_err(|e| AnalysisError::ContentFetchFailed(e.to_string()))?;
        
        // è½¬æ¢MCPç»“æœä¸ºå†…éƒ¨æ ¼å¼
        PageContent::from_mcp_result(result)
    }
}
```

#### 3.4 ç§»é™¤çš„ç»„ä»¶

ä»¥ä¸‹ç»„ä»¶å°†è¢«ç§»é™¤æˆ–é‡æ„ï¼Œå› ä¸ºå®ƒä»¬çš„åŠŸèƒ½ç°åœ¨ç”±å¤–éƒ¨MCPå·¥å…·æä¾›ï¼š

- ~~`IntelligentScraper`~~: ç½‘é¡µçˆ¬è™«åŠŸèƒ½ç”±Playwright MCPæœåŠ¡å™¨æä¾›
- ~~`DocCrawler`~~: æ–‡æ¡£çˆ¬å–åŠŸèƒ½ç”±Playwrightå’ŒGit MCPæœåŠ¡å™¨æä¾›
- ~~`enhanced_content_pipeline.rs`~~: å†…å®¹å¤„ç†æµæ°´çº¿ç®€åŒ–ï¼Œç›´æ¥ä½¿ç”¨MCPå·¥å…·

### 4. AIé›†æˆ

#### 4.1 äº‘ç«¯LLMé›†æˆ

**æ”¯æŒçš„LLMæœåŠ¡**:
- **OpenAI API**: GPT-4, GPT-3.5-turboç”¨äºå†…å®¹ç”Ÿæˆå’Œåˆ†æ
- **Anthropic API**: Claudeç”¨äºå¤æ‚æ–‡æ¡£ç†è§£å’Œæ‘˜è¦
- **å…¶ä»–å…¼å®¹OpenAI APIçš„æœåŠ¡**: å¦‚Azure OpenAI, æœ¬åœ°éƒ¨ç½²çš„å…¼å®¹æœåŠ¡

**LLMå®¢æˆ·ç«¯æŠ½è±¡**:
```rust
#[async_trait]
pub trait LLMClient {
    async fn generate_completion(&self, prompt: &str, config: &CompletionConfig) -> Result<String, LLMError>;
    async fn generate_structured(&self, prompt: &str, schema: &serde_json::Value) -> Result<serde_json::Value, LLMError>;
    async fn generate_embeddings(&self, texts: &[String]) -> Result<Vec<Vec<f32>>, LLMError>;
}

pub struct OpenAIClient {
    client: async_openai::Client<async_openai::config::OpenAIConfig>,
    default_model: String,
}

pub struct AnthropicClient {
    client: anthropic_sdk::Client,
    default_model: String,
}
```

#### 4.2 æœ¬åœ°å°å‹NLPæ¨¡å‹

**æ”¯æŒçš„æœ¬åœ°æ¨¡å‹**:
- **rust-bert**: ç”¨äºå‘½åå®ä½“è¯†åˆ«(NER)ã€æ–‡æœ¬åˆ†ç±»
- **whatlang**: è¯­è¨€æ£€æµ‹
- **tokenizers**: æ–‡æœ¬åˆ†è¯å’Œé¢„å¤„ç†

**æœ¬åœ°æ¨¡å‹ç”¨é€”**:
```rust
pub struct LocalNLPModels {
    ner_model: Option<NERModel>,
    language_detector: whatlang::Detector,
    tokenizer: Option<tokenizers::Tokenizer>,
}

impl LocalNLPModels {
    pub fn detect_language(&self, text: &str) -> Option<whatlang::Lang> {
        self.language_detector.detect(text)
    }
    
    pub async fn extract_entities(&self, text: &str) -> Result<Vec<Entity>, NLPError> {
        if let Some(ref model) = self.ner_model {
            model.predict(text).await
        } else {
            Ok(Vec::new())
        }
    }
    
    pub fn classify_code_block(&self, code: &str) -> CodeBlockType {
        // åŸºäºç®€å•è§„åˆ™å’Œæ¨¡å¼åŒ¹é…çš„ä»£ç å—åˆ†ç±»
        if code.contains("import ") || code.contains("from ") {
            CodeBlockType::Import
        } else if code.contains("def ") || code.contains("function ") {
            CodeBlockType::Function
        } else if code.contains("class ") {
            CodeBlockType::Class
        } else {
            CodeBlockType::Example
        }
    }
}
```

### 5. é…ç½®ä¸å¯æ‰©å±•æ€§

#### 5.1 é…ç½®ç®¡ç†

**è¯­è¨€ç‰¹æ€§é…ç½®ç¤ºä¾‹**:
```json
{
  "ai_collector": {
    "llm_provider": "openai",
    "default_model": "gpt-4",
    "max_tokens": 4000,
    "temperature": 0.1,
    "prompt_templates": {
      "changelog_extraction": "templates/changelog_extraction.txt",
      "example_extraction": "templates/example_extraction.txt"
    }
  },
  "url_discovery": {
    "max_urls_per_search": 10,
    "timeout_seconds": 30,
    "official_docs_patterns": {
      "python": ["https://docs.python.org/", "https://pypi.org/project/{}/"],
      "rust": ["https://docs.rs/{}/", "https://crates.io/crates/{}"],
      "javascript": ["https://www.npmjs.com/package/{}", "https://developer.mozilla.org/"]
    }
  },
  "content_analyzer": {
    "enable_local_nlp": true,
    "max_content_size": "10MB",
    "supported_languages": ["python", "rust", "javascript", "typescript", "java", "go"]
  },
  "mcp_integration": {
    "playwright_server": "playwright",
    "git_server": "git",
    "filesystem_server": "filesystem",
    "timeout_seconds": 60
  }
}
```

#### 5.2 æ ¸å¿ƒæµç¨‹ç¤ºä¾‹

**åŸºäºæç¤ºè¯çš„ç›®æ ‡æ€§æ–‡æ¡£è·å–æµç¨‹**:

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·è¯·æ±‚
    participant URLEngine as URLå‘ç°å¼•æ“
    participant MCPClient as MCPå®¢æˆ·ç«¯
    participant Playwright as Playwright MCP
    participant ContentAnalyzer as å†…å®¹åˆ†æå™¨
    participant AICollector as AIæ”¶é›†å™¨
    participant LLM as äº‘ç«¯LLM

    User->>URLEngine: è¯·æ±‚Python requestsåº“ç¤ºä¾‹
    Note over User: å‚æ•°: language="python", library="requests", type="examples"
    
    URLEngine->>URLEngine: ç”Ÿæˆå€™é€‰URLåˆ—è¡¨
    Note over URLEngine: GitHubæœç´¢ã€å®˜æ–¹æ–‡æ¡£ã€æ•™ç¨‹ç«™ç‚¹
    
    URLEngine->>MCPClient: éªŒè¯URLå¯è®¿é—®æ€§
    MCPClient->>Playwright: æ‰¹é‡æ£€æŸ¥URLçŠ¶æ€
    Playwright-->>MCPClient: è¿”å›å¯è®¿é—®URLåˆ—è¡¨
    MCPClient-->>URLEngine: è¿”å›éªŒè¯ç»“æœ
    
    URLEngine->>ContentAnalyzer: åˆ†ææœ€ä½³URLå†…å®¹
    ContentAnalyzer->>MCPClient: è¯·æ±‚é¡µé¢å†…å®¹æå–
    MCPClient->>Playwright: å¯¼èˆªå¹¶æå–å†…å®¹
    Note over Playwright: ä½¿ç”¨CSSé€‰æ‹©å™¨æå–ä»£ç ç¤ºä¾‹
    
    Playwright-->>MCPClient: è¿”å›é¡µé¢å†…å®¹å’Œä»£ç å—
    MCPClient-->>ContentAnalyzer: è¿”å›ç»“æ„åŒ–å†…å®¹
    
    ContentAnalyzer->>AICollector: è¯·æ±‚æ™ºèƒ½å†…å®¹å¤„ç†
    AICollector->>LLM: å‘é€æç¤ºè¯å’Œå†…å®¹
    Note over LLM: æç¤ºè¯: "æå–Python requestsåº“çš„å®Œæ•´ä»£ç ç¤ºä¾‹..."
    
    LLM-->>AICollector: è¿”å›ç»“æ„åŒ–ç¤ºä¾‹æ•°æ®
    AICollector-->>ContentAnalyzer: è¿”å›å¤„ç†åå†…å®¹
    ContentAnalyzer-->>URLEngine: è¿”å›æœ€ç»ˆç»“æœ
    URLEngine-->>User: è¿”å›é«˜è´¨é‡ç¤ºä¾‹é›†åˆ
```

## ğŸš€ åŸºäºMCPçš„å¤šAgentçˆ¬è™«ç³»ç»Ÿ

### 5. å¤šAgentæ™ºèƒ½çˆ¬è™«æ¶æ„

å½“ä¼ ç»ŸCLIå·¥å…·æ— æ³•æä¾›æ»¡æ„çš„æ–‡æ¡£æ—¶ï¼Œè¯­è¨€ç‰¹æ€§æ¨¡å—æä¾›åŸºäºMCPå®¢æˆ·ç«¯çš„å¤šagentçˆ¬è™«ç³»ç»Ÿï¼Œä¸“æ³¨äºå‡†ç¡®çš„å†…å®¹æå–å’Œèåˆã€‚

#### 5.1 æ ¸å¿ƒAgentæ¶æ„ï¼ˆåŸºäºLLMé©±åŠ¨ï¼‰
```rust
pub struct MultiAgentCrawlerSystem {
    // æ ¸å¿ƒåè°ƒå™¨
    task_coordinator: Arc<TaskCoordinator>,
    
    // LLMé©±åŠ¨çš„ä¸“ç”¨Agent
    url_discovery_agent: Arc<URLDiscoveryAgent>,
    content_extraction_agent: Arc<ContentExtractionAgent>,
    page_navigation_agent: Arc<PageNavigationAgent>,
    content_fusion_agent: Arc<ContentFusionAgent>,
    
    // MCPå®¢æˆ·ç«¯ç®¡ç†
    mcp_client_manager: Arc<MCPClientManager>,
    
    // LLMæœåŠ¡é›†æˆ - æ ¸å¿ƒç»„ä»¶
    llm_orchestrator: Arc<LLMOrchestrator>,
    prompt_manager: Arc<PromptManager>,
    
    // é…ç½®
    crawler_config: CrawlerConfig,
}

impl MultiAgentCrawlerSystem {
    pub async fn crawl_documentation(&self, request: DocumentationRequest) -> CrawledDocumentation {
        // ç¬¬ä¸€é˜¶æ®µï¼šLLMé©±åŠ¨çš„ä»»åŠ¡åˆ†æå’ŒURLå‘ç°
        let task_analysis = self.task_coordinator.analyze_task_with_llm(&request).await?;
        let discovered_urls = self.url_discovery_agent.discover_urls_with_llm(&task_analysis).await?;
        
        // ç¬¬äºŒé˜¶æ®µï¼šLLMå¼•å¯¼çš„å¤šé¡µé¢å¹¶è¡Œçˆ¬å–
        let crawl_results = self.content_extraction_agent.crawl_with_llm_guidance(discovered_urls).await?;
        
        // ç¬¬ä¸‰é˜¶æ®µï¼šLLMé©±åŠ¨çš„å†…å®¹èåˆ
        let fused_content = self.content_fusion_agent.fuse_content_with_llm(crawl_results).await?;
        
        CrawledDocumentation {
            content: fused_content,
            sources: crawl_results.iter().map(|r| r.source_url.clone()).collect(),
            crawl_metadata: self.create_crawl_metadata(),
        }
    }
}
```

#### 5.2 LLMé©±åŠ¨çš„ä»»åŠ¡åè°ƒAgent
```rust
pub struct TaskCoordinator {
    mcp_client: Arc<MCPClientManager>,
    llm_orchestrator: Arc<LLMOrchestrator>,
    prompt_manager: Arc<PromptManager>,
}

impl TaskCoordinator {
    pub async fn analyze_task_with_llm(&self, request: &DocumentationRequest) -> TaskAnalysis {
        // ä½¿ç”¨LLMæ·±åº¦ç†è§£ç”¨æˆ·æ„å›¾
        let intent_analysis = self.analyze_user_intent_with_llm(request).await?;
        
        // LLMååŠ©è¯†åˆ«ç›®æ ‡æŠ€æœ¯æ ˆå’Œç›¸å…³æ¦‚å¿µ
        let tech_context = self.extract_technical_context_with_llm(request).await?;
        
        // LLMç¡®å®šæœ€ä½³çˆ¬å–ç­–ç•¥
        let crawl_strategy = self.determine_crawl_strategy_with_llm(&intent_analysis, &tech_context).await?;
        
        TaskAnalysis {
            user_intent: intent_analysis,
            technical_context: tech_context,
            content_type: crawl_strategy.content_type,
            target_sites: crawl_strategy.target_sites,
            crawl_depth: crawl_strategy.depth,
            priority_patterns: crawl_strategy.patterns,
        }
    }
    
    async fn analyze_user_intent_with_llm(&self, request: &DocumentationRequest) -> UserIntent {
        let prompt = self.prompt_manager.create_intent_analysis_prompt(request);
        
        let llm_response = self.llm_orchestrator.generate_structured_response(
            &prompt,
            &json_schema_for_user_intent()
        ).await?;
        
        UserIntent::from_llm_response(llm_response)
    }
    
    async fn extract_technical_context_with_llm(&self, request: &DocumentationRequest) -> TechnicalContext {
        let prompt = format!(
            "åˆ†æä»¥ä¸‹æŠ€æœ¯æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
            è¯­è¨€: {}
            ç›®æ ‡: {}
            æŸ¥è¯¢: {}
            
            è¯·è¯†åˆ«ï¼š
            1. ä¸»è¦æŠ€æœ¯æ ˆå’Œæ¡†æ¶
            2. ç›¸å…³çš„ç”Ÿæ€ç³»ç»Ÿç»„ä»¶
            3. å¯èƒ½çš„ç”¨ä¾‹åœºæ™¯
            4. æŠ€æœ¯å¤æ‚åº¦çº§åˆ«
            5. æ¨èçš„æ–‡æ¡£æ¥æºç±»å‹
            
            ä»¥JSONæ ¼å¼è¿”å›ç»“æ„åŒ–ä¿¡æ¯ã€‚",
            request.language, request.target, request.query
        );
        
        let llm_response = self.llm_orchestrator.generate_structured_response(
            &prompt,
            &json_schema_for_technical_context()
        ).await?;
        
        TechnicalContext::from_llm_response(llm_response)
    }
}
```

#### 5.3 LLMå¢å¼ºçš„URLå‘ç°Agent
```rust
pub struct URLDiscoveryAgent {
    mcp_client: Arc<MCPClientManager>,
    llm_orchestrator: Arc<LLMOrchestrator>,
    url_patterns: URLPatternDatabase,
}

impl URLDiscoveryAgent {
    pub async fn discover_urls_with_llm(&self, task: &TaskAnalysis) -> Vec<DiscoveredURL> {
        // ç¬¬ä¸€æ­¥ï¼šåŸºäºæ¨¡å¼ç”Ÿæˆå€™é€‰URL
        let pattern_based_urls = self.generate_pattern_based_urls(task).await;
        
        // ç¬¬äºŒæ­¥ï¼šLLMç”Ÿæˆåˆ›æ–°çš„URLå‘ç°ç­–ç•¥
        let llm_generated_urls = self.generate_urls_with_llm_creativity(task).await?;
        
        // ç¬¬ä¸‰æ­¥ï¼šåˆå¹¶å¹¶é€šè¿‡LLMè¯„ä¼°ç›¸å…³æ€§
        let all_candidates = [pattern_based_urls, llm_generated_urls].concat();
        let llm_scored_urls = self.score_urls_with_llm(all_candidates, task).await?;
        
        // ç¬¬å››æ­¥ï¼šPlaywrightéªŒè¯å¯è®¿é—®æ€§
        let validated_urls = self.validate_urls_with_playwright(llm_scored_urls).await?;
        
        validated_urls
    }
    
    async fn generate_urls_with_llm_creativity(&self, task: &TaskAnalysis) -> Result<Vec<String>, CrawlerError> {
        let prompt = format!(
            "åŸºäºä»¥ä¸‹ä»»åŠ¡åˆ†æï¼Œåˆ›é€ æ€§åœ°ç”Ÿæˆå¯èƒ½åŒ…å«ç›¸å…³æ–‡æ¡£çš„URLï¼š
            
            æŠ€æœ¯ä¸Šä¸‹æ–‡: {:?}
            å†…å®¹ç±»å‹: {:?}
            ç›®æ ‡æŠ€æœ¯: {}
            
            è¯·è€ƒè™‘ï¼š
            1. å®˜æ–¹æ–‡æ¡£ç«™ç‚¹çš„å˜ä½“URL
            2. ç¤¾åŒºç»´æŠ¤çš„æ–‡æ¡£èµ„æº
            3. GitHubä»“åº“ä¸­çš„ç‰¹å®šæ–‡æ¡£æ–‡ä»¶
            4. ç¬¬ä¸‰æ–¹æ•™ç¨‹å’Œåšå®¢ç«™ç‚¹
            5. æŠ€æœ¯è®ºå›å’Œé—®ç­”ç¤¾åŒº
            
            è¿”å›JSONæ•°ç»„ï¼Œæ¯ä¸ªURLåŒ…å«urlå’Œreasoningå­—æ®µã€‚",
            task.technical_context,
            task.content_type,
            task.technical_context.primary_technology
        );
        
        let llm_response = self.llm_orchestrator.generate_structured_response(
            &prompt,
            &json_schema_for_url_suggestions()
        ).await?;
        
        Ok(extract_urls_from_llm_response(llm_response))
    }
    
    async fn score_urls_with_llm(&self, urls: Vec<String>, task: &TaskAnalysis) -> Result<Vec<ScoredURL>, CrawlerError> {
        let prompt = format!(
            "è¯„ä¼°ä»¥ä¸‹URLåˆ—è¡¨å¯¹äºç‰¹å®šæ–‡æ¡£éœ€æ±‚çš„ç›¸å…³æ€§ï¼š
            
            ä»»åŠ¡éœ€æ±‚: {:?}
            URLåˆ—è¡¨: {:?}
            
            ä¸ºæ¯ä¸ªURLè¯„åˆ†ï¼ˆ0-1ï¼‰å¹¶è¯´æ˜ç†ç”±ã€‚è€ƒè™‘ï¼š
            1. URLç»“æ„ä¸éœ€æ±‚çš„åŒ¹é…åº¦
            2. ç½‘ç«™çš„æƒå¨æ€§å’Œå¯é æ€§
            3. å¯èƒ½åŒ…å«ç›®æ ‡å†…å®¹çš„æ¦‚ç‡
            4. å†…å®¹çš„æ—¶æ•ˆæ€§å’Œå‡†ç¡®æ€§
            
            è¿”å›JSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«urlã€scoreã€reasoningå­—æ®µã€‚",
            task.user_intent,
            urls
        );
        
        let llm_response = self.llm_orchestrator.generate_structured_response(
            &prompt,
            &json_schema_for_url_scoring()
        ).await?;
        
        Ok(ScoredURL::from_llm_response(llm_response))
    }
}
```

#### 5.4 LLMå¢å¼ºçš„å†…å®¹æå–Agent
```rust
pub struct ContentExtractionAgent {
    mcp_client: Arc<MCPClientManager>,
    llm_orchestrator: Arc<LLMOrchestrator>,
    extraction_config: ExtractionConfig,
}

impl ContentExtractionAgent {
    pub async fn crawl_with_llm_guidance(&self, urls: Vec<DiscoveredURL>) -> Vec<CrawlResult> {
        let mut crawl_tasks = Vec::new();
        
        for url in urls {
            let task = self.crawl_single_page_with_llm(url);
            crawl_tasks.push(task);
        }
        
        let results = futures::try_join_all(crawl_tasks).await?;
        results.into_iter().flatten().collect()
    }
    
    async fn crawl_single_page_with_llm(&self, url: DiscoveredURL) -> Option<CrawlResult> {
        // ç¬¬ä¸€æ­¥ï¼šLLMåˆ†æé¡µé¢ç»“æ„å’Œå†…å®¹ç±»å‹
        let page_analysis = self.analyze_page_with_llm(&url).await?;
        
        // ç¬¬äºŒæ­¥ï¼šåŸºäºLLMåˆ†æè°ƒæ•´æå–ç­–ç•¥
        let extraction_strategy = self.adapt_extraction_strategy_with_llm(&page_analysis).await?;
        
        // ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨Playwrightæ‰§è¡Œæå–
        let page_content = self.extract_with_playwright(&url, &extraction_strategy).await?;
        
        // ç¬¬å››æ­¥ï¼šLLMåå¤„ç†å’Œç»“æ„åŒ–å†…å®¹
        let structured_content = self.structure_content_with_llm(&page_content, &url.content_type).await?;
        
        // ç¬¬äº”æ­¥ï¼šLLMæŒ‡å¯¼çš„å­é¡µé¢å‘ç°
        let sub_pages = self.discover_sub_pages_with_llm(&structured_content, &url).await;
        
        let mut all_content = vec![structured_content];
        for sub_url in sub_pages {
            if let Some(sub_content) = self.extract_sub_page_with_llm(&sub_url).await {
                all_content.push(sub_content);
            }
        }
        
        Some(CrawlResult {
            source_url: url.url,
            content_fragments: all_content,
            content_type: url.content_type,
            crawl_timestamp: chrono::Utc::now(),
        })
    }
    
    async fn analyze_page_with_llm(&self, url: &DiscoveredURL) -> Option<PageAnalysis> {
        // å…ˆè·å–é¡µé¢çš„åŸºæœ¬ä¿¡æ¯
        let page_preview = self.get_page_preview_with_playwright(url).await?;
        
        let prompt = format!(
            "åˆ†æä»¥ä¸‹ç½‘é¡µçš„ç»“æ„å’Œå†…å®¹ç‰¹å¾ï¼š
            
            URL: {}
            é¡µé¢æ ‡é¢˜: {}
            é¡µé¢å¤§çº²: {}
            ä¸»è¦å…ƒç´ : {:?}
            
            è¯·ç¡®å®šï¼š
            1. é¡µé¢çš„ä¸»è¦å†…å®¹ç±»å‹ï¼ˆæ–‡æ¡£ã€æ•™ç¨‹ã€APIå‚è€ƒã€ç¤ºä¾‹ç­‰ï¼‰
            2. æœ€ä½³çš„å†…å®¹æå–é€‰æ‹©å™¨
            3. å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†çš„åŠ¨æ€å†…å®¹
            4. ç›¸å…³å­é¡µé¢çš„é“¾æ¥æ¨¡å¼
            5. å†…å®¹çš„ç»“æ„åŒ–ç¨‹åº¦å’Œè´¨é‡è¯„ä¼°
            
            è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœã€‚",
            url.url,
            page_preview.title,
            page_preview.outline,
            page_preview.main_elements
        );
        
        let llm_response = self.llm_orchestrator.generate_structured_response(
            &prompt,
            &json_schema_for_page_analysis()
        ).await.ok()?;
        
        PageAnalysis::from_llm_response(llm_response)
    }
    
    async fn structure_content_with_llm(&self, raw_content: &PageContent, content_type: &ContentType) -> Option<StructuredContent> {
        let prompt = format!(
            "å°†ä»¥ä¸‹åŸå§‹ç½‘é¡µå†…å®¹ç»“æ„åŒ–ä¸ºæœ‰ç”¨çš„æ–‡æ¡£æ ¼å¼ï¼š
            
            å†…å®¹ç±»å‹: {:?}
            åŸå§‹æ–‡æœ¬: {}
            HTMLç»“æ„: {}
            
            è¯·æå–å’Œæ•´ç†ï¼š
            1. ä¸»è¦çš„æ–‡æœ¬å†…å®¹ï¼ˆå»é™¤å¯¼èˆªã€å¹¿å‘Šç­‰æ— å…³ä¿¡æ¯ï¼‰
            2. ä»£ç ç¤ºä¾‹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            3. é‡è¦çš„æ ‡é¢˜å’Œç« èŠ‚ç»“æ„
            4. å…³é”®çš„é“¾æ¥å’Œå¼•ç”¨
            5. å®ç”¨çš„å…ƒæ•°æ®ä¿¡æ¯
            
            è¿”å›JSONæ ¼å¼çš„ç»“æ„åŒ–å†…å®¹ã€‚",
            content_type,
            raw_content.text.chars().take(2000).collect::<String>(),
            raw_content.html.as_deref().unwrap_or("").chars().take(1000).collect::<String>()
        );
        
        let llm_response = self.llm_orchestrator.generate_structured_response(
            &prompt,
            &json_schema_for_structured_content()
        ).await.ok()?;
        
        StructuredContent::from_llm_response(llm_response)
    }
    
    async fn discover_sub_pages_with_llm(&self, content: &StructuredContent, base_url: &DiscoveredURL) -> Vec<DiscoveredURL> {
        let prompt = format!(
            "åŸºäºå½“å‰é¡µé¢å†…å®¹ï¼Œæ™ºèƒ½è¯†åˆ«ç›¸å…³çš„å­é¡µé¢é“¾æ¥ï¼š
            
            å½“å‰é¡µé¢: {}
            å†…å®¹ç±»å‹: {:?}
            é¡µé¢å†…å®¹: {}
            å‘ç°çš„é“¾æ¥: {:?}
            
            è¯·è¯†åˆ«å“ªäº›é“¾æ¥æœ€å¯èƒ½åŒ…å«ç›¸å…³çš„è¡¥å……ä¿¡æ¯ï¼š
            1. ç›¸å…³çš„ä»£ç ç¤ºä¾‹
            2. è¯¦ç»†çš„APIæ–‡æ¡£
            3. æ•™ç¨‹çš„åç»­ç« èŠ‚
            4. ç›¸å…³çš„é…ç½®æŒ‡å—
            
            è¿”å›æœ€ç›¸å…³çš„3-5ä¸ªé“¾æ¥ï¼ŒåŒ…å«URLå’Œç›¸å…³æ€§ç†ç”±ã€‚",
            base_url.url,
            base_url.content_type,
            content.text.chars().take(1500).collect::<String>(),
            content.links
        );
        
        if let Ok(llm_response) = self.llm_orchestrator.generate_structured_response(
            &prompt,
            &json_schema_for_sub_page_discovery()
        ).await {
            return DiscoveredURL::from_llm_subpage_response(llm_response, base_url);
        }
        
        vec![]
    }
}
```

#### 5.5 LLMé©±åŠ¨çš„å†…å®¹èåˆAgent
```rust
pub struct ContentFusionAgent {
    llm_orchestrator: Arc<LLMOrchestrator>,
    fusion_rules: FusionRules,
}

impl ContentFusionAgent {
    pub async fn fuse_content_with_llm(&self, crawl_results: Vec<CrawlResult>) -> FusedContent {
        // ç¬¬ä¸€æ­¥ï¼šLLMåˆ†ææ‰€æœ‰å†…å®¹çš„ç›¸å…³æ€§å’Œé‡å åº¦
        let content_analysis = self.analyze_content_relationships_with_llm(&crawl_results).await?;
        
        // ç¬¬äºŒæ­¥ï¼šLLMé©±åŠ¨çš„å†…å®¹åˆ†ç»„å’Œä¼˜å…ˆçº§æ’åº
        let grouped_content = self.group_content_with_llm(&crawl_results, &content_analysis).await?;
        
        // ç¬¬ä¸‰æ­¥ï¼šä¸ºæ¯ç§å†…å®¹ç±»å‹è¿›è¡ŒLLMå¢å¼ºçš„èåˆ
        let mut fused_sections = Vec::new();
        for (content_type, content_group) in grouped_content {
            let section = self.fuse_content_type_with_llm(content_type, content_group).await;
            fused_sections.push(section);
        }
        
        // ç¬¬å››æ­¥ï¼šLLMç»„ç»‡æœ€ç»ˆæ–‡æ¡£ç»“æ„
        let final_structure = self.organize_final_structure_with_llm(fused_sections).await?;
        
        FusedContent {
            sections: final_structure.sections,
            total_sources: crawl_results.len(),
            fusion_timestamp: chrono::Utc::now(),
            quality_score: final_structure.quality_score,
        }
    }
    
    async fn analyze_content_relationships_with_llm(&self, crawl_results: &[CrawlResult]) -> ContentRelationshipAnalysis {
        let content_summaries: Vec<String> = crawl_results.iter()
            .map(|result| format!("æ¥æº: {}\nå†…å®¹æ‘˜è¦: {}", 
                result.source_url, 
                result.content_fragments.iter()
                    .map(|f| f.text.chars().take(300).collect::<String>())
                    .collect::<Vec<_>>()
                    .join("\n")
            ))
            .collect();
        
        let prompt = format!(
            "åˆ†æä»¥ä¸‹å¤šä¸ªçˆ¬å–ç»“æœä¹‹é—´çš„å†…å®¹å…³ç³»ï¼š
            
            çˆ¬å–ç»“æœæ•°é‡: {}
            å†…å®¹æ‘˜è¦:
            {}
            
            è¯·åˆ†æï¼š
            1. å†…å®¹ä¹‹é—´çš„é‡å å’Œå†—ä½™
            2. äº’è¡¥æ€§å’Œå±‚æ¬¡å…³ç³»
            3. ä¿¡æ¯çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§
            4. æ¨èçš„èåˆç­–ç•¥
            
            è¿”å›JSONæ ¼å¼çš„å…³ç³»åˆ†æã€‚",
            crawl_results.len(),
            content_summaries.join("\n---\n")
        );
        
        let llm_response = self.llm_orchestrator.generate_structured_response(
            &prompt,
            &json_schema_for_content_analysis()
        ).await?;
        
        ContentRelationshipAnalysis::from_llm_response(llm_response)
    }
    
    async fn fuse_content_type_with_llm(&self, content_type: ContentType, content_group: Vec<CrawlResult>) -> ContentSection {
        match content_type {
            ContentType::Examples => self.fuse_examples_with_llm(content_group).await,
            ContentType::Changelog => self.fuse_changelog_with_llm(content_group).await,
            ContentType::ApiDocs => self.fuse_api_docs_with_llm(content_group).await,
            ContentType::General => self.fuse_general_content_with_llm(content_group).await,
        }
    }
    
    async fn fuse_examples_with_llm(&self, results: Vec<CrawlResult>) -> ContentSection {
        let all_examples: Vec<String> = results.iter()
            .flat_map(|r| &r.content_fragments)
            .flat_map(|f| &f.code_blocks)
            .cloned()
            .collect();
        
        let prompt = format!(
            "æ•´ç†å’Œä¼˜åŒ–ä»¥ä¸‹ä»£ç ç¤ºä¾‹é›†åˆï¼š
            
            ä»£ç ç¤ºä¾‹ï¼š
            {}
            
            è¯·ï¼š
            1. å»é™¤é‡å¤å’Œç›¸ä¼¼çš„ç¤ºä¾‹
            2. æŒ‰å¤æ‚åº¦å’Œç”¨é€”åˆ†ç±»
            3. ä¸ºæ¯ä¸ªç¤ºä¾‹æ·»åŠ æ¸…æ™°çš„è¯´æ˜
            4. ç¡®ä¿ç¤ºä¾‹çš„å®Œæ•´æ€§å’Œå¯è¿è¡Œæ€§
            5. æŒ‰å­¦ä¹ è¿›åº¦æ’åºï¼ˆä»ç®€å•åˆ°å¤æ‚ï¼‰
            
            è¿”å›Markdownæ ¼å¼çš„ç»“æ„åŒ–ç¤ºä¾‹æ–‡æ¡£ã€‚",
            all_examples.join("\n\n```\n\n")
        );
        
        let formatted_content = self.llm_orchestrator.generate_completion(&prompt).await
            .unwrap_or_else(|_| self.fallback_format_examples(all_examples));
        
        ContentSection {
            title: "ä»£ç ç¤ºä¾‹".to_string(),
            content: formatted_content,
            source_urls: results.iter().map(|r| r.source_url.clone()).collect(),
        }
    }
    
    async fn fuse_changelog_with_llm(&self, results: Vec<CrawlResult>) -> ContentSection {
        let changelog_texts: Vec<String> = results.iter()
            .flat_map(|r| &r.content_fragments)
            .map(|f| f.text.clone())
            .collect();
        
        let prompt = format!(
            "æ•´ç†ä»¥ä¸‹æ›´æ–°æ—¥å¿—ä¿¡æ¯ï¼š
            
            {}
            
            è¯·ï¼š
            1. æå–ç‰ˆæœ¬å·å’Œå‘å¸ƒæ—¥æœŸ
            2. å½’ç±»å˜æ›´ç±»å‹ï¼ˆæ–°åŠŸèƒ½ã€ä¿®å¤ã€ç ´åæ€§å˜æ›´ç­‰ï¼‰
            3. å»é™¤é‡å¤ä¿¡æ¯
            4. æŒ‰æ—¶é—´å€’åºæ’åˆ—
            5. çªå‡ºé‡è¦çš„å˜æ›´å’Œå½±å“
            
            è¿”å›æ¸…æ™°çš„Markdownæ ¼å¼æ›´æ–°æ—¥å¿—ã€‚",
            changelog_texts.join("\n\n---\n\n")
        );
        
        let formatted_content = self.llm_orchestrator.generate_completion(&prompt).await
            .unwrap_or_else(|_| self.fallback_format_changelog(changelog_texts));
        
        ContentSection {
            title: "æ›´æ–°æ—¥å¿—".to_string(),
            content: formatted_content,
            source_urls: results.iter().map(|r| r.source_url.clone()).collect(),
        }
    }
}
```

#### 5.6 LLMæœåŠ¡é›†æˆå±‚
```rust
pub struct LLMOrchestrator {
    openai_client: Option<OpenAIClient>,
    anthropic_client: Option<AnthropicClient>,
    default_provider: LLMProvider,
    fallback_providers: Vec<LLMProvider>,
}

impl LLMOrchestrator {
    pub async fn generate_completion(&self, prompt: &str) -> Result<String, LLMError> {
        let providers = [vec![self.default_provider], self.fallback_providers.clone()].concat();
        
        for provider in providers {
            match self.try_provider(provider, prompt).await {
                Ok(response) => return Ok(response),
                Err(e) => {
                    warn!("LLM provider {} failed: {}", provider, e);
                    continue;
                }
            }
        }
        
        Err(LLMError::AllProvidersFailed)
    }
    
    pub async fn generate_structured_response(&self, prompt: &str, schema: &serde_json::Value) -> Result<serde_json::Value, LLMError> {
        let structured_prompt = format!(
            "{}\n\nè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSON Schemaæ ¼å¼è¿”å›ï¼š\n{}\n\nåªè¿”å›æœ‰æ•ˆçš„JSONï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šæ–‡å­—ã€‚",
            prompt,
            serde_json::to_string_pretty(schema)?
        );
        
        let response = self.generate_completion(&structured_prompt).await?;
        
        // å°è¯•è§£æJSONå“åº”
        serde_json::from_str(&response)
            .or_else(|_| self.extract_json_from_response(&response))
            .map_err(|e| LLMError::InvalidJsonResponse(e.to_string()))
    }
    
    async fn try_provider(&self, provider: LLMProvider, prompt: &str) -> Result<String, LLMError> {
        match provider {
            LLMProvider::OpenAI => {
                self.openai_client.as_ref()
                    .ok_or(LLMError::ProviderNotConfigured)?
                    .generate_completion(prompt).await
            },
            LLMProvider::Anthropic => {
                self.anthropic_client.as_ref()
                    .ok_or(LLMError::ProviderNotConfigured)?
                    .generate_completion(prompt).await
            },
        }
    }
}
```

### 6. å®é™…åº”ç”¨æµç¨‹

#### 6.1 ç®€å•ç¤ºä¾‹ï¼šè·å–Python requestsåº“çš„ä½¿ç”¨ç¤ºä¾‹
```rust
// ç”¨æˆ·è¯·æ±‚
let request = DocumentationRequest {
    language: "python".to_string(),
    target: "requests".to_string(),
    query: "examples".to_string(),
};

// ç³»ç»Ÿå¤„ç†æµç¨‹ï¼š
// 1. TaskCoordinatorè¯†åˆ«ä¸ºç¤ºä¾‹ç±»å‹
// 2. URLDiscoveryAgentå‘ç°ç›¸å…³URL:
//    - https://docs.python-requests.org/en/master/user/quickstart/
//    - https://github.com/psf/requests/tree/main/examples
//    - https://requests.readthedocs.io/en/latest/
// 3. ContentExtractionAgentå¹¶è¡Œçˆ¬å–è¿™äº›é¡µé¢
// 4. PageNavigationAgentå¤„ç†éœ€è¦ç‚¹å‡»æŸ¥çœ‹çš„ç¤ºä¾‹
// 5. ContentFusionAgentå°†æ‰€æœ‰ç¤ºä¾‹åˆå¹¶æˆç»“æ„åŒ–æ–‡æ¡£
```

#### 6.2 å¤æ‚ç¤ºä¾‹ï¼šè·å–Rust tokioçš„changelog
```rust
// ç”¨æˆ·è¯·æ±‚
let request = DocumentationRequest {
    language: "rust".to_string(),
    target: "tokio".to_string(),
    query: "changelog".to_string(),
};

// ç³»ç»Ÿå¤„ç†æµç¨‹ï¼š
// 1. å‘ç°changelogç›¸å…³URL
// 2. çˆ¬å–GitHub releasesé¡µé¢
// 3. æå–ç‰ˆæœ¬ä¿¡æ¯å’Œå˜æ›´æè¿°
// 4. ç»„ç»‡æˆæ—¶é—´çº¿æ ¼å¼çš„æ–‡æ¡£
```

## æ€»ç»“

è¯­è¨€ç‰¹æ€§æ¨¡å—é€šè¿‡é›†æˆé«˜çº§çˆ¬è™«æŠ€æœ¯ã€å¤šç»´åº¦å†…å®¹åˆ†æèƒ½åŠ›ä»¥åŠä¸æœ€å…ˆè¿›çš„AIæ¨¡å‹ï¼ˆåŒ…æ‹¬LLMå’Œæœ¬åœ°NLPæ¨¡å‹ï¼‰çš„äº¤äº’ï¼Œæå¤§åœ°å¢å¼ºäº† `grape-mcp-devtools` çš„æ™ºèƒ½æ°´å¹³ã€‚å®ƒä½¿å¾—å·¥å…·èƒ½å¤Ÿæ›´æ·±å…¥åœ°ç†è§£ä»£ç å’Œæ–‡æ¡£çš„è¯­ä¹‰ï¼Œä»è€Œæä¾›æ›´ç²¾å‡†ã€æ›´æœ‰ä»·å€¼çš„è¾…åŠ©ã€‚æ¨¡å—çš„å¯é…ç½®æ€§å’Œå¯æ‰©å±•æ€§è®¾è®¡ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸æ–­å‘å±•çš„AIæŠ€æœ¯å’Œå¤šæ ·åŒ–çš„ç”¨æˆ·éœ€æ±‚ï¼Œæ˜¯é¡¹ç›®ä¿æŒæŠ€æœ¯é¢†å…ˆæ€§çš„å…³é”®ç»„ä»¶ã€‚ 

## ğŸ”§ é«˜çº§æ™ºèƒ½çˆ¬è™«è¯¦ç»†ç®—æ³•å®ç°

### 5.6 æ ¸å¿ƒç®—æ³•å®ç°è¯¦è§£

åŸºäºæˆ‘ä»¬åœ¨ `src/ai/advanced_intelligent_crawler.rs` ä¸­çš„å®é™…å®ç°ï¼Œä»¥ä¸‹æ˜¯ä¸‰å¤§æ ¸å¿ƒAgentçš„è¯¦ç»†ç®—æ³•ï¼š

#### 5.6.1 URLå‘ç°Agentç®—æ³•å®ç°

**ç®—æ³•åç§°ï¼šAIé©±åŠ¨çš„ä¼˜å…ˆçº§é“¾æ¥å‘ç°ç®—æ³•**

```rust
/// æ ¸å¿ƒæ•°æ®ç»“æ„
struct PrioritizedUrl {
    url: String,
    priority: f32,        // 0.0-1.0ï¼ŒAIè¯„ä¼°çš„ä¼˜å…ˆçº§
    depth: u32,           // çˆ¬å–æ·±åº¦
    source_page_url: Option<String>, // æ¥æºé¡µé¢
}

/// ä¸»è¦ç®—æ³•ï¼šæ™ºèƒ½é“¾æ¥å‘ç°ä¸ä¼˜å…ˆçº§è°ƒåº¦
impl UrlDiscoveryAgent {
    async fn discover_links_from_content(
        &self, 
        page_content: &str, 
        current_url: &str, 
        current_depth: u32
    ) -> Result<()> {
        // ğŸ” ç¬¬ä¸€é˜¶æ®µï¼šAIåˆ†æé¡µé¢å†…å®¹
        let ai_request = AIRequest {
            system_prompt: Some(self.get_link_discovery_system_prompt()),
            user_message: self.get_link_discovery_user_message(page_content, current_url),
            temperature: Some(0.3), // ä½æ¸©åº¦ç¡®ä¿åˆ†æå‡†ç¡®æ€§
            max_tokens: Some(2000),
        };
        
        let response = self.ai_service.request(ai_request).await?;
        
        // ğŸ§  ç¬¬äºŒé˜¶æ®µï¼šç»“æ„åŒ–è§£æAIå“åº”
        let discovered_links = self.parse_link_discovery_response(&response.content, current_url).await?;
        
        // âš¡ ç¬¬ä¸‰é˜¶æ®µï¼šå¹¶å‘å¤„ç†é“¾æ¥è§„èŒƒåŒ–ä¸å»é‡
        let mut queue = self.pending_urls.write().await;
        let visited = self.visited_urls.read().await;
        
        for link in discovered_links {
            if let Ok(normalized_url) = self.normalize_url(&link.url, current_url) {
                // å»é‡æ£€æŸ¥ï¼šé¿å…é‡å¤è®¿é—®
                if !visited.contains(&normalized_url) && 
                   !queue.iter().any(|p| p.url == normalized_url) {
                    
                    let prioritized_url = PrioritizedUrl {
                        url: normalized_url.clone(),
                        priority: link.priority,
                        depth: current_depth + 1,
                        source_page_url: Some(current_url.to_string()),
                    };
                    
                    // ğŸ¯ å…³é”®ç®—æ³•ï¼šä¼˜å…ˆçº§é˜Ÿåˆ—æ’å…¥
                    self.insert_by_priority(&mut queue, prioritized_url);
                }
            }
        }
        
        Ok(())
    }
    
    /// ä¼˜å…ˆçº§è°ƒåº¦æ ¸å¿ƒç®—æ³•ï¼šç»´æŠ¤æœ‰åºé˜Ÿåˆ—
    fn insert_by_priority(&self, queue: &mut VecDeque<PrioritizedUrl>, new_url: PrioritizedUrl) {
        // çº¿æ€§æœç´¢æ’å…¥ä½ç½®ï¼ˆå°è§„æ¨¡é˜Ÿåˆ—æ•ˆç‡è¶³å¤Ÿï¼‰
        let mut insert_index = queue.len();
        
        for (i, existing) in queue.iter().enumerate() {
            if new_url.priority > existing.priority {
                insert_index = i;
                break;
            }
        }
        
        queue.insert(insert_index, new_url);
        // æ—¶é—´å¤æ‚åº¦ï¼šO(n)ï¼Œç©ºé—´å¤æ‚åº¦ï¼šO(1)
    }
}
```

**AI Promptå·¥ç¨‹ç­–ç•¥ï¼š**

```rust
fn get_link_discovery_system_prompt(&self) -> String {
    r#"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç½‘é¡µé“¾æ¥åˆ†æä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»HTMLå†…å®¹ä¸­æ™ºèƒ½è¯†åˆ«å’Œè¯„ä¼°ä¸ç‰¹å®šæŠ€æœ¯ä»»åŠ¡ç›¸å…³çš„é“¾æ¥ã€‚

ç®—æ³•è¦æ±‚ï¼š
1. åˆ†æHTMLä¸­çš„æ‰€æœ‰é“¾æ¥ï¼ˆ<a>æ ‡ç­¾ã€å¯¼èˆªèœå•ã€ç›¸å…³é“¾æ¥ç­‰ï¼‰
2. æ ¹æ®ä»»åŠ¡ç›®æ ‡è¯„ä¼°æ¯ä¸ªé“¾æ¥çš„ç›¸å…³æ€§å’Œä»·å€¼
3. ä¸ºæ¯ä¸ªç›¸å…³é“¾æ¥åˆ†é…ä¼˜å…ˆçº§åˆ†æ•°ï¼ˆ0.0-1.0ï¼‰
4. è¿”å›ç»“æ„åŒ–çš„JSONç»“æœ

è¯„ä¼°æ ‡å‡†ï¼ˆä¼˜å…ˆçº§ç®—æ³•ï¼‰ï¼š
- å®˜æ–¹æ–‡æ¡£é“¾æ¥ï¼šä¼˜å…ˆçº§ 0.9-1.0
- APIå‚è€ƒå’Œæ•™ç¨‹ï¼šä¼˜å…ˆçº§ 0.7-0.9
- ä»£ç ç¤ºä¾‹å’Œç”¨ä¾‹ï¼šä¼˜å…ˆçº§ 0.6-0.8
- ç¤¾åŒºè®¨è®ºå’Œåšå®¢ï¼šä¼˜å…ˆçº§ 0.4-0.6
- ç›¸å…³ä½†éæ ¸å¿ƒå†…å®¹ï¼šä¼˜å…ˆçº§ 0.2-0.4
- æ— å…³å†…å®¹ï¼šä¼˜å…ˆçº§ 0.0-0.2

è¿”å›æ ¼å¼ï¼šJSONæ•°ç»„ï¼Œæ¯ä¸ªå¯¹è±¡åŒ…å«ï¼š
{
  "url": "é“¾æ¥URL",
  "priority": 0.85,
  "link_text": "é“¾æ¥æ–‡æœ¬", 
  "context": "é“¾æ¥ä¸Šä¸‹æ–‡æè¿°",
  "reasoning": "é€‰æ‹©æ­¤é“¾æ¥çš„åŸå› "
}"#.to_string()
}
```

#### 5.6.2 å†…å®¹æå–Agentç®—æ³•å®ç°

**ç®—æ³•åç§°ï¼šAIé©±åŠ¨çš„ç»“æ„åŒ–å†…å®¹æå–ç®—æ³•**

```rust
/// æ ¸å¿ƒæ•°æ®ç»“æ„
pub struct ContentFragment {
    pub source_url: String,
    pub fragment_type: ContentType,
    pub title: Option<String>,
    pub content: String,
    pub relevance_score: f32, // AIè¯„ä¼°çš„ç›¸å…³æ€§åˆ†æ•°
}

/// ä¸»è¦ç®—æ³•ï¼šå¤šé˜¶æ®µå†…å®¹æå–æµæ°´çº¿
impl ContentExtractionAgent {
    async fn fetch_and_extract(&self, url: &str) -> Result<Vec<ContentFragment>> {
        // ğŸŒ ç¬¬ä¸€é˜¶æ®µï¼šå¥å£®çš„HTTPå†…å®¹è·å–
        let html_content = self.fetch_page_content(url).await?;

        // ğŸ§  ç¬¬äºŒé˜¶æ®µï¼šAIé©±åŠ¨çš„å†…å®¹åˆ†æä¸æå–
        let request = AIRequest {
            system_prompt: Some(self.get_extraction_system_prompt()),
            user_message: self.get_extraction_user_message(&html_content, url),
            temperature: Some(0.2), // ä½æ¸©åº¦ç¡®ä¿æå–ç²¾ç¡®æ€§
            max_tokens: Some(3000),
        };
        
        let response = self.ai_service.request(request).await?;
        
        // ğŸ“Š ç¬¬ä¸‰é˜¶æ®µï¼šç»“æ„åŒ–è§£æä¸éªŒè¯
        let fragments = self.parse_extraction_response(&response.content, url).await?;
        
        Ok(fragments)
    }
    
    /// ç½‘ç»œè¯·æ±‚ç®—æ³•ï¼šæŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶
    async fn fetch_page_content(&self, url: &str) -> Result<String> {
        let client = reqwest::Client::builder()
            .timeout(std::time::Duration::from_secs(self.config.timeout_secs))
            .user_agent(self.config.user_agent.clone())
            .build()?;

        let mut attempts = 0;
        while attempts < self.config.max_retries {
            match client.get(url).send().await {
                Ok(response) => {
                    if response.status().is_success() {
                        return Ok(response.text().await?);
                    }
                }
                Err(e) => {
                    warn!("ç½‘ç»œè¯·æ±‚å¤±è´¥ (å°è¯• {}/{}): {}", attempts + 1, self.config.max_retries, e);
                }
            }
            attempts += 1;
            if attempts < self.config.max_retries {
                // æŒ‡æ•°é€€é¿ï¼š1s, 2s, 3s...
                tokio::time::sleep(std::time::Duration::from_millis(1000 * attempts as u64)).await;
            }
        }
        Err(anyhow::anyhow!("è·å–é¡µé¢å¤±è´¥ï¼Œå·²é‡è¯• {} æ¬¡", self.config.max_retries))
    }
    
    /// å†…å®¹è§£æç®—æ³•ï¼šJSONä¼˜å…ˆï¼Œå¤‡ç”¨æœºåˆ¶ä¿è¯å¥å£®æ€§
    async fn parse_extraction_response(&self, ai_response_content: &str, source_url: &str) -> Result<Vec<ContentFragment>> {
        // ä¸»è¦è§£æç­–ç•¥ï¼šJSONç»“æ„åŒ–è§£æ
        if let Ok(json_value) = serde_json::from_str::<serde_json::Value>(ai_response_content) {
            if let Some(fragments_array) = json_value.as_array() {
                let mut content_fragments = Vec::new();
                
                for fragment_obj in fragments_array {
                    if let (Some(content), Some(relevance_score)) = (
                        fragment_obj.get("content").and_then(|v| v.as_str()),
                        fragment_obj.get("relevance_score").and_then(|v| v.as_f64())
                    ) {
                        let fragment_type = fragment_obj.get("fragment_type")
                            .and_then(|v| v.as_str())
                            .and_then(|s| self.parse_content_type(s))
                            .unwrap_or(ContentType::Documentation);
                        
                        content_fragments.push(ContentFragment {
                            source_url: source_url.to_string(),
                            fragment_type,
                            title: fragment_obj.get("title")
                                .and_then(|v| v.as_str())
                                .map(|s| s.to_string()),
                            content: content.to_string(),
                            relevance_score: relevance_score as f32,
                        });
                    }
                }
                
                return Ok(content_fragments);
            }
        }
        
        // å¤‡ç”¨ç­–ç•¥ï¼šåˆ›å»ºåŸºæœ¬å†…å®¹ç‰‡æ®µï¼ˆç¡®ä¿ç³»ç»Ÿå¥å£®æ€§ï¼‰
        Ok(vec![ContentFragment {
            source_url: source_url.to_string(),
            fragment_type: ContentType::Documentation,
            title: Some("AIæå–çš„å†…å®¹".to_string()),
            content: ai_response_content.chars().take(1000).collect(),
            relevance_score: 0.6,
        }])
    }
}
```

**AI Promptå·¥ç¨‹ç­–ç•¥ï¼š**

```rust
fn get_extraction_system_prompt(&self) -> String {
    r#"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯å†…å®¹æå–ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»HTMLç½‘é¡µä¸­æå–ä¸ç‰¹å®šç¼–ç¨‹ä»»åŠ¡ç›¸å…³çš„ç»“æ„åŒ–ä¿¡æ¯ã€‚

ç®—æ³•è¦æ±‚ï¼š
1. åˆ†æHTMLå†…å®¹ï¼Œè¯†åˆ«ä¸ä»»åŠ¡ç›®æ ‡ç›¸å…³çš„æŠ€æœ¯ä¿¡æ¯
2. æå–ä»£ç ç¤ºä¾‹ã€APIæ–‡æ¡£ã€æ•™ç¨‹æ­¥éª¤ã€é…ç½®è¯´æ˜ç­‰
3. ä¸ºæ¯ä¸ªå†…å®¹ç‰‡æ®µåˆ†é…ç›¸å…³æ€§åˆ†æ•°ï¼ˆ0.0-1.0ï¼‰
4. è¿”å›ç»“æ„åŒ–çš„JSONç»“æœ

å†…å®¹ç±»å‹åˆ†ç±»ç®—æ³•ï¼š
- Documentation: å®˜æ–¹æ–‡æ¡£å’Œè¯´æ˜
- Tutorial: æ•™ç¨‹å’ŒæŒ‡å—
- ApiReference: APIå‚è€ƒæ–‡æ¡£
- Examples: ä»£ç ç¤ºä¾‹å’Œç”¨ä¾‹
- GettingStarted: å…¥é—¨æŒ‡å—
- Installation: å®‰è£…è¯´æ˜
- Configuration: é…ç½®æ–‡æ¡£
- Troubleshooting: æ•…éšœæ’é™¤

è¿”å›æ ¼å¼ï¼šJSONæ•°ç»„ï¼Œæ¯ä¸ªå¯¹è±¡åŒ…å«ï¼š
{
  "fragment_type": "Documentation",
  "title": "ç‰‡æ®µæ ‡é¢˜",
  "content": "æå–çš„å†…å®¹æ–‡æœ¬",
  "relevance_score": 0.85,
  "code_language": "rust" // å¦‚æœåŒ…å«ä»£ç 
}"#.to_string()
}
```

#### 5.6.3 çŸ¥è¯†èšåˆå™¨ç®—æ³•å®ç°

**ç®—æ³•åç§°ï¼šAIé©±åŠ¨çš„å¤šæºçŸ¥è¯†èåˆç®—æ³•**

```rust
/// ä¸»è¦ç®—æ³•ï¼šæ™ºèƒ½çŸ¥è¯†èšåˆä¸æ–‡æ¡£ç”Ÿæˆ
impl KnowledgeAggregator {
    async fn aggregate_knowledge(&self) -> Result<String> {
        let fragments = self.collected_fragments.read().await;
        if fragments.is_empty() {
            return Ok("æœªæ”¶é›†åˆ°ä»»ä½•å†…å®¹ç‰‡æ®µè¿›è¡Œèšåˆã€‚".to_string());
        }

        // ğŸ“‹ ç¬¬ä¸€é˜¶æ®µï¼šå†…å®¹é¢„å¤„ç†ä¸ç»“æ„åŒ–
        let mut content_to_aggregate = String::new();
        for (i, fragment) in fragments.iter().enumerate() {
            content_to_aggregate.push_str(&format!(
                "--- Fragment {} from {} (Relevance: {:.2}) ---\nTitle: {}\nType: {:?}\nContent:\n{}\n\n",
                i + 1,
                fragment.source_url,
                fragment.relevance_score,
                fragment.title.as_deref().unwrap_or("N/A"),
                fragment.fragment_type,
                fragment.content
            ));
        }
        
        // ğŸ§  ç¬¬äºŒé˜¶æ®µï¼šAIé©±åŠ¨çš„çŸ¥è¯†èåˆ
        let request = AIRequest {
            system_prompt: Some(self.get_aggregation_system_prompt()),
            user_message: self.get_aggregation_user_message(&content_to_aggregate),
            temperature: Some(0.5), // é€‚ä¸­æ¸©åº¦å¹³è¡¡åˆ›é€ æ€§å’Œå‡†ç¡®æ€§
            max_tokens: Some(4000), // å…è®¸ç”Ÿæˆè¾ƒé•¿æ–‡æ¡£
        };

        let response = self.ai_service.request(request).await?;
        Ok(response.content)
    }
}
```

**çŸ¥è¯†èšåˆPromptå·¥ç¨‹ï¼š**

```rust
fn get_aggregation_system_prompt(&self) -> String {
    r#"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£ç¼–å†™ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†æ¥è‡ªå¤šä¸ªç½‘é¡µçš„æŠ€æœ¯å†…å®¹ç‰‡æ®µæ•´åˆæˆä¸€ä»½è¿è´¯ã€å…¨é¢ã€é«˜è´¨é‡çš„æŠ€æœ¯æ–‡æ¡£ã€‚

çŸ¥è¯†èšåˆç®—æ³•è¦æ±‚ï¼š
1. åˆ†ææ‰€æœ‰å†…å®¹ç‰‡æ®µï¼Œç†è§£å®ƒä»¬ä¹‹é—´çš„å…³ç³»å’Œå±‚æ¬¡
2. å»é™¤é‡å¤ä¿¡æ¯ï¼Œæ•´åˆç›¸å…³å†…å®¹
3. æŒ‰é€»è¾‘é¡ºåºç»„ç»‡å†…å®¹ï¼ˆæ¦‚è¿°â†’å®‰è£…â†’åŸºç¡€ç”¨æ³•â†’é«˜çº§ç‰¹æ€§â†’ç¤ºä¾‹â†’æ•…éšœæ’é™¤ï¼‰
4. ç¡®ä¿æŠ€æœ¯ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
5. æ·»åŠ å¿…è¦çš„è¿‡æ¸¡å’Œè§£é‡Šæ–‡æœ¬
6. ç”Ÿæˆç»“æ„åŒ–çš„Markdownæ–‡æ¡£

æ–‡æ¡£ç»“æ„ç®—æ³•ï¼š
- ä½¿ç”¨æ¸…æ™°çš„æ ‡é¢˜å±‚æ¬¡ï¼ˆ#, ##, ###ï¼‰
- ä»£ç å—ä½¿ç”¨æ­£ç¡®çš„è¯­è¨€æ ‡è¯†
- åŒ…å«ç›®å½•å’Œç« èŠ‚å¯¼èˆª
- çªå‡ºé‡è¦ä¿¡æ¯å’Œæœ€ä½³å®è·µ
- æä¾›å®ç”¨çš„ä»£ç ç¤ºä¾‹

è¾“å‡ºæ ¼å¼ï¼šå®Œæ•´çš„Markdownæ–‡æ¡£ï¼ŒåŒ…å«ï¼š
1. æ–‡æ¡£æ ‡é¢˜å’Œç®€ä»‹
2. ç›®å½•
3. ä¸»è¦å†…å®¹ç« èŠ‚
4. ä»£ç ç¤ºä¾‹å’Œç”¨æ³•
5. å‚è€ƒé“¾æ¥å’Œæ¥æº"#.to_string()
}
```

### 5.7 ç®—æ³•å¤æ‚åº¦åˆ†æä¸æ€§èƒ½ä¼˜åŒ–

#### 5.7.1 æ—¶é—´å¤æ‚åº¦åˆ†æ

**URLå‘ç°Agentï¼š**
- é“¾æ¥å‘ç°ï¼šO(n) - nä¸ºé¡µé¢ä¸­çš„é“¾æ¥æ•°
- ä¼˜å…ˆçº§æ’å…¥ï¼šO(m) - mä¸ºé˜Ÿåˆ—ä¸­çš„URLæ•°
- æ€»ä½“ï¼šO(p Ã— n Ã— m) - pä¸ºå¤„ç†çš„é¡µé¢æ•°

**å†…å®¹æå–Agentï¼š**
- HTTPè¯·æ±‚ï¼šO(1) - å•æ¬¡ç½‘ç»œè¯·æ±‚
- AIå†…å®¹åˆ†æï¼šO(1) - å•æ¬¡AIè°ƒç”¨
- JSONè§£æï¼šO(k) - kä¸ºå“åº”å†…å®¹å¤§å°
- æ€»ä½“ï¼šO(p Ã— k) - pä¸ºé¡µé¢æ•°

**çŸ¥è¯†èšåˆå™¨ï¼š**
- å†…å®¹é¢„å¤„ç†ï¼šO(f) - fä¸ºå†…å®¹ç‰‡æ®µæ•°
- AIèšåˆï¼šO(1) - å•æ¬¡AIè°ƒç”¨
- æ€»ä½“ï¼šO(f)

#### 5.7.2 ç©ºé—´å¤æ‚åº¦åˆ†æ

**å†…å­˜ä½¿ç”¨ï¼š**
- URLé˜Ÿåˆ—ï¼šO(u) - uä¸ºå¾…è®¿é—®URLæ•°
- å†…å®¹ç‰‡æ®µï¼šO(f Ã— s) - fä¸ªç‰‡æ®µï¼Œå¹³å‡å¤§å°s
- è®¿é—®è®°å½•ï¼šO(v) - vä¸ºå·²è®¿é—®URLæ•°
- æ€»ä½“ï¼šO(u + fÃ—s + v)

#### 5.7.3 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

**1. å¹¶å‘å¤„ç†ä¼˜åŒ–ï¼š**
```rust
// å¹¶è¡Œå†…å®¹æå–
let extraction_tasks: Vec<_> = urls.into_iter()
    .map(|url| self.content_extractor.fetch_and_extract(&url))
    .collect();

let results = futures::try_join_all(extraction_tasks).await?;
```

**2. ç¼“å­˜æœºåˆ¶ï¼š**
```rust
// URLçº§åˆ«ç¼“å­˜
if let Some(cached_content) = self.cache.get(&url).await {
    return Ok(cached_content);
}

// AIå“åº”ç¼“å­˜
let cache_key = format!("ai_extract_{}", hash(&html_content));
if let Some(cached_fragments) = self.ai_cache.get(&cache_key).await {
    return Ok(cached_fragments);
}
```

**3. æ™ºèƒ½å‰ªæï¼š**
```rust
// åŸºäºç›¸å…³æ€§é˜ˆå€¼çš„æ—©æœŸç»ˆæ­¢
if link.priority < self.config.min_relevance_score {
    continue; // è·³è¿‡ä½ç›¸å…³æ€§é“¾æ¥
}

// æ·±åº¦é™åˆ¶
if current_depth >= self.task.max_depth {
    break; // åœæ­¢æ·±åº¦çˆ¬å–
}
```

### 5.8 ä¸ç°æœ‰ç³»ç»Ÿé›†æˆ

#### 5.8.1 DocumentProcessoré›†æˆ

```rust
impl DocumentProcessor {
    async fn process_documentation_request(&self, /* ... */) -> Result<Vec<DocumentFragment>> {
        // 1. å°è¯•CLIå·¥å…·ï¼ˆä¸»è¦ç­–ç•¥ï¼‰
        if let Ok(cli_result) = self.try_cli_approach(language, library, query).await {
            info!("âœ… CLIå·¥å…·æˆåŠŸï¼Œè¿”å› {} ä¸ªæ–‡æ¡£ç‰‡æ®µ", cli_result.len());
            return Ok(cli_result);
        }
        
        // 2. å¤‡ç”¨ç­–ç•¥ï¼šé«˜çº§æ™ºèƒ½çˆ¬è™«
        warn!("âš ï¸  CLIå·¥å…·å¤±è´¥ï¼Œå¯åŠ¨AIçˆ¬è™«å¤‡ç”¨ç­–ç•¥");
        
        let crawler = AdvancedIntelligentCrawler::new(
            self.ai_service_config.clone(),
            self.crawler_config.clone()
        ).await?;
        
        let task = CrawlTask {
            task_id: format!("doc_{}_{}", language, library),
            target_description: format!("ä¸º{}è¯­è¨€çš„{}åº“æ”¶é›†ç›¸å…³æ–‡æ¡£å’Œç¤ºä¾‹", language, library),
            start_url: self.construct_start_url(language, library)?,
            library_name: library.to_string(),
            programming_language: language.to_string(),
            expected_content_types: vec![
                ContentType::Documentation,
                ContentType::Tutorial,
                ContentType::Examples,
                ContentType::ApiReference,
            ],
            max_depth: 2,
            max_pages: 10,
            created_at: chrono::Utc::now(),
        };
        
        let result = crawler.execute_task(task).await?;
        
        // 3. è½¬æ¢ä¸ºDocumentFragmentæ ¼å¼
        let fragments = self.convert_advanced_result_to_fragments(result)?;
        
        info!("âœ… AIçˆ¬è™«æˆåŠŸï¼Œç”Ÿæˆ {} ä¸ªæ–‡æ¡£ç‰‡æ®µ", fragments.len());
        Ok(fragments)
    }
    
    /// ç»“æœæ ¼å¼è½¬æ¢
    fn convert_advanced_result_to_fragments(&self, result: AdvancedTaskResult) -> Result<Vec<DocumentFragment>> {
        let mut fragments = Vec::new();
        
        // ä¸»è¦èšåˆæ–‡æ¡£
        fragments.push(DocumentFragment {
            file_path: format!("{}_comprehensive_guide.md", result.task.library_name),
            content: result.aggregated_document,
            language: result.task.programming_language.clone(),
            package_name: result.task.library_name.clone(),
            fragment_type: "comprehensive_documentation".to_string(),
            metadata: HashMap::from([
                ("source".to_string(), "ai_crawler".to_string()),
                ("sources_count".to_string(), result.visited_urls_count.to_string()),
                ("fragments_count".to_string(), result.source_fragments.len().to_string()),
            ]),
        });
        
        // å„ä¸ªæºç‰‡æ®µ
        for (i, fragment) in result.source_fragments.iter().enumerate() {
            fragments.push(DocumentFragment {
                file_path: format!("{}_{}_fragment_{}.md", 
                    result.task.library_name, 
                    format!("{:?}", fragment.fragment_type).to_lowercase(),
                    i + 1
                ),
                content: fragment.content.clone(),
                language: result.task.programming_language.clone(),
                package_name: result.task.library_name.clone(),
                fragment_type: format!("{:?}", fragment.fragment_type).to_lowercase(),
                metadata: HashMap::from([
                    ("source_url".to_string(), fragment.source_url.clone()),
                    ("relevance_score".to_string(), fragment.relevance_score.to_string()),
                    ("title".to_string(), fragment.title.as_deref().unwrap_or("").to_string()),
                ]),
            });
        }
        
        Ok(fragments)
    }
}
```

#### 5.8.2 æ€§èƒ½ç›‘æ§ä¸è°ƒä¼˜

**å…³é”®æ€§èƒ½æŒ‡æ ‡ï¼ˆKPIï¼‰ï¼š**

```rust
pub struct CrawlerMetrics {
    pub pages_per_second: f64,        // çˆ¬å–é€Ÿåº¦
    pub ai_call_latency_ms: u64,      // AIè°ƒç”¨å»¶è¿Ÿ
    pub content_quality_score: f32,   // å†…å®¹è´¨é‡åˆ†æ•°
    pub memory_usage_mb: u64,         // å†…å­˜ä½¿ç”¨é‡
    pub cache_hit_rate: f32,          // ç¼“å­˜å‘½ä¸­ç‡
    pub error_rate: f32,              // é”™è¯¯ç‡
}

impl AdvancedIntelligentCrawler {
    async fn collect_metrics(&self) -> CrawlerMetrics {
        CrawlerMetrics {
            pages_per_second: self.calculate_crawl_speed().await,
            ai_call_latency_ms: self.measure_ai_latency().await,
            content_quality_score: self.evaluate_content_quality().await,
            memory_usage_mb: self.get_memory_usage().await,
            cache_hit_rate: self.calculate_cache_hit_rate().await,
            error_rate: self.calculate_error_rate().await,
        }
    }
}
```

**è‡ªé€‚åº”ä¼˜åŒ–ç­–ç•¥ï¼š**

```rust
impl AdvancedIntelligentCrawler {
    async fn adaptive_optimization(&mut self, metrics: &CrawlerMetrics) {
        // åŸºäºæ€§èƒ½æŒ‡æ ‡åŠ¨æ€è°ƒæ•´å‚æ•°
        if metrics.ai_call_latency_ms > 5000 {
            // AIè°ƒç”¨è¿‡æ…¢ï¼Œå‡å°‘å¹¶å‘åº¦
            self.config.max_concurrent_requests = (self.config.max_concurrent_requests / 2).max(1);
        }
        
        if metrics.cache_hit_rate < 0.3 {
            // ç¼“å­˜å‘½ä¸­ç‡ä½ï¼Œå¢åŠ ç¼“å­˜æ—¶é—´
            self.config.cache_ttl_hours *= 2;
        }
        
        if metrics.content_quality_score < 0.6 {
            // å†…å®¹è´¨é‡ä½ï¼Œæé«˜ç›¸å…³æ€§é˜ˆå€¼
            self.config.min_relevance_score += 0.1;
        }
    }
}
```

è¿™å¥—é«˜çº§æ™ºèƒ½çˆ¬è™«ç³»ç»Ÿé€šè¿‡AIé©±åŠ¨çš„ä¸‰é˜¶æ®µå¤„ç†æµæ°´çº¿ï¼Œå®ç°äº†ä»ç®€å•URLçˆ¬å–åˆ°æ™ºèƒ½çŸ¥è¯†èšåˆçš„å®Œæ•´è§£å†³æ–¹æ¡ˆï¼Œä¸ºå¼€å‘è€…æä¾›é«˜è´¨é‡çš„æŠ€æœ¯æ–‡æ¡£æ”¯æŒã€‚ 